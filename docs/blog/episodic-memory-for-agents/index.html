<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Beyond Vector RAG: Building Agent Memory That Learns From Experience. | BharatMLStack</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://meesho.github.io/BharatMLStack/blog/episodic-memory-for-agents"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Beyond Vector RAG: Building Agent Memory That Learns From Experience. | BharatMLStack"><meta data-rh="true" name="description" content="Current agent memory is just search. We built an episodic memory system that tracks outcomes, forms causal links, extracts reasoning heuristics, and actually learns from failure — without retraining the model."><meta data-rh="true" property="og:description" content="Current agent memory is just search. We built an episodic memory system that tracks outcomes, forms causal links, extracts reasoning heuristics, and actually learns from failure — without retraining the model."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2026-02-19T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/a0d00kc"><meta data-rh="true" property="article:tag" content="ai-agents,memory,architecture,llm,episodic-memory"><link data-rh="true" rel="icon" href="/BharatMLStack/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://meesho.github.io/BharatMLStack/blog/episodic-memory-for-agents"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/blog/episodic-memory-for-agents" hreflang="en"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/blog/episodic-memory-for-agents" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://meesho.github.io/BharatMLStack/blog/episodic-memory-for-agents","mainEntityOfPage":"https://meesho.github.io/BharatMLStack/blog/episodic-memory-for-agents","url":"https://meesho.github.io/BharatMLStack/blog/episodic-memory-for-agents","headline":"Beyond Vector RAG: Building Agent Memory That Learns From Experience.","name":"Beyond Vector RAG: Building Agent Memory That Learns From Experience.","description":"Current agent memory is just search. We built an episodic memory system that tracks outcomes, forms causal links, extracts reasoning heuristics, and actually learns from failure — without retraining the model.","datePublished":"2026-02-19T00:00:00.000Z","author":{"@type":"Person","name":"Adarsha Das","description":"Senior Architect @ Meesho","url":"https://github.com/a0d00kc","image":"https://github.com/a0d00kc.png"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://meesho.github.io/BharatMLStack/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/BharatMLStack/blog/rss.xml" title="BharatMLStack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/BharatMLStack/blog/atom.xml" title="BharatMLStack Atom Feed"><link rel="stylesheet" href="/BharatMLStack/assets/css/styles.aaf16941.css">
<script src="/BharatMLStack/assets/js/runtime~main.a356c557.js" defer="defer"></script>
<script src="/BharatMLStack/assets/js/main.6f8db0ca.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="https://github.com/a0d00kc.png"><div class="gradient-bg-global"><div class="gradient-orb-global orb-global-1"></div><div class="gradient-orb-global orb-global-2"></div><div class="gradient-orb-global orb-global-3"></div></div><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/BharatMLStack/"><b class="navbar__title text--truncate">BharatMLStack</b></a><a class="navbar__item navbar__link" href="/BharatMLStack/intro">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/BharatMLStack/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/BharatMLStack/blog/episodic-memory-for-agents">Beyond Vector RAG: Building Agent Memory That Learns From Experience.</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/llm-inference-optimization-sub-sec-latency">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/multi-engine-llm-inferencing-platform">Designing a Production-Grade LLM Inference Platform: From Model Weights to Scalable GPU Serving</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/scaling-model-inference-and-embedding-search">Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/building-meeshos-mlplatform-lessons-from-first-gen">Building Meesho’s ML Platform: Lessons from the First-Gen System (Part 2)</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/building-meeshos-mlplatform">Building Meesho’s ML Platform: From Chaos to Cutting-Edge (Part 1)</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Beyond Vector RAG: Building Agent Memory That Learns From Experience.</h1><div class="container_mt6G margin-vert--md"><time datetime="2026-02-19T00:00:00.000Z">February 19, 2026</time> · <!-- -->12 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/a0d00kc" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/a0d00kc.png" alt="Adarsha Das"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/a0d00kc" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Adarsha Das</span></a></div><small class="authorTitle_nd0D" title="Senior Architect @ Meesho">Senior Architect @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p><img decoding="async" loading="lazy" alt="BharatMLStack" src="/BharatMLStack/assets/images/bms-7399e8796d2cd24617c432518ce3f312.png" width="1396" height="460" class="img_ev3q">
Agent memory has come a long way. Persistent context, vector retrieval, knowledge graphs — the building blocks are real and getting better fast.</p>
<p>But most of what we call &quot;memory&quot; today is still closer to search: chunk text, embed it, retrieve whatever looks similar at query time. That works well for recalling facts and preferences. It starts to break down when you need an agent to recall what happened last time, learn from a mistake, or avoid repeating a failed approach.</p>
<p>We are trying to experiment something different. An episodic memory system where a frozen LLM — same weights, no retraining — produces increasingly better decisions over time because the memory feeding it context is continuously evolving.
Then we tested it. The results were interesting.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-gap-nobody-talks-about">The Gap Nobody Talks About<a href="#the-gap-nobody-talks-about" class="hash-link" aria-label="Direct link to The Gap Nobody Talks About" title="Direct link to The Gap Nobody Talks About">​</a></h2>
<p>Here&#x27;s a scenario every engineering team has encountered: AI agent hits a Redis connection pool exhaustion issue. It misdiagnoses it as a database problem. You correct it. Next week, a different service has the exact same failure pattern. The agent makes the exact same mistake.</p>
<p>Why? Because LLMs don&#x27;t learn at inference time. Corrections adjust behavior within a conversation. Once the session ends, the lesson is gone. The model weights haven&#x27;t changed. The next conversation starts from zero.</p>
<p>Current &quot;memory&quot; systems don&#x27;t fully address this. They store facts — user preferences, document chunks, conversation summaries. But facts aren&#x27;t experience. Knowing that &quot;Redis connection pools can exhaust under load&quot; is different from remembering &quot;last time I saw 500 errors under load, I assumed it was the database, I was wrong, it was actually the connection pool, and here&#x27;s the correction I received.&quot;</p>
<p>The first is a fact. The second is an episode. The difference matters.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-wrong-with-vector-rag-as-memory">What&#x27;s Wrong With Vector RAG as Memory<a href="#whats-wrong-with-vector-rag-as-memory" class="hash-link" aria-label="Direct link to What&#x27;s Wrong With Vector RAG as Memory" title="Direct link to What&#x27;s Wrong With Vector RAG as Memory">​</a></h2>
<p>We identified five structural gaps in how current agent frameworks handle memory:</p>
<p><strong>No concept of time.</strong> Two events are either semantically similar or they&#x27;re not. The system can&#x27;t represent &quot;this happened after that&quot; without distorting similarity scores. An agent can&#x27;t reason about sequence or causality.</p>
<p><strong>No concept of situation.</strong> A production incident and a design review might use the same technical vocabulary. Flat vector search can&#x27;t distinguish them. Your agent retrieves planning notes when it should be retrieving incident postmortems.</p>
<p><strong>No outcome tracking.</strong> The system stores <em>what happened</em> but not <em>whether it worked</em>. A failed approach and a successful one are equally retrievable. The agent has no way to prefer strategies that worked over strategies that didn&#x27;t.</p>
<p><strong>Summaries destroy evidence.</strong> Summarization-based memory compresses experience but discards the reasoning chain. The agent loses the ability to explain <em>how</em> it arrived at a conclusion. The audit trail is gone.</p>
<p><strong>No causal links.</strong> Each memory chunk is independent. There&#x27;s no way to express that incident A caused decision B, which led to outcome C, which was corrected by approach D. Without this structure, the agent can&#x27;t traverse chains of reasoning.</p>
<p>These gaps compound. As an agent accumulates more experience, flat vector memory gets noisier, more contradictory, and less useful. The system degrades precisely when it should be improving.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-architecture-episodic-memory">The Architecture: Episodic Memory<a href="#the-architecture-episodic-memory" class="hash-link" aria-label="Direct link to The Architecture: Episodic Memory" title="Direct link to The Architecture: Episodic Memory">​</a></h2>
<p>We are building a memory system modeled on how human episodic memory works — not as a metaphor, but as an engineering specification.</p>
<p>The system has four layers:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="layer-1-immutable-timeline">Layer 1: Immutable Timeline<a href="#layer-1-immutable-timeline" class="hash-link" aria-label="Direct link to Layer 1: Immutable Timeline" title="Direct link to Layer 1: Immutable Timeline">​</a></h3>
<p>Every piece of agent experience is recorded as an append-only timeline entry. Each entry carries a semantic embedding (what it means), a timestamp (when it happened), and a state label (what situation the agent was in — debugging, planning, code review, incident response). Entries are never modified, never deleted, never summarized. This is the source of truth.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="layer-2-episode-segmentation">Layer 2: Episode Segmentation<a href="#layer-2-episode-segmentation" class="hash-link" aria-label="Direct link to Layer 2: Episode Segmentation" title="Direct link to Layer 2: Episode Segmentation">​</a></h3>
<p>The system watches the timeline and detects when one coherent unit of experience ends and another begins — via state transitions, semantic shifts, temporal gaps, or explicit signals. Each episode is a reference into the timeline (not a copy) with a generated summary, an outcome (SUCCESS, FAILURE, PARTIAL, UNKNOWN), decisions made, assumptions held, and corrections received.</p>
<p>The outcome field is the most important thing that doesn&#x27;t exist in any current memory system. Without it, you can&#x27;t learn from mistakes.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="layer-3-episodic-graph">Layer 3: Episodic Graph<a href="#layer-3-episodic-graph" class="hash-link" aria-label="Direct link to Layer 3: Episodic Graph" title="Direct link to Layer 3: Episodic Graph">​</a></h3>
<p>Episodes are connected through typed, weighted links: CAUSED_BY, LED_TO, RETRY_OF, LEARNED_FROM, CONTINUATION, CONTRADICTED. Over time, this forms a directed graph that enables traversal by meaning and causality. You can follow the chain: &quot;this incident caused that investigation, which led to a failed fix, which was corrected by this approach.&quot;</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="layer-4-generalized-facts">Layer 4: Generalized Facts<a href="#layer-4-generalized-facts" class="hash-link" aria-label="Direct link to Layer 4: Generalized Facts" title="Direct link to Layer 4: Generalized Facts">​</a></h3>
<p>When multiple episodes exhibit consistent patterns, the system extracts reasoning heuristics: &quot;When services fail immediately after deployment with no traffic change, investigate configuration errors before connection pool problems.&quot; Facts are versioned, never overwritten, and maintain links back to supporting and contradicting episodes. When contradicting evidence accumulates, confidence decreases. When confidence drops below a threshold, the fact is revised — but the old version is preserved.</p>
<p>The LLM sits above all four layers. At query time, the system assembles structured context — relevant episodes with outcomes, applicable facts with confidence scores, causal narratives — and passes it to the LLM for reasoning. The model reasons over structured memory. It doesn&#x27;t store or manage memory.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-reinforcement-loop">The Reinforcement Loop<a href="#the-reinforcement-loop" class="hash-link" aria-label="Direct link to The Reinforcement Loop" title="Direct link to The Reinforcement Loop">​</a></h3>
<p>This is where it comes together:</p>
<ol>
<li>Agent reasons using retrieved episodes and facts</li>
<li>Outcome is detected (CI pass/fail, user correction, test result)</li>
<li>New episode is created with outcome tracking</li>
<li>Links are created between the retrieved episodes and the new episode</li>
<li>Facts are reinforced (if outcome aligned) or contradicted (if outcome conflicted)</li>
<li>If the decision was wrong and corrected, a LEARNED_FROM link is created</li>
</ol>
<p>The model weights never change. The memory structure evolves continuously. A frozen LLM produces better decisions over time because it receives better context from richer memory.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-experiment">The Experiment<a href="#the-experiment" class="hash-link" aria-label="Direct link to The Experiment" title="Direct link to The Experiment">​</a></h2>
<p>We built the full system in Python (~1,000 lines) and tested it head-to-head against a baseline flat-vector RAG agent across a 9-round synthetic debugging scenario. Both agents used the identical LLM (Claude Sonnet 4) for reasoning. The only variable was the memory system.</p>
<p>The scenario was designed to test five capabilities:</p>
<table><thead><tr><th>Round Type</th><th>What It Tests</th><th>Rounds</th></tr></thead><tbody><tr><td>LEARN</td><td>Can the agent build experience from failures?</td><td>1, 2, 4</td></tr><tr><td>RED HERRING</td><td>Can the agent resist applying a pattern when it doesn&#x27;t fit?</td><td>3</td></tr><tr><td>TEST</td><td>Can the agent apply learned patterns to new services?</td><td>5, 6</td></tr><tr><td>SUBTLE</td><td>Can the agent generalize to different symptoms, same root cause?</td><td>7</td></tr><tr><td>CORRECTION</td><td>After being corrected, does the agent adapt?</td><td>8, 9</td></tr></tbody></table>
<p>Rounds 1-4 build experience: three connection pool failures across different services, plus one red herring (a deployment config error that <em>looks</em> like a connection pool issue). Rounds 5-7 test whether the agent applies the learned pattern to unfamiliar services and subtle symptom variations. Rounds 8-9 are the critical test: the agent is corrected after misdiagnosing a deployment-correlated error, then tested on a near-identical scenario to see if it adapts.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="decision-accuracy">Decision Accuracy<a href="#decision-accuracy" class="hash-link" aria-label="Direct link to Decision Accuracy" title="Direct link to Decision Accuracy">​</a></h3>
<table><thead><tr><th>Round</th><th>Type</th><th>Episodic Agent</th><th>Baseline Agent</th></tr></thead><tbody><tr><td>1</td><td>LEARN</td><td>✗</td><td>✓</td></tr><tr><td>2</td><td>LEARN</td><td>✓</td><td>✓</td></tr><tr><td>3</td><td>RED HERRING</td><td>✗</td><td>✗</td></tr><tr><td>4</td><td>LEARN</td><td>✓</td><td>✓</td></tr><tr><td>5</td><td>TEST</td><td><strong>✓</strong></td><td>✗</td></tr><tr><td>6</td><td>TEST</td><td><strong>✓</strong></td><td>✗</td></tr><tr><td>7</td><td>SUBTLE</td><td><strong>✓</strong></td><td>✗</td></tr><tr><td>8</td><td>CORRECTION</td><td>✓</td><td>✓</td></tr><tr><td>9</td><td>CORRECTION</td><td>✓</td><td>✓</td></tr><tr><td><strong>Total</strong></td><td></td><td><strong>7/9 (78%)</strong></td><td><strong>5/9 (56%)</strong></td></tr></tbody></table>
<p>The episodic agent won 7-5. A 40% relative improvement in decision accuracy using the exact same LLM.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="where-the-gap-opened">Where the Gap Opened<a href="#where-the-gap-opened" class="hash-link" aria-label="Direct link to Where the Gap Opened" title="Direct link to Where the Gap Opened">​</a></h3>
<p>The episodic agent&#x27;s advantage concentrated in exactly the rounds designed to test memory quality:</p>
<p><strong>Rounds 5-6 (pattern application):</strong> The episodic agent cited 4 past failure episodes with connection pool exhaustion as root cause, complete with correction annotations. It correctly identified pool exhaustion in new services. The baseline retrieved disconnected chunks and suggested checking timeout configurations — a pattern it picked up from the Round 3 red herring.</p>
<p><strong>Round 7 (subtle symptoms — latency increase, no errors):</strong> Both agents had the same evidence available. The episodic agent&#x27;s retrieval surfaced a diverse set of episodes (thanks to MMR diversity filtering) including the Redis pool exhaustion from Round 6, which primed it to recognize that latency without errors can still be pool contention. The baseline defaulted to &quot;check recent config changes.&quot;</p>
<p><strong>Round 9 (adaptation after correction):</strong> This is the result we&#x27;re most proud of. Look at the episodic agent&#x27;s reasoning:</p>
<blockquote>
<p><em>&quot;Episode 1 directly parallels this situation — errors spiking immediately after a deployment (v2.4.1 then, v3.1.0 now) with no traffic change. In that case, the root cause was a database migration that dropped an index. The generalized fact confirms that deployment-related issues with immediate onset after version changes are more likely caused by configuration errors or missing dependencies than by connection pool problems.&quot;</em></p>
</blockquote>
<p>It cited a specific past episode by analogy, quoted a generalized fact, and explained <em>why</em> this situation matches the deployment pattern rather than the connection pool pattern. The baseline gave a vaguer assessment.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="retrieval-quality">Retrieval Quality<a href="#retrieval-quality" class="hash-link" aria-label="Direct link to Retrieval Quality" title="Direct link to Retrieval Quality">​</a></h3>
<p>This is where the structural difference is most visible:</p>
<table><thead><tr><th>Metric</th><th>Episodic Agent</th><th>Baseline Agent</th></tr></thead><tbody><tr><td>Retrieved items with explicit outcome labels</td><td><strong>100%</strong></td><td>25%</td></tr><tr><td>Correct pattern applications (Rounds 4-7)</td><td><strong>4/4</strong></td><td>1/4</td></tr><tr><td>False positives (Rounds 8-9)</td><td><strong>0</strong></td><td>0</td></tr></tbody></table>
<p>Every item the episodic agent retrieved carried a structured outcome label (SUCCESS or FAILURE) with correction details. Only 25% of the baseline&#x27;s chunks contained any outcome information — and those were incidental text mentions, not structured labels.</p>
<p>The episodic agent correctly applied the connection pool pattern in all four rounds where it was the root cause, and correctly avoided it in both rounds where it wasn&#x27;t. The baseline applied it correctly once.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-didnt-work">What Didn&#x27;t Work<a href="#what-didnt-work" class="hash-link" aria-label="Direct link to What Didn&#x27;t Work" title="Direct link to What Didn&#x27;t Work">​</a></h2>
<p>Two things didn&#x27;t work as anticipated:</p>
<p><strong>Round 3 (red herring):</strong> Both agents failed. The symptoms looked like connection pool issues, but the root cause was a deployment config change. At this point, the episodic agent had only seen connection pool episodes — it had no counter-evidence for deployment-correlated errors. You can&#x27;t distinguish patterns you&#x27;ve only seen one side of. After Round 8 introduced a correction, the agent successfully avoided this mistake in Round 9.</p>
<p><strong>Fact quality variance.</strong> Some extracted facts were specific and actionable (&quot;Deployment-related issues with immediate onset are more likely configuration errors&quot;). Others were vague (&quot;Initial symptom-based diagnosis often leads to misidentifying the root cause&quot;). A production system needs a usefulness filter, not just a confidence score.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-this-means">What This Means<a href="#what-this-means" class="hash-link" aria-label="Direct link to What This Means" title="Direct link to What This Means">​</a></h2>
<p>The most important finding isn&#x27;t the accuracy improvement. It&#x27;s that the reinforcement loop closes without retraining.</p>
<p>In the POC, we observed:</p>
<ul>
<li>Rounds 1-4: Agent encounters failures, episodes recorded with outcomes and corrections</li>
<li>After Round 4: Fact extracted — &quot;Connection pool exhaustion is a common root cause under load&quot;</li>
<li>Rounds 5-7: Agent applies the pattern with increasing confidence (fact support count grows)</li>
<li>Round 8: Agent encounters a deployment error, correctly identifies it as config, gets corrected</li>
<li>After Round 8: New fact — &quot;Deployment-related issues with immediate onset are more likely configuration errors&quot;</li>
<li>Round 9: Agent receives near-identical scenario, correctly avoids connection pool pattern, cites the Round 8 correction</li>
</ul>
<p>The model didn&#x27;t change. The memory evolved. That&#x27;s the whole point.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-it-compares-to-existing-solutions">How It Compares to Existing Solutions<a href="#how-it-compares-to-existing-solutions" class="hash-link" aria-label="Direct link to How It Compares to Existing Solutions" title="Direct link to How It Compares to Existing Solutions">​</a></h2>
<p>Agent memory is a fast-moving space with several strong systems, each solving a different slice of the problem:</p>
<p><strong>Mem0</strong> excels at persistent personalization — extracting user preferences, managing session context, and reducing token costs through intelligent compression. It&#x27;s the most production-ready memory layer available and integrates with nearly every agent framework. Its focus is on remembering about users and conversations rather than learning from task-level outcomes, which is a different problem than the one we&#x27;re exploring here.</p>
<p><strong>Zep/Graphiti</strong> is doing some of the most interesting work in temporal knowledge graphs. Their bi-temporal model — tracking both when an event occurred and when it was ingested — addresses a real structural gap in how agent memory handles changing facts over time. Their episode and entity subgraphs share some philosophical DNA with our approach. Where our work diverges is in outcome tracking and reinforcement: we&#x27;re specifically focused on whether a decision worked, and using that signal to update memory structure.</p>
<p><strong>Letta (formerly MemGPT)</strong> pioneered self-editing memory — giving the LLM tools to manage its own memory blocks. This is a powerful paradigm, and their recent work on &quot;Context Repositories&quot; and sleep-time compute suggests they&#x27;re actively pushing toward agents that learn over time. Their team has been transparent that experiential learning is an unsolved problem, which is part of what motivated our exploration.</p>
<p><strong>MemRL (Jan 2026 paper)</strong> is the closest to our work academically. It shares the core insight of decoupling stable LLM reasoning from plastic, evolving memory. Their approach uses reinforcement learning to assign utility Q-values to memories, which is elegant but requires training a value function. Our approach is purely structural — no training step, no Q-values, just graph evolution and LLM-based reasoning over outcomes.</p>
<p>The common thread: most existing systems focus on knowledge persistence — remembering facts, preferences, and conversation history across sessions. The problem we&#x27;re exploring is experiential learning — tracking whether past decisions worked, forming causal chains between episodes, and extracting reasoning heuristics that improve over time. These are complementary capabilities that would be needed by an ideal production system.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="try-it-yourself">Try It Yourself<a href="#try-it-yourself" class="hash-link" aria-label="Direct link to Try It Yourself" title="Direct link to Try It Yourself">​</a></h2>
<p>The prototype is available in our experiments directory:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">experiments/episodic-memory-prototype/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── memory/          # Timeline, encoder, episodes, graph, facts, retriever, reinforcer</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── agent/           # Episodic memory agent</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── baseline/        # Flat vector RAG agent (comparison)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── simulator/       # 9-round debugging scenario</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── eval/            # Head-to-head comparison + scoring</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└── tests/</span><br></span></code></pre></div></div>
<p>To run the comparison:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd experiments/episodic-memory-prototype</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">python -m venv .venv &amp;&amp; source .venv/bin/activate</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">pip install -r requirements.txt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export ANTHROPIC_API_KEY=sk-ant-...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">python -m eval.compare</span><br></span></code></pre></div></div>
<p>Without an API key, it runs in heuristic mode (keyword-based decisions). With a key, both agents use Claude Sonnet for reasoning — that&#x27;s where the quality gap becomes visible.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>This is a 9-round synthetic scenario we designed. It demonstrates the poc architecture works end-to-end and shows where episodic memory provides qualitatively different reasoning. It is not a peer-reviewed benchmark and should not be interpreted as a statistically rigorous claim. We&#x27;re publishing the prototype so others can reproduce and extend the evaluation.
If this sparks interest do trigger github discussion.</p>
<hr>
<p><em>The episodic memory prototype is available in <code>BharatMLStack</code> repo at <code>/experiments/episodic-memory-prototype</code></em></p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/ai-agents">ai-agents</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/memory">memory</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/architecture">architecture</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/episodic-memory">episodic-memory</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/Meesho/BharatMLStack/tree/main/docs/blog/bharatmlstack-history/episodic-memory-for-agents/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/BharatMLStack/blog/llm-inference-optimization-sub-sec-latency"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-gap-nobody-talks-about" class="table-of-contents__link toc-highlight">The Gap Nobody Talks About</a></li><li><a href="#whats-wrong-with-vector-rag-as-memory" class="table-of-contents__link toc-highlight">What&#39;s Wrong With Vector RAG as Memory</a></li><li><a href="#the-architecture-episodic-memory" class="table-of-contents__link toc-highlight">The Architecture: Episodic Memory</a><ul><li><a href="#layer-1-immutable-timeline" class="table-of-contents__link toc-highlight">Layer 1: Immutable Timeline</a></li><li><a href="#layer-2-episode-segmentation" class="table-of-contents__link toc-highlight">Layer 2: Episode Segmentation</a></li><li><a href="#layer-3-episodic-graph" class="table-of-contents__link toc-highlight">Layer 3: Episodic Graph</a></li><li><a href="#layer-4-generalized-facts" class="table-of-contents__link toc-highlight">Layer 4: Generalized Facts</a></li><li><a href="#the-reinforcement-loop" class="table-of-contents__link toc-highlight">The Reinforcement Loop</a></li></ul></li><li><a href="#the-experiment" class="table-of-contents__link toc-highlight">The Experiment</a></li><li><a href="#results" class="table-of-contents__link toc-highlight">Results</a><ul><li><a href="#decision-accuracy" class="table-of-contents__link toc-highlight">Decision Accuracy</a></li><li><a href="#where-the-gap-opened" class="table-of-contents__link toc-highlight">Where the Gap Opened</a></li><li><a href="#retrieval-quality" class="table-of-contents__link toc-highlight">Retrieval Quality</a></li></ul></li><li><a href="#what-didnt-work" class="table-of-contents__link toc-highlight">What Didn&#39;t Work</a></li><li><a href="#what-this-means" class="table-of-contents__link toc-highlight">What This Means</a></li><li><a href="#how-it-compares-to-existing-solutions" class="table-of-contents__link toc-highlight">How It Compares to Existing Solutions</a></li><li><a href="#try-it-yourself" class="table-of-contents__link toc-highlight">Try It Yourself</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github Discussions<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discord.gg/XkT7XsV2AU" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/BharatMLStack/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Meesho Ltd. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>