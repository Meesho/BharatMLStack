<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale | BharatMLStack</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://meesho.github.io/BharatMLStack/blog/post-five"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale | BharatMLStack"><meta data-rh="true" name="description" content="BharatMLStack"><meta data-rh="true" property="og:description" content="BharatMLStack"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-06-02T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/jayakommuru"><meta data-rh="true" property="article:tag" content="llm,vllm,tensorrt-llm,mlplatform,meesho,bharatmlstack"><link data-rh="true" rel="icon" href="/BharatMLStack/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://meesho.github.io/BharatMLStack/blog/post-five"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/blog/post-five" hreflang="en"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/blog/post-five" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://meesho.github.io/BharatMLStack/blog/post-five","mainEntityOfPage":"https://meesho.github.io/BharatMLStack/blog/post-five","url":"https://meesho.github.io/BharatMLStack/blog/post-five","headline":"LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale","name":"LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale","description":"BharatMLStack","datePublished":"2025-06-02T00:00:00.000Z","author":{"@type":"Person","name":"Jaya Kumar","description":"Lead ML Engineer @ Meesho","url":"https://github.com/jayakommuru","image":"https://github.com/jayakommuru.png"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://meesho.github.io/BharatMLStack/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/BharatMLStack/blog/rss.xml" title="BharatMLStack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/BharatMLStack/blog/atom.xml" title="BharatMLStack Atom Feed"><link rel="stylesheet" href="/BharatMLStack/assets/css/styles.030f898a.css">
<script src="/BharatMLStack/assets/js/runtime~main.2b9c67a0.js" defer="defer"></script>
<script src="/BharatMLStack/assets/js/main.5b79a858.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="https://github.com/jayakommuru.png"><div class="gradient-bg-global"><div class="gradient-orb-global orb-global-1"></div><div class="gradient-orb-global orb-global-2"></div><div class="gradient-orb-global orb-global-3"></div></div><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/BharatMLStack/"><b class="navbar__title text--truncate">BharatMLStack</b></a><a class="navbar__item navbar__link" href="/BharatMLStack/category/online-feature-store">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/BharatMLStack/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/BharatMLStack/blog/post-five">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-four">Designing a Production-Grade LLM Inference Platform: From Model Weights to Scalable GPU Serving</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-three">Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-two">Building Meesho’s ML Platform: Lessons from the First-Gen System (Part 2)</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-one">Building Meesho’s ML Platform: From Chaos to Cutting-Edge (Part 1)</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-06-02T00:00:00.000Z">June 2, 2025</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/jayakommuru.png" alt="Jaya Kumar"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Jaya Kumar</span></a></div><small class="authorTitle_nd0D" title="Lead ML Engineer @ Meesho">Lead ML Engineer @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p><img decoding="async" loading="lazy" alt="BharatMLStack" src="/BharatMLStack/assets/images/bms-7399e8796d2cd24617c432518ce3f312.png" width="1396" height="460" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-inference-optimization-techniques-engineering-sub-second-latency-at-scale">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale<a href="#llm-inference-optimization-techniques-engineering-sub-second-latency-at-scale" class="hash-link" aria-label="Direct link to LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale" title="Direct link to LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale">​</a></h2>
<p>Raw execution of Large Language Models is inherently expensive and memory-intensive. To achieve sub-second latency and high throughput, we implement a multi-layered optimization strategy that targets the entire inference stack—from memory management to kernel execution.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-advanced-memory-management-paged--prefix-kv-caching">1. Advanced Memory Management: Paged &amp; Prefix KV Caching<a href="#1-advanced-memory-management-paged--prefix-kv-caching" class="hash-link" aria-label="Direct link to 1. Advanced Memory Management: Paged &amp; Prefix KV Caching" title="Direct link to 1. Advanced Memory Management: Paged &amp; Prefix KV Caching">​</a></h2>
<p>The most significant bottleneck in LLM inference is not always compute, but memory bandwidth—specifically managing the Key-Value (KV) cache.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="paged-kv-caching">Paged KV caching<a href="#paged-kv-caching" class="hash-link" aria-label="Direct link to Paged KV caching" title="Direct link to Paged KV caching">​</a></h3>
<p>Standard caching suffers from fragmentation. We use <strong>Paged KV caching</strong>, which operates similarly to an operating system&#x27;s virtual memory: the KV cache is divided into non-contiguous blocks. This lets us serve larger batch sizes without running out of memory.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="kv-cache-quantization">KV cache quantization<a href="#kv-cache-quantization" class="hash-link" aria-label="Direct link to KV cache quantization" title="Direct link to KV cache quantization">​</a></h3>
<p>To further maximize available memory, we implement <strong>KV cache quantization</strong> (e.g., FP8). By compressing stored attention keys and values from 16-bit to 8-bit, we nearly double the effective context window capacity of the GPU, allowing longer conversations or larger batches without materially degrading quality.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prefix-caching-the-voice-bot-optimizer">Prefix caching (the &quot;voice bot&quot; optimizer)<a href="#prefix-caching-the-voice-bot-optimizer" class="hash-link" aria-label="Direct link to Prefix caching (the &quot;voice bot&quot; optimizer)" title="Direct link to Prefix caching (the &quot;voice bot&quot; optimizer)">​</a></h3>
<p>For use cases like GenAI voice bots where the system prompt (e.g., &quot;You are a helpful assistant...&quot;) is static across thousands of requests, we enable <strong>prefix caching</strong>.</p>
<ul>
<li><strong>Impact</strong>: By reusing pre-computed KV states for common prefixes, we achieve a cache hit rate of ~90%. This reduces <strong>Time To First Token (TTFT)</strong> by skipping redundant computation of the system prompt.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-aggressive-quantization-int4-awq--fp8">2. Aggressive Quantization (INT4 AWQ &amp; FP8)<a href="#2-aggressive-quantization-int4-awq--fp8" class="hash-link" aria-label="Direct link to 2. Aggressive Quantization (INT4 AWQ &amp; FP8)" title="Direct link to 2. Aggressive Quantization (INT4 AWQ &amp; FP8)">​</a></h2>
<p>Running models in their native 16-bit precision (BF16) restricts maximum batch size and throughput. We use quantization to shrink model weights without sacrificing accuracy.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="int4-awq-activation-aware-weight-quantization">INT4 AWQ (Activation-aware Weight Quantization)<a href="#int4-awq-activation-aware-weight-quantization" class="hash-link" aria-label="Direct link to INT4 AWQ (Activation-aware Weight Quantization)" title="Direct link to INT4 AWQ (Activation-aware Weight Quantization)">​</a></h3>
<p>For the Llama 3 family, we use <strong>AWQ</strong> to compress weights to 4 bits. This reduces model size by ~75%, allowing larger models to fit into L4 GPU memory and significantly improving token generation speed.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fp8-precision">FP8 precision<a href="#fp8-precision" class="hash-link" aria-label="Direct link to FP8 precision" title="Direct link to FP8 precision">​</a></h3>
<p>For NVIDIA Hopper (H100) architectures, we are exploring <strong>FP8 quantization</strong>, leveraging native FP8 tensor cores to accelerate matrix multiplications while maintaining a higher dynamic range than integer quantization.</p>
<ul>
<li><strong>Verification</strong>: We validate quantized models by comparing dot-product similarity of embeddings against the FP16 baseline, consistently achieving <strong>&gt;99% similarity</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-kernel-fusion--custom-plugins">3. Kernel Fusion &amp; Custom Plugins<a href="#3-kernel-fusion--custom-plugins" class="hash-link" aria-label="Direct link to 3. Kernel Fusion &amp; Custom Plugins" title="Direct link to 3. Kernel Fusion &amp; Custom Plugins">​</a></h2>
<p>To minimize overhead from launching thousands of small GPU operations, we fuse them into monolithic kernels using NVIDIA TensorRT plugins.</p>
<ul>
<li><strong>Flash attention &amp; FMHA</strong>: We enable <strong>Fused Multi-Head Attention (FMHA)</strong> combined with flash attention to reduce memory reads/writes.</li>
<li><strong>GEMM plugins</strong>: We use specialized <strong>GEMM</strong> plugins to accelerate transformer linear layers.</li>
<li><strong>Removing input padding</strong>: Instead of padding short sequences to match the longest, we remove input padding so the GPU processes only valid tokens.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-inflight-continuous-batching">4. Inflight (Continuous) Batching<a href="#4-inflight-continuous-batching" class="hash-link" aria-label="Direct link to 4. Inflight (Continuous) Batching" title="Direct link to 4. Inflight (Continuous) Batching">​</a></h2>
<p>Traditional static batching waits for all requests in a batch to finish before returning results—so one long response delays everyone else.</p>
<p>We implement <strong>inflight batching</strong>: as soon as one request completes, its slot is freed and filled by a new request from the queue. This keeps GPUs saturated and decouples latency of short queries from long ones.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-parallelism-strategies-scaling-beyond-one-gpu">5. Parallelism Strategies: Scaling Beyond One GPU<a href="#5-parallelism-strategies-scaling-beyond-one-gpu" class="hash-link" aria-label="Direct link to 5. Parallelism Strategies: Scaling Beyond One GPU" title="Direct link to 5. Parallelism Strategies: Scaling Beyond One GPU">​</a></h2>
<p>For large models (e.g., 70B+ parameters) that cannot fit into the VRAM of a single GPU, we use parallelism strategies.</p>
<ul>
<li><strong>Tensor parallelism (TP)</strong>: Split weight matrices across multiple GPUs (e.g., 4× L4 or 8× A100). Each GPU computes a shard and outputs are reduced at every layer.</li>
<li><strong>Pipeline parallelism (PP)</strong>: Split model layers across GPUs to pipeline compute (e.g., while one GPU computes later layers for Request A, another starts early layers for Request B).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-speculative-decoding">6. Speculative Decoding<a href="#6-speculative-decoding" class="hash-link" aria-label="Direct link to 6. Speculative Decoding" title="Direct link to 6. Speculative Decoding">​</a></h2>
<p>To reduce inter-token latency (ITL), we explore <strong>speculative decoding</strong>.</p>
<ul>
<li><strong>Mechanism</strong>: A smaller, faster &quot;draft&quot; model speculatively generates a short token sequence (e.g., 5 tokens).</li>
<li><strong>Verification</strong>: The larger target model verifies those tokens in one parallel forward pass. If correct, we effectively generate multiple tokens per large-model step; if not, we discard and regenerate. This is effective for predictable text, improving perceived generation speed.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="few-benchmarks">Few Benchmarks<a href="#few-benchmarks" class="hash-link" aria-label="Direct link to Few Benchmarks" title="Direct link to Few Benchmarks">​</a></h2>
<p>Below are a couple of representative use cases and performance numbers.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="search-query-rewriting">Search query rewriting<a href="#search-query-rewriting" class="hash-link" aria-label="Direct link to Search query rewriting" title="Direct link to Search query rewriting">​</a></h3>
<ul>
<li><strong>LLM</strong>: Fine-tuned llama-3.2-1B</li>
<li><strong>Input &amp; output token length</strong>: ~10–20</li>
<li><strong>Response type</strong>: Non-streaming</li>
</ul>
<table><thead><tr><th>Inference runtime</th><th>Hardware</th><th style="text-align:right">Max requests/sec</th><th style="text-align:right">Max p99 latency</th></tr></thead><tbody><tr><td>TensorRT-LLM</td><td>4 × L4 GPUs (multi-GPU)</td><td style="text-align:right">1000</td><td style="text-align:right">95 ms</td></tr><tr><td>TensorRT-LLM</td><td>1 × A100 40 GB GPU</td><td style="text-align:right">1000</td><td style="text-align:right">69 ms</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="voice-bot-query">Voice bot query<a href="#voice-bot-query" class="hash-link" aria-label="Direct link to Voice bot query" title="Direct link to Voice bot query">​</a></h3>
<ul>
<li><strong>LLM</strong>: Llama-3.1-8B</li>
<li><strong>Input token length</strong>: ~1900–2000</li>
<li><strong>Output token length</strong>: ~200</li>
<li><strong>Response type</strong>: Streaming</li>
</ul>
<table><thead><tr><th>Inference runtime</th><th style="text-align:right">Concurrency</th><th style="text-align:right">p99 TTFT (ms)</th><th style="text-align:right">p99 ITL (ms)</th><th style="text-align:right">Token throughput (tokens/sec)</th><th style="text-align:right">Request throughput (req/sec)</th><th>Hardware</th></tr></thead><tbody><tr><td>TensorRT-LLM</td><td style="text-align:right">1</td><td style="text-align:right">36.27</td><td style="text-align:right">22.78</td><td style="text-align:right">45.66</td><td style="text-align:right">0.23</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">2</td><td style="text-align:right">49.81</td><td style="text-align:right">23.21</td><td style="text-align:right">89.37</td><td style="text-align:right">0.45</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">4</td><td style="text-align:right">55.33</td><td style="text-align:right">36.62</td><td style="text-align:right">153.39</td><td style="text-align:right">0.78</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">8</td><td style="text-align:right">66.5</td><td style="text-align:right">39.11</td><td style="text-align:right">279.88</td><td style="text-align:right">1.47</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">16</td><td style="text-align:right">131.8</td><td style="text-align:right">30.39</td><td style="text-align:right">547.8</td><td style="text-align:right">2.77</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">32</td><td style="text-align:right">277.22</td><td style="text-align:right">48.02</td><td style="text-align:right">925.7</td><td style="text-align:right">4.78</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">64</td><td style="text-align:right">498.52</td><td style="text-align:right">71.62</td><td style="text-align:right">1,164.40</td><td style="text-align:right">6.2</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">128</td><td style="text-align:right">677.31</td><td style="text-align:right">120.37</td><td style="text-align:right">1,445.18</td><td style="text-align:right">7.69</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">256</td><td style="text-align:right">1,926.31</td><td style="text-align:right">216.88</td><td style="text-align:right">1,600.81</td><td style="text-align:right">8.52</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">1</td><td style="text-align:right">21.17</td><td style="text-align:right">9.24</td><td style="text-align:right">130.05</td><td style="text-align:right">0.68</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">2</td><td style="text-align:right">25.78</td><td style="text-align:right">9.21</td><td style="text-align:right">264.5</td><td style="text-align:right">1.35</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">4</td><td style="text-align:right">28.52</td><td style="text-align:right">10.99</td><td style="text-align:right">437.69</td><td style="text-align:right">2.27</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">8</td><td style="text-align:right">34.4</td><td style="text-align:right">12.61</td><td style="text-align:right">760.49</td><td style="text-align:right">3.96</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">16</td><td style="text-align:right">68.03</td><td style="text-align:right">14.32</td><td style="text-align:right">1,343.80</td><td style="text-align:right">7.01</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">32</td><td style="text-align:right">185.96</td><td style="text-align:right">16.82</td><td style="text-align:right">2,287.30</td><td style="text-align:right">11.92</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">64</td><td style="text-align:right">136.87</td><td style="text-align:right">21.17</td><td style="text-align:right">3,625.22</td><td style="text-align:right">18.89</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">128</td><td style="text-align:right">463.78</td><td style="text-align:right">34.15</td><td style="text-align:right">4,456.51</td><td style="text-align:right">23.24</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">256</td><td style="text-align:right">890.12</td><td style="text-align:right">59.18</td><td style="text-align:right">5,188.24</td><td style="text-align:right">27.05</td><td>A100</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>High-performance LLM inference is fundamentally a systems engineering problem: memory efficiency, kernel execution, batching strategy, and parallelism determine real-world latency and throughput. Techniques such as paged KV caching, aggressive quantization, kernel fusion, and inflight batching improve GPU utilization while reducing latency and memory pressure.</p>
<p>These optimizations enable the platform to deliver sub-second responses, sustain high concurrency, and efficiently serve both lightweight and long-context workloads. By continuously optimizing across the full inference stack, we keep LLM serving scalable, cost-efficient, and production-ready for real-time AI applications.</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/vllm">vllm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/tensorrt-llm">tensorrt-llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/mlplatform">mlplatform</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/meesho">meesho</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/bharatmlstack">bharatmlstack</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/Meesho/BharatMLStack/tree/main/docs/blog/bharatmlstack-history/post-five/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/BharatMLStack/blog/post-four"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Designing a Production-Grade LLM Inference Platform: From Model Weights to Scalable GPU Serving</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#llm-inference-optimization-techniques-engineering-sub-second-latency-at-scale" class="table-of-contents__link toc-highlight">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale</a></li><li><a href="#1-advanced-memory-management-paged--prefix-kv-caching" class="table-of-contents__link toc-highlight">1. Advanced Memory Management: Paged &amp; Prefix KV Caching</a><ul><li><a href="#paged-kv-caching" class="table-of-contents__link toc-highlight">Paged KV caching</a></li><li><a href="#kv-cache-quantization" class="table-of-contents__link toc-highlight">KV cache quantization</a></li><li><a href="#prefix-caching-the-voice-bot-optimizer" class="table-of-contents__link toc-highlight">Prefix caching (the &quot;voice bot&quot; optimizer)</a></li></ul></li><li><a href="#2-aggressive-quantization-int4-awq--fp8" class="table-of-contents__link toc-highlight">2. Aggressive Quantization (INT4 AWQ &amp; FP8)</a><ul><li><a href="#int4-awq-activation-aware-weight-quantization" class="table-of-contents__link toc-highlight">INT4 AWQ (Activation-aware Weight Quantization)</a></li><li><a href="#fp8-precision" class="table-of-contents__link toc-highlight">FP8 precision</a></li></ul></li><li><a href="#3-kernel-fusion--custom-plugins" class="table-of-contents__link toc-highlight">3. Kernel Fusion &amp; Custom Plugins</a></li><li><a href="#4-inflight-continuous-batching" class="table-of-contents__link toc-highlight">4. Inflight (Continuous) Batching</a></li><li><a href="#5-parallelism-strategies-scaling-beyond-one-gpu" class="table-of-contents__link toc-highlight">5. Parallelism Strategies: Scaling Beyond One GPU</a></li><li><a href="#6-speculative-decoding" class="table-of-contents__link toc-highlight">6. Speculative Decoding</a></li><li><a href="#few-benchmarks" class="table-of-contents__link toc-highlight">Few Benchmarks</a><ul><li><a href="#search-query-rewriting" class="table-of-contents__link toc-highlight">Search query rewriting</a></li><li><a href="#voice-bot-query" class="table-of-contents__link toc-highlight">Voice bot query</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github Discussions<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discord.gg/XkT7XsV2AU" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/BharatMLStack/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Meesho Ltd. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>