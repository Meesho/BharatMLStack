<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">4 posts tagged with &quot;bharatmlstack&quot; | BharatMLStack</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://meesho.github.io/BharatMLStack/blog/tags/bharatmlstack"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="4 posts tagged with &quot;bharatmlstack&quot; | BharatMLStack"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/BharatMLStack/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://meesho.github.io/BharatMLStack/blog/tags/bharatmlstack"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/blog/tags/bharatmlstack" hreflang="en"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/blog/tags/bharatmlstack" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/BharatMLStack/blog/rss.xml" title="BharatMLStack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/BharatMLStack/blog/atom.xml" title="BharatMLStack Atom Feed"><link rel="stylesheet" href="/BharatMLStack/assets/css/styles.14b2d0af.css">
<script src="/BharatMLStack/assets/js/runtime~main.d5e46064.js" defer="defer"></script>
<script src="/BharatMLStack/assets/js/main.3e15e71d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="https://github.com/jayakommuru.png"><link rel="preload" as="image" href="https://github.com/Adit2607.png"><link rel="preload" as="image" href="https://github.com/a0d00kc.png"><link rel="preload" as="image" href="https://github.com/singh-bhawani.png"><link rel="preload" as="image" href="https://github.com/jigarpatel26.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/BharatMLStack/"><b class="navbar__title text--truncate">BharatMLStack</b></a><a class="navbar__item navbar__link" href="/BharatMLStack/category/online-feature-store">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/BharatMLStack/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-five">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-three">Designing a Production-Grade LLM Inference Platform: From Model Weights to Scalable GPU Serving</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-three">Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-two">Building Meesho‚Äôs ML Platform: Lessons from the First-Gen System (Part 2)</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-one">Building Meesho‚Äôs ML Platform: From Chaos to Cutting-Edge (Part 1)</a></li></ul></div></nav></aside><main class="col col--7"><header class="margin-bottom--xl"><h1>4 posts tagged with &quot;bharatmlstack&quot;</h1><a href="/BharatMLStack/blog/tags">View All Tags</a></header><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/BharatMLStack/blog/post-five">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-02T00:00:00.000Z">June 2, 2025</time> ¬∑ <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/jayakommuru.png" alt="Jaya Kumar"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Jaya Kumar</span></a></div><small class="authorTitle_nd0D" title="Lead ML Engineer @ Meesho">Lead ML Engineer @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><img decoding="async" loading="lazy" alt="BharatMLStack" src="/BharatMLStack/assets/images/bms-7399e8796d2cd24617c432518ce3f312.png" width="1396" height="460" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-inference-optimization-techniques-engineering-sub-second-latency-at-scale">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale<a href="#llm-inference-optimization-techniques-engineering-sub-second-latency-at-scale" class="hash-link" aria-label="Direct link to LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale" title="Direct link to LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale">‚Äã</a></h2>
<p>Raw execution of Large Language Models is inherently expensive and memory-intensive. To achieve sub-second latency and high throughput, we implement a multi-layered optimization strategy that targets the entire inference stack‚Äîfrom memory management to kernel execution.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-advanced-memory-management-paged--prefix-kv-caching">1. Advanced Memory Management: Paged &amp; Prefix KV Caching<a href="#1-advanced-memory-management-paged--prefix-kv-caching" class="hash-link" aria-label="Direct link to 1. Advanced Memory Management: Paged &amp; Prefix KV Caching" title="Direct link to 1. Advanced Memory Management: Paged &amp; Prefix KV Caching">‚Äã</a></h2>
<p>The most significant bottleneck in LLM inference is not always compute, but memory bandwidth‚Äîspecifically managing the Key-Value (KV) cache.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="paged-kv-caching">Paged KV caching<a href="#paged-kv-caching" class="hash-link" aria-label="Direct link to Paged KV caching" title="Direct link to Paged KV caching">‚Äã</a></h3>
<p>Standard caching suffers from fragmentation. We use <strong>Paged KV caching</strong>, which operates similarly to an operating system&#x27;s virtual memory: the KV cache is divided into non-contiguous blocks. This lets us serve larger batch sizes without running out of memory.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="kv-cache-quantization">KV cache quantization<a href="#kv-cache-quantization" class="hash-link" aria-label="Direct link to KV cache quantization" title="Direct link to KV cache quantization">‚Äã</a></h3>
<p>To further maximize available memory, we implement <strong>KV cache quantization</strong> (e.g., FP8). By compressing stored attention keys and values from 16-bit to 8-bit, we nearly double the effective context window capacity of the GPU, allowing longer conversations or larger batches without materially degrading quality.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prefix-caching-the-voice-bot-optimizer">Prefix caching (the &quot;voice bot&quot; optimizer)<a href="#prefix-caching-the-voice-bot-optimizer" class="hash-link" aria-label="Direct link to Prefix caching (the &quot;voice bot&quot; optimizer)" title="Direct link to Prefix caching (the &quot;voice bot&quot; optimizer)">‚Äã</a></h3>
<p>For use cases like GenAI voice bots where the system prompt (e.g., &quot;You are a helpful assistant...&quot;) is static across thousands of requests, we enable <strong>prefix caching</strong>.</p>
<ul>
<li><strong>Impact</strong>: By reusing pre-computed KV states for common prefixes, we achieve a cache hit rate of ~90%. This reduces <strong>Time To First Token (TTFT)</strong> by skipping redundant computation of the system prompt.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-aggressive-quantization-int4-awq--fp8">2. Aggressive Quantization (INT4 AWQ &amp; FP8)<a href="#2-aggressive-quantization-int4-awq--fp8" class="hash-link" aria-label="Direct link to 2. Aggressive Quantization (INT4 AWQ &amp; FP8)" title="Direct link to 2. Aggressive Quantization (INT4 AWQ &amp; FP8)">‚Äã</a></h2>
<p>Running models in their native 16-bit precision (BF16) restricts maximum batch size and throughput. We use quantization to shrink model weights without sacrificing accuracy.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="int4-awq-activation-aware-weight-quantization">INT4 AWQ (Activation-aware Weight Quantization)<a href="#int4-awq-activation-aware-weight-quantization" class="hash-link" aria-label="Direct link to INT4 AWQ (Activation-aware Weight Quantization)" title="Direct link to INT4 AWQ (Activation-aware Weight Quantization)">‚Äã</a></h3>
<p>For the Llama 3 family, we use <strong>AWQ</strong> to compress weights to 4 bits. This reduces model size by ~75%, allowing larger models to fit into L4 GPU memory and significantly improving token generation speed.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fp8-precision">FP8 precision<a href="#fp8-precision" class="hash-link" aria-label="Direct link to FP8 precision" title="Direct link to FP8 precision">‚Äã</a></h3>
<p>For NVIDIA Hopper (H100) architectures, we are exploring <strong>FP8 quantization</strong>, leveraging native FP8 tensor cores to accelerate matrix multiplications while maintaining a higher dynamic range than integer quantization.</p>
<ul>
<li><strong>Verification</strong>: We validate quantized models by comparing dot-product similarity of embeddings against the FP16 baseline, consistently achieving <strong>&gt;99% similarity</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-kernel-fusion--custom-plugins">3. Kernel Fusion &amp; Custom Plugins<a href="#3-kernel-fusion--custom-plugins" class="hash-link" aria-label="Direct link to 3. Kernel Fusion &amp; Custom Plugins" title="Direct link to 3. Kernel Fusion &amp; Custom Plugins">‚Äã</a></h2>
<p>To minimize overhead from launching thousands of small GPU operations, we fuse them into monolithic kernels using NVIDIA TensorRT plugins.</p>
<ul>
<li><strong>Flash attention &amp; FMHA</strong>: We enable <strong>Fused Multi-Head Attention (FMHA)</strong> combined with flash attention to reduce memory reads/writes.</li>
<li><strong>GEMM plugins</strong>: We use specialized <strong>GEMM</strong> plugins to accelerate transformer linear layers.</li>
<li><strong>Removing input padding</strong>: Instead of padding short sequences to match the longest, we remove input padding so the GPU processes only valid tokens.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-inflight-continuous-batching">4. Inflight (Continuous) Batching<a href="#4-inflight-continuous-batching" class="hash-link" aria-label="Direct link to 4. Inflight (Continuous) Batching" title="Direct link to 4. Inflight (Continuous) Batching">‚Äã</a></h2>
<p>Traditional static batching waits for all requests in a batch to finish before returning results‚Äîso one long response delays everyone else.</p>
<p>We implement <strong>inflight batching</strong>: as soon as one request completes, its slot is freed and filled by a new request from the queue. This keeps GPUs saturated and decouples latency of short queries from long ones.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-parallelism-strategies-scaling-beyond-one-gpu">5. Parallelism Strategies: Scaling Beyond One GPU<a href="#5-parallelism-strategies-scaling-beyond-one-gpu" class="hash-link" aria-label="Direct link to 5. Parallelism Strategies: Scaling Beyond One GPU" title="Direct link to 5. Parallelism Strategies: Scaling Beyond One GPU">‚Äã</a></h2>
<p>For large models (e.g., 70B+ parameters) that cannot fit into the VRAM of a single GPU, we use parallelism strategies.</p>
<ul>
<li><strong>Tensor parallelism (TP)</strong>: Split weight matrices across multiple GPUs (e.g., 4√ó L4 or 8√ó A100). Each GPU computes a shard and outputs are reduced at every layer.</li>
<li><strong>Pipeline parallelism (PP)</strong>: Split model layers across GPUs to pipeline compute (e.g., while one GPU computes later layers for Request A, another starts early layers for Request B).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-speculative-decoding">6. Speculative Decoding<a href="#6-speculative-decoding" class="hash-link" aria-label="Direct link to 6. Speculative Decoding" title="Direct link to 6. Speculative Decoding">‚Äã</a></h2>
<p>To reduce inter-token latency (ITL), we explore <strong>speculative decoding</strong>.</p>
<ul>
<li><strong>Mechanism</strong>: A smaller, faster &quot;draft&quot; model speculatively generates a short token sequence (e.g., 5 tokens).</li>
<li><strong>Verification</strong>: The larger target model verifies those tokens in one parallel forward pass. If correct, we effectively generate multiple tokens per large-model step; if not, we discard and regenerate. This is effective for predictable text, improving perceived generation speed.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="few-benchmarks">Few Benchmarks<a href="#few-benchmarks" class="hash-link" aria-label="Direct link to Few Benchmarks" title="Direct link to Few Benchmarks">‚Äã</a></h2>
<p>Below are a couple of representative use cases and performance numbers.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="search-query-rewriting">Search query rewriting<a href="#search-query-rewriting" class="hash-link" aria-label="Direct link to Search query rewriting" title="Direct link to Search query rewriting">‚Äã</a></h3>
<ul>
<li><strong>LLM</strong>: Fine-tuned llama-3.2-1B</li>
<li><strong>Input &amp; output token length</strong>: ~10‚Äì20</li>
<li><strong>Response type</strong>: Non-streaming</li>
</ul>
<table><thead><tr><th>Inference runtime</th><th>Hardware</th><th style="text-align:right">Max requests/sec</th><th style="text-align:right">Max p99 latency</th></tr></thead><tbody><tr><td>TensorRT-LLM</td><td>4 √ó L4 GPUs (multi-GPU)</td><td style="text-align:right">1000</td><td style="text-align:right">95 ms</td></tr><tr><td>TensorRT-LLM</td><td>1 √ó A100 40 GB GPU</td><td style="text-align:right">1000</td><td style="text-align:right">69 ms</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="voice-bot-query">Voice bot query<a href="#voice-bot-query" class="hash-link" aria-label="Direct link to Voice bot query" title="Direct link to Voice bot query">‚Äã</a></h3>
<ul>
<li><strong>LLM</strong>: Llama-3.1-8B</li>
<li><strong>Input token length</strong>: ~1900‚Äì2000</li>
<li><strong>Output token length</strong>: ~200</li>
<li><strong>Response type</strong>: Streaming</li>
</ul>
<table><thead><tr><th>Inference runtime</th><th style="text-align:right">Concurrency</th><th style="text-align:right">p99 TTFT (ms)</th><th style="text-align:right">p99 ITL (ms)</th><th style="text-align:right">Token throughput (tokens/sec)</th><th style="text-align:right">Request throughput (req/sec)</th><th>Hardware</th></tr></thead><tbody><tr><td>TensorRT-LLM</td><td style="text-align:right">1</td><td style="text-align:right">36.27</td><td style="text-align:right">22.78</td><td style="text-align:right">45.66</td><td style="text-align:right">0.23</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">2</td><td style="text-align:right">49.81</td><td style="text-align:right">23.21</td><td style="text-align:right">89.37</td><td style="text-align:right">0.45</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">4</td><td style="text-align:right">55.33</td><td style="text-align:right">36.62</td><td style="text-align:right">153.39</td><td style="text-align:right">0.78</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">8</td><td style="text-align:right">66.5</td><td style="text-align:right">39.11</td><td style="text-align:right">279.88</td><td style="text-align:right">1.47</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">16</td><td style="text-align:right">131.8</td><td style="text-align:right">30.39</td><td style="text-align:right">547.8</td><td style="text-align:right">2.77</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">32</td><td style="text-align:right">277.22</td><td style="text-align:right">48.02</td><td style="text-align:right">925.7</td><td style="text-align:right">4.78</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">64</td><td style="text-align:right">498.52</td><td style="text-align:right">71.62</td><td style="text-align:right">1,164.40</td><td style="text-align:right">6.2</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">128</td><td style="text-align:right">677.31</td><td style="text-align:right">120.37</td><td style="text-align:right">1,445.18</td><td style="text-align:right">7.69</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">256</td><td style="text-align:right">1,926.31</td><td style="text-align:right">216.88</td><td style="text-align:right">1,600.81</td><td style="text-align:right">8.52</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">1</td><td style="text-align:right">21.17</td><td style="text-align:right">9.24</td><td style="text-align:right">130.05</td><td style="text-align:right">0.68</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">2</td><td style="text-align:right">25.78</td><td style="text-align:right">9.21</td><td style="text-align:right">264.5</td><td style="text-align:right">1.35</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">4</td><td style="text-align:right">28.52</td><td style="text-align:right">10.99</td><td style="text-align:right">437.69</td><td style="text-align:right">2.27</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">8</td><td style="text-align:right">34.4</td><td style="text-align:right">12.61</td><td style="text-align:right">760.49</td><td style="text-align:right">3.96</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">16</td><td style="text-align:right">68.03</td><td style="text-align:right">14.32</td><td style="text-align:right">1,343.80</td><td style="text-align:right">7.01</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">32</td><td style="text-align:right">185.96</td><td style="text-align:right">16.82</td><td style="text-align:right">2,287.30</td><td style="text-align:right">11.92</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">64</td><td style="text-align:right">136.87</td><td style="text-align:right">21.17</td><td style="text-align:right">3,625.22</td><td style="text-align:right">18.89</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">128</td><td style="text-align:right">463.78</td><td style="text-align:right">34.15</td><td style="text-align:right">4,456.51</td><td style="text-align:right">23.24</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">256</td><td style="text-align:right">890.12</td><td style="text-align:right">59.18</td><td style="text-align:right">5,188.24</td><td style="text-align:right">27.05</td><td>A100</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2>
<p>High-performance LLM inference is fundamentally a systems engineering problem: memory efficiency, kernel execution, batching strategy, and parallelism determine real-world latency and throughput. Techniques such as paged KV caching, aggressive quantization, kernel fusion, and inflight batching improve GPU utilization while reducing latency and memory pressure.</p>
<p>These optimizations enable the platform to deliver sub-second responses, sustain high concurrency, and efficiently serve both lightweight and long-context workloads. By continuously optimizing across the full inference stack, we keep LLM serving scalable, cost-efficient, and production-ready for real-time AI applications.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/vllm">vllm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/tensorrt-llm">tensorrt-llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/mlplatform">mlplatform</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/meesho">meesho</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/bharatmlstack">bharatmlstack</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/BharatMLStack/blog/post-three">Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-05-21T00:00:00.000Z">May 21, 2024</time> ¬∑ <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/Adit2607" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/Adit2607.png" alt="Aditya Kumar"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/Adit2607" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Aditya Kumar</span></a></div><small class="authorTitle_nd0D" title="Lead Software Engineer  @ Meesho">Lead Software Engineer  @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/jayakommuru.png" alt="Jaya Kumar"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Jaya Kumar</span></a></div><small class="authorTitle_nd0D" title="Lead ML Engineer @ Meesho">Lead ML Engineer @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/a0d00kc" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/a0d00kc.png" alt="Adarsha Das"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/a0d00kc" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Adarsha Das</span></a></div><small class="authorTitle_nd0D" title="Senior Architect @ Meesho">Senior Architect @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><img decoding="async" loading="lazy" alt="BharatMLStack" src="/BharatMLStack/assets/images/bms-7399e8796d2cd24617c432518ce3f312.png" width="1396" height="460" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cracking-the-code-scaling-model-inference--real-time-embedding-search">Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search<a href="#cracking-the-code-scaling-model-inference--real-time-embedding-search" class="hash-link" aria-label="Direct link to Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search" title="Direct link to Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search">‚Äã</a></h2>
<p>By mid-2023, we had transformed our ML stack‚Äîbuilding a real-time feature store, optimizing model retrieval, and fine-tuning ranking. But two critical gaps remained:</p>
<ul>
<li>üîπ Scaling model inference without hitting infrastructure roadblocks</li>
<li>üîπ Moving embedding search from batch to real-time for candidate generation</li>
</ul>
<p>Here‚Äôs how we tackled these last-mile challenges, broke free from infrastructure constraints, and built a cost-efficient, high-performance system.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="breaking-free-from-the-scalability-ceiling">Breaking Free from the Scalability Ceiling<a href="#breaking-free-from-the-scalability-ceiling" class="hash-link" aria-label="Direct link to Breaking Free from the Scalability Ceiling" title="Direct link to Breaking Free from the Scalability Ceiling">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-model-serving-bottlenecka-wake-up-call">The Model Serving Bottleneck‚ÄîA Wake-Up Call<a href="#the-model-serving-bottlenecka-wake-up-call" class="hash-link" aria-label="Direct link to The Model Serving Bottleneck‚ÄîA Wake-Up Call" title="Direct link to The Model Serving Bottleneck‚ÄîA Wake-Up Call">‚Äã</a></h3>
<p>July 2023. With just months left for the Mega Blockbuster Sale (MBS), we noticed a serious issue‚Äîscaling our model-serving infrastructure was taking 10‚Äì15 minutes. In real-time ML, that‚Äôs an eternity.
In one of our war rooms, we ran a quick experiment:</p>
<ul>
<li>üöÄ We deployed an XGBoost model on a self-hosted Triton Inference Server running on a 16-core machine.</li>
<li>üöÄ Fired requests and compared the outputs with our existing cloud-hosted setup.</li>
<li>üöÄ The results matched‚Äîperfectly.</li>
</ul>
<p>That moment changed everything. We prepped a backup Triton setup on EKS, just in case our cloud provider couldn&#x27;t allocate enough compute resources in time. Luckily, they did‚Äîbut the seed was planted.
Then in October, just two weeks before MBS, we got an alarming response from our infrastructure team:
&quot;Node availability may be an issue.&quot;
With no time to waste, we moved 30% of real-time ML traffic to our self-hosted Triton cluster. The results?</p>
<ul>
<li>‚úÖ p99 latency dropped from 90‚Äì100ms to 30‚Äì40ms</li>
<li>‚úÖ Triton handled significantly higher throughput on fewer resources</li>
<li>‚úÖ No model changes were needed</li>
</ul>
<p>MBS ran without a hitch, proving that self-hosted inference was the way forward.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-triton-on-gke">Scaling Triton on GKE<a href="#scaling-triton-on-gke" class="hash-link" aria-label="Direct link to Scaling Triton on GKE" title="Direct link to Scaling Triton on GKE">‚Äã</a></h3>
<p>This left us with two choices:</p>
<ul>
<li>1Ô∏è‚É£ Port models to a managed cloud inference service, investing time in learning a new deployment stack</li>
<li>2Ô∏è‚É£ Scale our existing Triton setup on GKE, optimizing for cost and performance</li>
</ul>
<p>We went with Option 2‚Äîand it slashed inference costs to 35% of what we previously paid, while giving us full control over scaling and optimizations.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fixing-the-cold-start-problem">Fixing the Cold Start Problem<a href="#fixing-the-cold-start-problem" class="hash-link" aria-label="Direct link to Fixing the Cold Start Problem" title="Direct link to Fixing the Cold Start Problem">‚Äã</a></h3>
<p>As we onboarded more deep learning (DL) models, we hit a new bottleneck, new inference pods took 7‚Äì9 minutes to spin up.</p>
<p>After profiling, we found the culprits:</p>
<ul>
<li>Triton‚Äôs base image‚Äîa massive 5GB</li>
<li>Model binaries‚Äîoften 1GB+</li>
<li>Startup delay‚Äîmostly due to downloading and initializing these assets</li>
</ul>
<p>To fix this, we built a lightweight Triton image, stripping unused components and shrinking the size to 900MB. This cut cold start times drastically, making auto-scaling faster and smoother.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="embedding-search-the-last-piece-of-the-puzzle">Embedding Search: The Last Piece of the Puzzle<a href="#embedding-search-the-last-piece-of-the-puzzle" class="hash-link" aria-label="Direct link to Embedding Search: The Last Piece of the Puzzle" title="Direct link to Embedding Search: The Last Piece of the Puzzle">‚Äã</a></h2>
<p>By mid-2023, most of our ML stack had gone real-time‚Äîexcept for Candidate Generation (CG), which still ran in batch mode. To truly power real-time recommendations, we needed an online embedding search system.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="choosing-the-right-vector-database">Choosing the Right Vector Database<a href="#choosing-the-right-vector-database" class="hash-link" aria-label="Direct link to Choosing the Right Vector Database" title="Direct link to Choosing the Right Vector Database">‚Äã</a></h3>
<p>We benchmarked three production-ready vector DBs across key parameters:</p>
<ul>
<li>Milvus</li>
<li>Qdrant</li>
<li>Weaviate</li>
</ul>
<p>After extensive POCs, Qdrant stood out for its:</p>
<ul>
<li>‚úÖ Blazing-fast search latency on high-dimensional vectors</li>
<li>‚úÖ Efficient memory usage, crucial for in-memory workloads</li>
<li>‚úÖ Support for upserts and soft deletes, vital for Ads use cases</li>
<li>‚úÖ gRPC + REST APIs, making integration seamless</li>
<li>‚úÖ Powerful filtering, allowing fine-tuned retrieval (e.g., filtering Ads by category, active status, etc.)</li>
</ul>
<p>At its core, Qdrant uses HNSW indexing, delivering both high recall and low-latency nearest-neighbor search‚Äîa perfect fit for our needs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="embedding-freshness--real-time-updates">Embedding Freshness &amp; Real-Time Updates<a href="#embedding-freshness--real-time-updates" class="hash-link" aria-label="Direct link to Embedding Freshness &amp; Real-Time Updates" title="Direct link to Embedding Freshness &amp; Real-Time Updates">‚Äã</a></h3>
<p>To ensure embeddings stayed up to date, we built a dual ingestion pipeline:</p>
<ul>
<li>üìå Daily Refresh: A bulk pipeline updated embeddings overnight</li>
<li>üìå Real-Time Updates: Ads events triggered immediate upserts/deletes</li>
</ul>
<p>This setup powered real-time &quot;Similar Products&quot; recommendations on the product page and became the foundation for Ads Candidate Generation, ensuring the right ads surfaced in milliseconds.</p>
<p><img decoding="async" loading="lazy" alt="Skye" src="/BharatMLStack/assets/images/vss-c482f6eac4c68b3219e4c562a6b717ec.png" width="1260" height="644" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-takeaways-scaling-smartly-for-real-time-ml">Final Takeaways: Scaling Smartly for Real-Time ML<a href="#final-takeaways-scaling-smartly-for-real-time-ml" class="hash-link" aria-label="Direct link to Final Takeaways: Scaling Smartly for Real-Time ML" title="Direct link to Final Takeaways: Scaling Smartly for Real-Time ML">‚Äã</a></h2>
<ul>
<li>üöÄ Self-hosted inference on Triton gave us lower cost, faster scaling, and better performance than managed services</li>
<li>üöÄ Building a custom Triton image reduced cold starts, improving responsiveness</li>
<li>üöÄ Qdrant-based embedding search enabled real-time personalization at scale</li>
<li>üöÄ Real-time updates for embeddings unlocked dynamic, up-to-date recommendations</li>
</ul>
<p>By early 2024, Meesho‚Äôs ML stack had evolved into a fully real-time, scalable, and cost-efficient system, setting the foundation for even bigger leaps ahead.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/model-inference">model-inference</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/embedding-search">embedding-search</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/mlplatform">mlplatform</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/meesho">meesho</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/bharatmlstack">bharatmlstack</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/BharatMLStack/blog/post-three">Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-05-21T00:00:00.000Z">May 21, 2024</time> ¬∑ <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/Adit2607" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/Adit2607.png" alt="Aditya Kumar"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/Adit2607" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Aditya Kumar</span></a></div><small class="authorTitle_nd0D" title="Lead Software Engineer  @ Meesho">Lead Software Engineer  @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/jayakommuru.png" alt="Jaya Kumar"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Jaya Kumar</span></a></div><small class="authorTitle_nd0D" title="Lead ML Engineer @ Meesho">Lead ML Engineer @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/a0d00kc" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/a0d00kc.png" alt="Adarsha Das"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/a0d00kc" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Adarsha Das</span></a></div><small class="authorTitle_nd0D" title="Senior Architect @ Meesho">Senior Architect @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><img decoding="async" loading="lazy" alt="BharatMLStack" src="/BharatMLStack/assets/images/bms-7399e8796d2cd24617c432518ce3f312.png" width="1396" height="460" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cracking-the-code-scaling-model-inference--real-time-embedding-search">Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search<a href="#cracking-the-code-scaling-model-inference--real-time-embedding-search" class="hash-link" aria-label="Direct link to Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search" title="Direct link to Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search">‚Äã</a></h2>
<p>By mid-2023, we had transformed our ML stack‚Äîbuilding a real-time feature store, optimizing model retrieval, and fine-tuning ranking. But two critical gaps remained:</p>
<ul>
<li>üîπ Scaling model inference without hitting infrastructure roadblocks</li>
<li>üîπ Moving embedding search from batch to real-time for candidate generation</li>
</ul>
<p>Here‚Äôs how we tackled these last-mile challenges, broke free from infrastructure constraints, and built a cost-efficient, high-performance system.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="breaking-free-from-the-scalability-ceiling">Breaking Free from the Scalability Ceiling<a href="#breaking-free-from-the-scalability-ceiling" class="hash-link" aria-label="Direct link to Breaking Free from the Scalability Ceiling" title="Direct link to Breaking Free from the Scalability Ceiling">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-model-serving-bottlenecka-wake-up-call">The Model Serving Bottleneck‚ÄîA Wake-Up Call<a href="#the-model-serving-bottlenecka-wake-up-call" class="hash-link" aria-label="Direct link to The Model Serving Bottleneck‚ÄîA Wake-Up Call" title="Direct link to The Model Serving Bottleneck‚ÄîA Wake-Up Call">‚Äã</a></h3>
<p>July 2023. With just months left for the Mega Blockbuster Sale (MBS), we noticed a serious issue‚Äîscaling our model-serving infrastructure was taking 10‚Äì15 minutes. In real-time ML, that‚Äôs an eternity.
In one of our war rooms, we ran a quick experiment:</p>
<ul>
<li>üöÄ We deployed an XGBoost model on a self-hosted Triton Inference Server running on a 16-core machine.</li>
<li>üöÄ Fired requests and compared the outputs with our existing cloud-hosted setup.</li>
<li>üöÄ The results matched‚Äîperfectly.</li>
</ul>
<p>That moment changed everything. We prepped a backup Triton setup on EKS, just in case our cloud provider couldn&#x27;t allocate enough compute resources in time. Luckily, they did‚Äîbut the seed was planted.
Then in October, just two weeks before MBS, we got an alarming response from our infrastructure team:
&quot;Node availability may be an issue.&quot;
With no time to waste, we moved 30% of real-time ML traffic to our self-hosted Triton cluster. The results?</p>
<ul>
<li>‚úÖ p99 latency dropped from 90‚Äì100ms to 30‚Äì40ms</li>
<li>‚úÖ Triton handled significantly higher throughput on fewer resources</li>
<li>‚úÖ No model changes were needed</li>
</ul>
<p>MBS ran without a hitch, proving that self-hosted inference was the way forward.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-triton-on-gke">Scaling Triton on GKE<a href="#scaling-triton-on-gke" class="hash-link" aria-label="Direct link to Scaling Triton on GKE" title="Direct link to Scaling Triton on GKE">‚Äã</a></h3>
<p>This left us with two choices:</p>
<ul>
<li>1Ô∏è‚É£ Port models to a managed cloud inference service, investing time in learning a new deployment stack</li>
<li>2Ô∏è‚É£ Scale our existing Triton setup on GKE, optimizing for cost and performance</li>
</ul>
<p>We went with Option 2‚Äîand it slashed inference costs to 35% of what we previously paid, while giving us full control over scaling and optimizations.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fixing-the-cold-start-problem">Fixing the Cold Start Problem<a href="#fixing-the-cold-start-problem" class="hash-link" aria-label="Direct link to Fixing the Cold Start Problem" title="Direct link to Fixing the Cold Start Problem">‚Äã</a></h3>
<p>As we onboarded more deep learning (DL) models, we hit a new bottleneck, new inference pods took 7‚Äì9 minutes to spin up.</p>
<p>After profiling, we found the culprits:</p>
<ul>
<li>Triton‚Äôs base image‚Äîa massive 5GB</li>
<li>Model binaries‚Äîoften 1GB+</li>
<li>Startup delay‚Äîmostly due to downloading and initializing these assets</li>
</ul>
<p>To fix this, we built a lightweight Triton image, stripping unused components and shrinking the size to 900MB. This cut cold start times drastically, making auto-scaling faster and smoother.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="embedding-search-the-last-piece-of-the-puzzle">Embedding Search: The Last Piece of the Puzzle<a href="#embedding-search-the-last-piece-of-the-puzzle" class="hash-link" aria-label="Direct link to Embedding Search: The Last Piece of the Puzzle" title="Direct link to Embedding Search: The Last Piece of the Puzzle">‚Äã</a></h2>
<p>By mid-2023, most of our ML stack had gone real-time‚Äîexcept for Candidate Generation (CG), which still ran in batch mode. To truly power real-time recommendations, we needed an online embedding search system.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="choosing-the-right-vector-database">Choosing the Right Vector Database<a href="#choosing-the-right-vector-database" class="hash-link" aria-label="Direct link to Choosing the Right Vector Database" title="Direct link to Choosing the Right Vector Database">‚Äã</a></h3>
<p>We benchmarked three production-ready vector DBs across key parameters:</p>
<ul>
<li>Milvus</li>
<li>Qdrant</li>
<li>Weaviate</li>
</ul>
<p>After extensive POCs, Qdrant stood out for its:</p>
<ul>
<li>‚úÖ Blazing-fast search latency on high-dimensional vectors</li>
<li>‚úÖ Efficient memory usage, crucial for in-memory workloads</li>
<li>‚úÖ Support for upserts and soft deletes, vital for Ads use cases</li>
<li>‚úÖ gRPC + REST APIs, making integration seamless</li>
<li>‚úÖ Powerful filtering, allowing fine-tuned retrieval (e.g., filtering Ads by category, active status, etc.)</li>
</ul>
<p>At its core, Qdrant uses HNSW indexing, delivering both high recall and low-latency nearest-neighbor search‚Äîa perfect fit for our needs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="embedding-freshness--real-time-updates">Embedding Freshness &amp; Real-Time Updates<a href="#embedding-freshness--real-time-updates" class="hash-link" aria-label="Direct link to Embedding Freshness &amp; Real-Time Updates" title="Direct link to Embedding Freshness &amp; Real-Time Updates">‚Äã</a></h3>
<p>To ensure embeddings stayed up to date, we built a dual ingestion pipeline:</p>
<ul>
<li>üìå Daily Refresh: A bulk pipeline updated embeddings overnight</li>
<li>üìå Real-Time Updates: Ads events triggered immediate upserts/deletes</li>
</ul>
<p>This setup powered real-time &quot;Similar Products&quot; recommendations on the product page and became the foundation for Ads Candidate Generation, ensuring the right ads surfaced in milliseconds.</p>
<p><img decoding="async" loading="lazy" alt="Skye" src="/BharatMLStack/assets/images/vss-c482f6eac4c68b3219e4c562a6b717ec.png" width="1260" height="644" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-takeaways-scaling-smartly-for-real-time-ml">Final Takeaways: Scaling Smartly for Real-Time ML<a href="#final-takeaways-scaling-smartly-for-real-time-ml" class="hash-link" aria-label="Direct link to Final Takeaways: Scaling Smartly for Real-Time ML" title="Direct link to Final Takeaways: Scaling Smartly for Real-Time ML">‚Äã</a></h2>
<ul>
<li>üöÄ Self-hosted inference on Triton gave us lower cost, faster scaling, and better performance than managed services</li>
<li>üöÄ Building a custom Triton image reduced cold starts, improving responsiveness</li>
<li>üöÄ Qdrant-based embedding search enabled real-time personalization at scale</li>
<li>üöÄ Real-time updates for embeddings unlocked dynamic, up-to-date recommendations</li>
</ul>
<p>By early 2024, Meesho‚Äôs ML stack had evolved into a fully real-time, scalable, and cost-efficient system, setting the foundation for even bigger leaps ahead.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/model-inference">model-inference</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/embedding-search">embedding-search</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/mlplatform">mlplatform</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/meesho">meesho</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/bharatmlstack">bharatmlstack</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/BharatMLStack/blog/post-two">Building Meesho‚Äôs ML Platform: Lessons from the First-Gen System (Part 2)</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-04-10T00:00:00.000Z">April 10, 2023</time> ¬∑ <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/singh-bhawani" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/singh-bhawani.png" alt="Bhawani Singh"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/singh-bhawani" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Bhawani Singh</span></a></div><small class="authorTitle_nd0D" title="Architect @ Meesho">Architect @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/jigarpatel26" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/jigarpatel26.png" alt="Jigar Dave"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/jigarpatel26" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Jigar Dave</span></a></div><small class="authorTitle_nd0D" title="Lead Software Engineer @ Meesho">Lead Software Engineer @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/a0d00kc" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/a0d00kc.png" alt="Adarsha Das"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/a0d00kc" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Adarsha Das</span></a></div><small class="authorTitle_nd0D" title="Senior Architect @ Meesho">Senior Architect @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><img decoding="async" loading="lazy" alt="BharatMLStack" src="/BharatMLStack/assets/images/bms-7399e8796d2cd24617c432518ce3f312.png" width="1396" height="460" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="building-meeshos-ml-platform-lessons-from-the-first-gen-system-part-2">Building Meesho‚Äôs ML Platform: Lessons from the First-Gen System (Part 2)<a href="#building-meeshos-ml-platform-lessons-from-the-first-gen-system-part-2" class="hash-link" aria-label="Direct link to Building Meesho‚Äôs ML Platform: Lessons from the First-Gen System (Part 2)" title="Direct link to Building Meesho‚Äôs ML Platform: Lessons from the First-Gen System (Part 2)">‚Äã</a></h2>
<p>By late 2022, we had built something we were truly proud of‚Äîa real-time ML serving system with a DAG-based executor, a feature store, and an interaction store powering key ranking and personalization models. It was a major milestone, the culmination of months of effort from data scientists, ML engineers, and backend teams. Our system was live, and we were ready to push the boundaries of experimentation.
And it worked. Mostly.
But soon, cracks appeared. Every new model needed custom feature retrieval logic, DAGs became dense and unmanageable, and scaling turned into a constant firefight. Costs surged, and infra bottlenecks slowed experimentation. Our system worked, but it wasn‚Äôt built for scale.
This is the story of how we tackled these challenges‚Äîbuilding Inferflow for seamless feature retrieval, optimizing real-time infra, and cutting costs while scaling to millions of QPS.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-cost-of-success">The Cost of Success<a href="#the-cost-of-success" class="hash-link" aria-label="Direct link to The Cost of Success" title="Direct link to The Cost of Success">‚Äã</a></h3>
<p>Every new Ranker model required its own feature set, often pulling from different entities. Each addition meant:</p>
<ul>
<li>Adding new DAG nodes in IOP</li>
<li>Writing custom logic to fetch features from multiple sources (e.g., user, product, user √ó category)</li>
<li>Inferring intermediate features (e.g., extracting category from a product to fetch user √ó category data)</li>
<li>Optimizing I/O and dealing with the inevitable bugs</li>
</ul>
<p>What began as clean DAGs soon turned into a tangled web of cross-dependent graphs. Every experimentation cycle meant new nodes, new dependencies, and slower iterations.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-pains-and-cassandras-limits">Scaling Pains (and Cassandra‚Äôs Limits)<a href="#scaling-pains-and-cassandras-limits" class="hash-link" aria-label="Direct link to Scaling Pains (and Cassandra‚Äôs Limits)" title="Direct link to Scaling Pains (and Cassandra‚Äôs Limits)">‚Äã</a></h3>
<p>At some point, we were hitting:</p>
<ul>
<li>250‚Äì300K reads/sec</li>
<li>1M writes/sec (during lean hours)</li>
</ul>
<p>All of this ran on Cassandra. While its distributed architecture had been proven in production, operating large-scale clusters came with considerable infrastructure overhead. Our proof-of-concept (POC) demonstrated throughput of around 100K ops/sec, but as we scaled further, the challenges grew. Ensuring node health, optimizing compaction, and maintaining storage balance became increasingly demanding. We also observed latency spikes under heavy load, alongside a sharp increase in total cost of ownership.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="interaction-store-woes">Interaction Store Woes<a href="#interaction-store-woes" class="hash-link" aria-label="Direct link to Interaction Store Woes" title="Direct link to Interaction Store Woes">‚Äã</a></h3>
<p>Our interaction store was another ticking time bomb:</p>
<ul>
<li>üö® Clusters kept growing in size and cost</li>
<li>üö® Latency spikes became increasingly frequent</li>
<li>üö® The DMC proxy occasionally lost locality of nodes against shards, causing cross-node communication and degraded performance</li>
</ul>
<p>Each time this happened, we had to manually rebalance shards just to restore stable latency, making operations unsustainable at scale.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="silver-linings">Silver Linings<a href="#silver-linings" class="hash-link" aria-label="Direct link to Silver Linings" title="Direct link to Silver Linings">‚Äã</a></h3>
<p>Despite the chaos, the system was live and delivering value:</p>
<ul>
<li>Real-time infrastructure was in production</li>
<li>Costs dropped by 60‚Äì70% compared to offline personalization</li>
<li>New experiments rolled out faster and more successfully</li>
<li>User engagement metrics improved</li>
</ul>
<p>It wasn‚Äôt perfect. It was far from easy. But it worked‚Äîand that counted for a lot.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="round-two-solving-the-top-2-bottlenecks">Round Two: Solving the Top 2 Bottlenecks<a href="#round-two-solving-the-top-2-bottlenecks" class="hash-link" aria-label="Direct link to Round Two: Solving the Top 2 Bottlenecks" title="Direct link to Round Two: Solving the Top 2 Bottlenecks">‚Äã</a></h3>
<p>With the first-gen system stretched to its limits, we stepped back. Conversations with data scientists and backend engineers revealed three recurring pain points:</p>
<ol>
<li>Coding feature retrieval logic for every new model was becoming unsustainable</li>
<li>ML scale was exploding‚Äîbringing rising infra costs with it</li>
<li>Real-time embedding search was the next big unlock</li>
</ol>
<p>We tackled them one by one‚Äîstarting with the biggest pain point.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="problem-1-no-code-feature-retrieval-for-model-inference">Problem 1: No-Code Feature Retrieval for Model Inference<a href="#problem-1-no-code-feature-retrieval-for-model-inference" class="hash-link" aria-label="Direct link to Problem 1: No-Code Feature Retrieval for Model Inference" title="Direct link to Problem 1: No-Code Feature Retrieval for Model Inference">‚Äã</a></h4>
<p>We noticed a pattern: for personalized ranking, models needed features from:</p>
<ul>
<li>‚úÖ Product</li>
<li>‚úÖ User</li>
<li>‚úÖ User √ó Category</li>
<li>‚úÖ Region, cohort, sub-category, etc.</li>
</ul>
<p>A key insight emerged: Entities that contribute features for a model always map back to the context entities.</p>
<p><img decoding="async" loading="lazy" alt="MP Dag" src="/BharatMLStack/assets/images/mp-dag-976ff51caf25f09d977ccc10e70918f3.png" width="1272" height="512" class="img_ev3q"></p>
<p>With this, we designed Inferflow, a graph-driven feature retrieval and model orchestration system:</p>
<ul>
<li>1Ô∏è‚É£ Inferflow takes a modelId and context IDs (e.g., userId, productIds)</li>
<li>2Ô∏è‚É£ Loads a pre-defined feature retrieval graph from ZooKeeper</li>
<li>3Ô∏è‚É£ Executes the graph to resolve entity relationships dynamically</li>
<li>4Ô∏è‚É£ Outputs a 2D matrix of feature vectors</li>
</ul>
<p>üí° The impact?</p>
<ul>
<li>üöÄ No more custom feature retrieval code‚Äîjust graph updates in config</li>
<li>üöÄ Feature consistency across experiments</li>
<li>üöÄ Faster iteration cycles for ranking, fraud detection, and beyond</li>
</ul>
<p>Here‚Äôs a visual example that shows how this graph plays out during execution. We further extended the graph to call multiple models as needed:
<img decoding="async" loading="lazy" alt="MP matrix" src="/BharatMLStack/assets/images/mp-matrix-43994f433f78905ccbd10cfe284f3c9f.png" width="1262" height="768" class="img_ev3q">
We built Inferflow in GoLang, using gRPC and Proto3 serialization for efficiency.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="problem-2-scaling-without-breaking-the-bank">Problem 2: Scaling Without Breaking the Bank<a href="#problem-2-scaling-without-breaking-the-bank" class="hash-link" aria-label="Direct link to Problem 2: Scaling Without Breaking the Bank" title="Direct link to Problem 2: Scaling Without Breaking the Bank">‚Äã</a></h4>
<p>With more ML use cases coming online, we needed to cut costs without compromising performance. We focused on:</p>
<ul>
<li>üîπ Online Feature Store</li>
<li>üîπ Interaction Store</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="optimizing-the-online-feature-store">Optimizing the Online Feature Store<a href="#optimizing-the-online-feature-store" class="hash-link" aria-label="Direct link to Optimizing the Online Feature Store" title="Direct link to Optimizing the Online Feature Store">‚Äã</a></h4>
<p>Our costs were concentrated in:</p>
<ul>
<li>üìå Database (Cassandra)</li>
<li>üìå Cache (Redis)</li>
<li>üìå Running Pods (Java services)</li>
</ul>
<p>1Ô∏è‚É£ Replacing Cassandra with ScyllaDB
As we hit the operational limits of large Cassandra clusters, we transitioned to ScyllaDB, which offered a seamless drop-in replacement without major code changes. The switch brought significant benefits:</p>
<ul>
<li>Throughput: Matched or exceeded Cassandra&#x27;s performance under identical workloads, even under high concurrency.</li>
<li>Latency: Achieved consistently lower P99 latencies due to ScyllaDB&#x27;s shard-per-core architecture and better I/O utilization.</li>
<li>Cost Efficiency: Reduced infra footprint by ~70% through better CPU and memory efficiency, eliminating the need for over-provisioned nodes.</li>
</ul>
<p>2Ô∏è‚É£ Finding the Right Cache
To reduce backend load and improve response times, we benchmarked multiple caching solutions‚ÄîMemcached, KeyDB, and Dragonfly‚Äîunder real production traffic patterns. Dragonfly stood out due to its robust architecture and operational simplicity:</p>
<ul>
<li>Data Skew Handling: Efficiently managed extreme key hotness and uneven access patterns without performance degradation.</li>
<li>Throughput: Delivered consistently high throughput, even with large object sizes and concurrent access.</li>
<li>Ease of Adoption: Acted as a drop-in Redis replacement with full protocol compatibility‚Äîno changes needed in application code or client libraries.</li>
</ul>
<p>3Ô∏è‚É£ Moving to GoLang for Cost-Efficient Serving
Java services were memory-heavy‚Äîso we rewrote core services in GoLang. The results?</p>
<p>‚úÖ Memory usage dropped by ~80%
‚úÖ CPU utilization was significantly lower
‚úÖ Faster, more efficient deployments</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="optimizing-the-interaction-store">Optimizing the Interaction Store<a href="#optimizing-the-interaction-store" class="hash-link" aria-label="Direct link to Optimizing the Interaction Store" title="Direct link to Optimizing the Interaction Store">‚Äã</a></h4>
<p>We realized that we only need a user‚Äôs interaction data in Redis when they open the app. So, we implemented a tiered storage approach:</p>
<ul>
<li>üìå Cold Tier (ScyllaDB)‚ÄîStores click, order, wishlist events</li>
<li>üìå Hot Tier (Redis)‚ÄîLoads a user‚Äôs past interactions only when they open the app</li>
</ul>
<p>Smart Offloading: We introduced an inactivity tracker to detect when a user session ends. At that point, Redis data was flushed back to Scylla, reducing unnecessary writes.</p>
<p><img decoding="async" loading="lazy" alt="InteractionStore" src="/BharatMLStack/assets/images/interaction-str-d9e7aefea121aefb4e94c6c9f060d016.png" width="1242" height="572" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">‚Äã</a></h4>
<ul>
<li>Online Feature Store hit 1M QPS for the first time during the 2023 Mega Blockbuster Sale‚Äîwithout breaking a sweat</li>
<li>Infra costs for Online Feature Store and Interaction Store dropped by ~60%</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-catch-our-ml-hosting-hit-a-hard-limit">The Catch: Our ML Hosting Hit a Hard Limit<a href="#the-catch-our-ml-hosting-hit-a-hard-limit" class="hash-link" aria-label="Direct link to The Catch: Our ML Hosting Hit a Hard Limit" title="Direct link to The Catch: Our ML Hosting Hit a Hard Limit">‚Äã</a></h4>
<p>While planning for 2023 MBS, we ran into a critical scalability bottleneck:</p>
<ul>
<li>‚ùå Insufficient compute availability in our region for ML instances</li>
<li>‚ùå Couldn‚Äôt provision enough nodes to handle real-time inference at scale</li>
</ul>
<p>This forced us to rethink where and how we hosted our models. The existing setup was great for prototyping‚Äîbut it wasn‚Äôt built to handle the bursty, high-QPS demands of real-world production workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-from-firefighting-to-future-proofing">Conclusion: From Firefighting to Future-Proofing<a href="#conclusion-from-firefighting-to-future-proofing" class="hash-link" aria-label="Direct link to Conclusion: From Firefighting to Future-Proofing" title="Direct link to Conclusion: From Firefighting to Future-Proofing">‚Äã</a></h3>
<p>What started as an ambitious experiment turned into a real-time ML infrastructure that powered millions of requests per second. We battled scaling pains, rethought feature retrieval with Inferflow, and rebuilt our infra stack for efficiency‚Äîdriving down costs while improving experimentation velocity.
But new challenges emerged. Our infrastructure could now handle scale, but our ML model hosting setup hit a hard limit. With compute availability bottlenecks threatening real-time inference, we faced a critical decision: how do we make model serving as scalable and cost-efficient as the rest of our stack? That‚Äôs the next piece of the puzzle‚Äîand the story of Part 3.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/inferflow">inferflow</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/interaction-store">interaction-store</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/mlplatform">mlplatform</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/meesho">meesho</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/bharatmlstack">bharatmlstack</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github Discussions<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discord.gg/XkT7XsV2AU" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/BharatMLStack/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2026 Meesho Ltd. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>