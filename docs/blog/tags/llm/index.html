<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">2 posts tagged with &quot;llm&quot; | BharatMLStack</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://meesho.github.io/BharatMLStack/blog/tags/llm"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="2 posts tagged with &quot;llm&quot; | BharatMLStack"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/BharatMLStack/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://meesho.github.io/BharatMLStack/blog/tags/llm"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/blog/tags/llm" hreflang="en"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/blog/tags/llm" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/BharatMLStack/blog/rss.xml" title="BharatMLStack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/BharatMLStack/blog/atom.xml" title="BharatMLStack Atom Feed"><link rel="stylesheet" href="/BharatMLStack/assets/css/styles.05cee957.css">
<script src="/BharatMLStack/assets/js/runtime~main.7f42d8dd.js" defer="defer"></script>
<script src="/BharatMLStack/assets/js/main.391e8e6d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="https://github.com/jayakommuru.png"><div class="gradient-bg-global"><div class="gradient-orb-global orb-global-1"></div><div class="gradient-orb-global orb-global-2"></div><div class="gradient-orb-global orb-global-3"></div></div><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/BharatMLStack/"><b class="navbar__title text--truncate">BharatMLStack</b></a><a class="navbar__item navbar__link" href="/BharatMLStack/intro">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/BharatMLStack/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-five">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-four">Designing a Production-Grade LLM Inference Platform: From Model Weights to Scalable GPU Serving</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-three">Cracking the Code: Scaling Model Inference &amp; Real-Time Embedding Search</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-two">Building Meesho’s ML Platform: Lessons from the First-Gen System (Part 2)</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/BharatMLStack/blog/post-one">Building Meesho’s ML Platform: From Chaos to Cutting-Edge (Part 1)</a></li></ul></div></nav></aside><main class="col col--7"><header class="margin-bottom--xl"><h1>2 posts tagged with &quot;llm&quot;</h1><a href="/BharatMLStack/blog/tags">View All Tags</a></header><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/BharatMLStack/blog/post-five">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-02T00:00:00.000Z">June 2, 2025</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/jayakommuru.png" alt="Jaya Kumar"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Jaya Kumar</span></a></div><small class="authorTitle_nd0D" title="Lead ML Engineer @ Meesho">Lead ML Engineer @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><img decoding="async" loading="lazy" alt="BharatMLStack" src="/BharatMLStack/assets/images/bms-7399e8796d2cd24617c432518ce3f312.png" width="1396" height="460" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-inference-optimization-techniques-engineering-sub-second-latency-at-scale">LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale<a href="#llm-inference-optimization-techniques-engineering-sub-second-latency-at-scale" class="hash-link" aria-label="Direct link to LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale" title="Direct link to LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale">​</a></h2>
<p>Raw execution of Large Language Models is inherently expensive and memory-intensive. To achieve sub-second latency and high throughput, we implement a multi-layered optimization strategy that targets the entire inference stack—from memory management to kernel execution.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-advanced-memory-management-paged--prefix-kv-caching">1. Advanced Memory Management: Paged &amp; Prefix KV Caching<a href="#1-advanced-memory-management-paged--prefix-kv-caching" class="hash-link" aria-label="Direct link to 1. Advanced Memory Management: Paged &amp; Prefix KV Caching" title="Direct link to 1. Advanced Memory Management: Paged &amp; Prefix KV Caching">​</a></h2>
<p>The most significant bottleneck in LLM inference is not always compute, but memory bandwidth—specifically managing the Key-Value (KV) cache.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="paged-kv-caching">Paged KV caching<a href="#paged-kv-caching" class="hash-link" aria-label="Direct link to Paged KV caching" title="Direct link to Paged KV caching">​</a></h3>
<p>Standard caching suffers from fragmentation. We use <strong>Paged KV caching</strong>, which operates similarly to an operating system&#x27;s virtual memory: the KV cache is divided into non-contiguous blocks. This lets us serve larger batch sizes without running out of memory.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="kv-cache-quantization">KV cache quantization<a href="#kv-cache-quantization" class="hash-link" aria-label="Direct link to KV cache quantization" title="Direct link to KV cache quantization">​</a></h3>
<p>To further maximize available memory, we implement <strong>KV cache quantization</strong> (e.g., FP8). By compressing stored attention keys and values from 16-bit to 8-bit, we nearly double the effective context window capacity of the GPU, allowing longer conversations or larger batches without materially degrading quality.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prefix-caching-the-voice-bot-optimizer">Prefix caching (the &quot;voice bot&quot; optimizer)<a href="#prefix-caching-the-voice-bot-optimizer" class="hash-link" aria-label="Direct link to Prefix caching (the &quot;voice bot&quot; optimizer)" title="Direct link to Prefix caching (the &quot;voice bot&quot; optimizer)">​</a></h3>
<p>For use cases like GenAI voice bots where the system prompt (e.g., &quot;You are a helpful assistant...&quot;) is static across thousands of requests, we enable <strong>prefix caching</strong>.</p>
<ul>
<li><strong>Impact</strong>: By reusing pre-computed KV states for common prefixes, we achieve a cache hit rate of ~90%. This reduces <strong>Time To First Token (TTFT)</strong> by skipping redundant computation of the system prompt.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-aggressive-quantization-int4-awq--fp8">2. Aggressive Quantization (INT4 AWQ &amp; FP8)<a href="#2-aggressive-quantization-int4-awq--fp8" class="hash-link" aria-label="Direct link to 2. Aggressive Quantization (INT4 AWQ &amp; FP8)" title="Direct link to 2. Aggressive Quantization (INT4 AWQ &amp; FP8)">​</a></h2>
<p>Running models in their native 16-bit precision (BF16) restricts maximum batch size and throughput. We use quantization to shrink model weights without sacrificing accuracy.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="int4-awq-activation-aware-weight-quantization">INT4 AWQ (Activation-aware Weight Quantization)<a href="#int4-awq-activation-aware-weight-quantization" class="hash-link" aria-label="Direct link to INT4 AWQ (Activation-aware Weight Quantization)" title="Direct link to INT4 AWQ (Activation-aware Weight Quantization)">​</a></h3>
<p>For the Llama 3 family, we use <strong>AWQ</strong> to compress weights to 4 bits. This reduces model size by ~75%, allowing larger models to fit into L4 GPU memory and significantly improving token generation speed.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fp8-precision">FP8 precision<a href="#fp8-precision" class="hash-link" aria-label="Direct link to FP8 precision" title="Direct link to FP8 precision">​</a></h3>
<p>For NVIDIA Hopper (H100) architectures, we are exploring <strong>FP8 quantization</strong>, leveraging native FP8 tensor cores to accelerate matrix multiplications while maintaining a higher dynamic range than integer quantization.</p>
<ul>
<li><strong>Verification</strong>: We validate quantized models by comparing dot-product similarity of embeddings against the FP16 baseline, consistently achieving <strong>&gt;99% similarity</strong>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-kernel-fusion--custom-plugins">3. Kernel Fusion &amp; Custom Plugins<a href="#3-kernel-fusion--custom-plugins" class="hash-link" aria-label="Direct link to 3. Kernel Fusion &amp; Custom Plugins" title="Direct link to 3. Kernel Fusion &amp; Custom Plugins">​</a></h2>
<p>To minimize overhead from launching thousands of small GPU operations, we fuse them into monolithic kernels using NVIDIA TensorRT plugins.</p>
<ul>
<li><strong>Flash attention &amp; FMHA</strong>: We enable <strong>Fused Multi-Head Attention (FMHA)</strong> combined with flash attention to reduce memory reads/writes.</li>
<li><strong>GEMM plugins</strong>: We use specialized <strong>GEMM</strong> plugins to accelerate transformer linear layers.</li>
<li><strong>Removing input padding</strong>: Instead of padding short sequences to match the longest, we remove input padding so the GPU processes only valid tokens.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-inflight-continuous-batching">4. Inflight (Continuous) Batching<a href="#4-inflight-continuous-batching" class="hash-link" aria-label="Direct link to 4. Inflight (Continuous) Batching" title="Direct link to 4. Inflight (Continuous) Batching">​</a></h2>
<p>Traditional static batching waits for all requests in a batch to finish before returning results—so one long response delays everyone else.</p>
<p>We implement <strong>inflight batching</strong>: as soon as one request completes, its slot is freed and filled by a new request from the queue. This keeps GPUs saturated and decouples latency of short queries from long ones.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-parallelism-strategies-scaling-beyond-one-gpu">5. Parallelism Strategies: Scaling Beyond One GPU<a href="#5-parallelism-strategies-scaling-beyond-one-gpu" class="hash-link" aria-label="Direct link to 5. Parallelism Strategies: Scaling Beyond One GPU" title="Direct link to 5. Parallelism Strategies: Scaling Beyond One GPU">​</a></h2>
<p>For large models (e.g., 70B+ parameters) that cannot fit into the VRAM of a single GPU, we use parallelism strategies.</p>
<ul>
<li><strong>Tensor parallelism (TP)</strong>: Split weight matrices across multiple GPUs (e.g., 4× L4 or 8× A100). Each GPU computes a shard and outputs are reduced at every layer.</li>
<li><strong>Pipeline parallelism (PP)</strong>: Split model layers across GPUs to pipeline compute (e.g., while one GPU computes later layers for Request A, another starts early layers for Request B).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-speculative-decoding">6. Speculative Decoding<a href="#6-speculative-decoding" class="hash-link" aria-label="Direct link to 6. Speculative Decoding" title="Direct link to 6. Speculative Decoding">​</a></h2>
<p>To reduce inter-token latency (ITL), we explore <strong>speculative decoding</strong>.</p>
<ul>
<li><strong>Mechanism</strong>: A smaller, faster &quot;draft&quot; model speculatively generates a short token sequence (e.g., 5 tokens).</li>
<li><strong>Verification</strong>: The larger target model verifies those tokens in one parallel forward pass. If correct, we effectively generate multiple tokens per large-model step; if not, we discard and regenerate. This is effective for predictable text, improving perceived generation speed.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="few-benchmarks">Few Benchmarks<a href="#few-benchmarks" class="hash-link" aria-label="Direct link to Few Benchmarks" title="Direct link to Few Benchmarks">​</a></h2>
<p>Below are a couple of representative use cases and performance numbers.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="search-query-rewriting">Search query rewriting<a href="#search-query-rewriting" class="hash-link" aria-label="Direct link to Search query rewriting" title="Direct link to Search query rewriting">​</a></h3>
<ul>
<li><strong>LLM</strong>: Fine-tuned llama-3.2-1B</li>
<li><strong>Input &amp; output token length</strong>: ~10–20</li>
<li><strong>Response type</strong>: Non-streaming</li>
</ul>
<table><thead><tr><th>Inference runtime</th><th>Hardware</th><th style="text-align:right">Max requests/sec</th><th style="text-align:right">Max p99 latency</th></tr></thead><tbody><tr><td>TensorRT-LLM</td><td>4 × L4 GPUs (multi-GPU)</td><td style="text-align:right">1000</td><td style="text-align:right">95 ms</td></tr><tr><td>TensorRT-LLM</td><td>1 × A100 40 GB GPU</td><td style="text-align:right">1000</td><td style="text-align:right">69 ms</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="voice-bot-query">Voice bot query<a href="#voice-bot-query" class="hash-link" aria-label="Direct link to Voice bot query" title="Direct link to Voice bot query">​</a></h3>
<ul>
<li><strong>LLM</strong>: Llama-3.1-8B</li>
<li><strong>Input token length</strong>: ~1900–2000</li>
<li><strong>Output token length</strong>: ~200</li>
<li><strong>Response type</strong>: Streaming</li>
</ul>
<table><thead><tr><th>Inference runtime</th><th style="text-align:right">Concurrency</th><th style="text-align:right">p99 TTFT (ms)</th><th style="text-align:right">p99 ITL (ms)</th><th style="text-align:right">Token throughput (tokens/sec)</th><th style="text-align:right">Request throughput (req/sec)</th><th>Hardware</th></tr></thead><tbody><tr><td>TensorRT-LLM</td><td style="text-align:right">1</td><td style="text-align:right">36.27</td><td style="text-align:right">22.78</td><td style="text-align:right">45.66</td><td style="text-align:right">0.23</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">2</td><td style="text-align:right">49.81</td><td style="text-align:right">23.21</td><td style="text-align:right">89.37</td><td style="text-align:right">0.45</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">4</td><td style="text-align:right">55.33</td><td style="text-align:right">36.62</td><td style="text-align:right">153.39</td><td style="text-align:right">0.78</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">8</td><td style="text-align:right">66.5</td><td style="text-align:right">39.11</td><td style="text-align:right">279.88</td><td style="text-align:right">1.47</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">16</td><td style="text-align:right">131.8</td><td style="text-align:right">30.39</td><td style="text-align:right">547.8</td><td style="text-align:right">2.77</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">32</td><td style="text-align:right">277.22</td><td style="text-align:right">48.02</td><td style="text-align:right">925.7</td><td style="text-align:right">4.78</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">64</td><td style="text-align:right">498.52</td><td style="text-align:right">71.62</td><td style="text-align:right">1,164.40</td><td style="text-align:right">6.2</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">128</td><td style="text-align:right">677.31</td><td style="text-align:right">120.37</td><td style="text-align:right">1,445.18</td><td style="text-align:right">7.69</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">256</td><td style="text-align:right">1,926.31</td><td style="text-align:right">216.88</td><td style="text-align:right">1,600.81</td><td style="text-align:right">8.52</td><td>L4</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">1</td><td style="text-align:right">21.17</td><td style="text-align:right">9.24</td><td style="text-align:right">130.05</td><td style="text-align:right">0.68</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">2</td><td style="text-align:right">25.78</td><td style="text-align:right">9.21</td><td style="text-align:right">264.5</td><td style="text-align:right">1.35</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">4</td><td style="text-align:right">28.52</td><td style="text-align:right">10.99</td><td style="text-align:right">437.69</td><td style="text-align:right">2.27</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">8</td><td style="text-align:right">34.4</td><td style="text-align:right">12.61</td><td style="text-align:right">760.49</td><td style="text-align:right">3.96</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">16</td><td style="text-align:right">68.03</td><td style="text-align:right">14.32</td><td style="text-align:right">1,343.80</td><td style="text-align:right">7.01</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">32</td><td style="text-align:right">185.96</td><td style="text-align:right">16.82</td><td style="text-align:right">2,287.30</td><td style="text-align:right">11.92</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">64</td><td style="text-align:right">136.87</td><td style="text-align:right">21.17</td><td style="text-align:right">3,625.22</td><td style="text-align:right">18.89</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">128</td><td style="text-align:right">463.78</td><td style="text-align:right">34.15</td><td style="text-align:right">4,456.51</td><td style="text-align:right">23.24</td><td>A100</td></tr><tr><td>TensorRT-LLM</td><td style="text-align:right">256</td><td style="text-align:right">890.12</td><td style="text-align:right">59.18</td><td style="text-align:right">5,188.24</td><td style="text-align:right">27.05</td><td>A100</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>High-performance LLM inference is fundamentally a systems engineering problem: memory efficiency, kernel execution, batching strategy, and parallelism determine real-world latency and throughput. Techniques such as paged KV caching, aggressive quantization, kernel fusion, and inflight batching improve GPU utilization while reducing latency and memory pressure.</p>
<p>These optimizations enable the platform to deliver sub-second responses, sustain high concurrency, and efficiently serve both lightweight and long-context workloads. By continuously optimizing across the full inference stack, we keep LLM serving scalable, cost-efficient, and production-ready for real-time AI applications.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/vllm">vllm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/tensorrt-llm">tensorrt-llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/mlplatform">mlplatform</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/meesho">meesho</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/bharatmlstack">bharatmlstack</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/BharatMLStack/blog/post-four">Designing a Production-Grade LLM Inference Platform: From Model Weights to Scalable GPU Serving</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-03-29T00:00:00.000Z">March 29, 2025</time> · <!-- -->14 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/jayakommuru.png" alt="Jaya Kumar"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/jayakommuru" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Jaya Kumar</span></a></div><small class="authorTitle_nd0D" title="Lead ML Engineer @ Meesho">Lead ML Engineer @ Meesho</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><img decoding="async" loading="lazy" alt="BharatMLStack" src="/BharatMLStack/assets/images/bms-7399e8796d2cd24617c432518ce3f312.png" width="1396" height="460" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="designing-a-production-grade-llm-inference-platform-from-model-weights-to-scalable-gpu-serving">Designing a Production-Grade LLM Inference Platform: From Model Weights to Scalable GPU Serving<a href="#designing-a-production-grade-llm-inference-platform-from-model-weights-to-scalable-gpu-serving" class="hash-link" aria-label="Direct link to Designing a Production-Grade LLM Inference Platform: From Model Weights to Scalable GPU Serving" title="Direct link to Designing a Production-Grade LLM Inference Platform: From Model Weights to Scalable GPU Serving">​</a></h2>
<p>Serving large language models in production introduces new challenges across infrastructure, performance optimization, and operational lifecycle management. The LLM Inference Platform addresses these challenges by providing a unified system for deploying and managing open-source and fine-tuned LLMs at scale.</p>
<p>The platform implements a complete LLMOps lifecycle — from model registration and automated compilation to deployment, runtime optimization, and monitoring. Designed as a self-service environment, users can onboard models directly from open repositories such as Hugging Face or upload custom fine-tuned models, and deploy them using a single-click workflow with no manual infrastructure or configuration steps required.</p>
<p>In addition to fully automated deployment, the platform allows users to select and apply custom inference optimization techniques — such as quantization strategies, batching configurations, and runtime-specific performance enhancements — enabling teams to balance latency, throughput, and cost based on their use case. The goal is to reduce operational friction while enabling high-performance, production-grade LLM inference.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-llm-inference-is-not-just-bigger-ml-model-serving">Why LLM Inference Is not just bigger ML model serving<a href="#why-llm-inference-is-not-just-bigger-ml-model-serving" class="hash-link" aria-label="Direct link to Why LLM Inference Is not just bigger ML model serving" title="Direct link to Why LLM Inference Is not just bigger ML model serving">​</a></h2>
<p>Large language model (LLM) inference introduces a fundamentally different set of challenges compared to traditional machine learning inference. While classical ML models typically perform a single forward pass to produce a fixed prediction, LLMs operate as autoregressive systems, generating outputs token by token based on previously generated context. This difference dramatically changes how inference systems must be designed, optimized, and scaled.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="autoregressive-generation-and-sequential-computation">Autoregressive Generation and Sequential Computation:<a href="#autoregressive-generation-and-sequential-computation" class="hash-link" aria-label="Direct link to Autoregressive Generation and Sequential Computation:" title="Direct link to Autoregressive Generation and Sequential Computation:">​</a></h3>
<p>Unlike traditional models such as classifiers or recommenders — where inference cost is relatively constant — LLMs generate responses incrementally. Each new token depends on all previously generated tokens, making inference inherently sequential and dynamic. This means latency and compute requirements vary significantly depending on prompt length and output size, introducing complexity in scheduling and resource allocation.
Because tokens cannot be generated fully in parallel during decoding, GPUs may become underutilized without specialized batching and scheduling strategies. This has led to the development of dedicated LLM inference engines optimized for token-level execution.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prefill-and-decode-phases">Prefill and Decode Phases:<a href="#prefill-and-decode-phases" class="hash-link" aria-label="Direct link to Prefill and Decode Phases:" title="Direct link to Prefill and Decode Phases:">​</a></h3>
<p>LLM inference typically consists of two distinct stages:</p>
<ul>
<li>Prefill phase — the model processes the input prompt and builds internal representations. This stage is compute-heavy and highly parallelizable.</li>
<li>Decode phase — the model generates tokens sequentially, predicting one token at a time using previously generated context.</li>
</ul>
<p>The decode stage often becomes memory-bound rather than compute-bound, which creates new performance bottlenecks compared to traditional ML workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="context-management-and-kv-caching">Context Management and KV Caching:<a href="#context-management-and-kv-caching" class="hash-link" aria-label="Direct link to Context Management and KV Caching:" title="Direct link to Context Management and KV Caching:">​</a></h3>
<p>Another fundamental difference lies in how LLMs maintain context. Transformer-based models rely on attention mechanisms that require access to past token representations. To avoid recomputing these representations repeatedly, inference engines use key-value (KV) caching, which stores intermediate activations from previous tokens.
KV caching significantly improves performance by eliminating redundant computation, but it introduces new challenges:</p>
<ul>
<li>Memory consumption grows with sequence length and batch size</li>
<li>GPU memory becomes a critical bottleneck</li>
<li>Efficient memory management becomes essential for scaling concurrent requests</li>
</ul>
<p>This tradeoff between compute efficiency and memory usage is unique to LLM inference workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="dynamic-and-irregular-workloads">Dynamic and Irregular Workloads:<a href="#dynamic-and-irregular-workloads" class="hash-link" aria-label="Direct link to Dynamic and Irregular Workloads:" title="Direct link to Dynamic and Irregular Workloads:">​</a></h3>
<p>Traditional ML inference typically operates on fixed-size inputs with predictable latency. In contrast, LLM requests vary widely in prompt length, output length, and runtime behavior. As a result:</p>
<ul>
<li>Batch sizes must be dynamic rather than static</li>
<li>Requests may enter and leave batches asynchronously</li>
<li>Scheduling systems must continuously rebalance workloads to maximize GPU utilization</li>
</ul>
<p>These characteristics require specialized serving architectures that differ significantly from standard ML serving pipelines.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="streaming-and-user-experience-constraints">Streaming and User Experience Constraints:<a href="#streaming-and-user-experience-constraints" class="hash-link" aria-label="Direct link to Streaming and User Experience Constraints:" title="Direct link to Streaming and User Experience Constraints:">​</a></h3>
<p>Another distinguishing factor is the expectation of real-time streaming responses. Instead of returning a single output, LLM systems often stream tokens to users as they are generated.
Because of these differences — sequential generation, growing memory requirements, dynamic workloads, and streaming constraints — LLM inference cannot be treated as a simple extension of existing ML serving systems. Production platforms must incorporate specialized runtime engines, advanced optimization techniques, and observability tailored specifically to LLM workloads.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llmops-high-level-architecture">LLMOps: High-Level Architecture<a href="#llmops-high-level-architecture" class="hash-link" aria-label="Direct link to LLMOps: High-Level Architecture" title="Direct link to LLMOps: High-Level Architecture">​</a></h2>
<p><img decoding="async" loading="lazy" alt="LLM Architecture" src="/BharatMLStack/assets/images/llm-plat-9ac69c0ffd8c387d177e582611b8c775.png" width="1302" height="830" class="img_ev3q"></p>
<p>The LLM Inference Framework is designed as a fully automated, end-to-end system for deploying and operating open-source and fine-tuned large language models at scale. The architecture abstracts the complexity of model optimization, hardware selection, deployment, and runtime management into a unified workflow that enables users to move from raw model weights to production-ready inference endpoints with minimal manual intervention.</p>
<p>Our LLM Inference Framework is architected not just as a serving engine, but as a complete lifecycle management system. As illustrated in the high-level design below, the platform automates the journey of a model through seven distinct stages, ensuring reproducibility, performance, and scalability.</p>
<ol>
<li>
<p>Onboarding &amp; Registration (The Source of Truth)</p>
<p>The lifecycle begins with the Data Scientist or engineer.</p>
<ul>
<li>Model Ingestion: Users onboard models—whether open-source (Hugging Face, NeMo) or internally fine-tuned—via the Truffle Box SDK/UI.</li>
<li>LLM + Prompt Registry: Unlike traditional systems that only track model weights, our registry is a unified control plane. It stores both the Model Artifacts and the Prompt Templates. This allows Data Scientists to register and version-control prompts (e.g., &quot;customer_support_v2&quot;) independently of the application code.</li>
</ul>
</li>
<li>
<p>The &quot;Black Box&quot; Build Engine</p>
<p>Once a model is registered, the Automated LLM Compiler + Quantizer Module kicks off a background job on ephemeral GPU resources.</p>
<ul>
<li>Transformation: The raw model is converted into a TRT-LLM Checkpoint.</li>
<li>Quantization: The system automatically applies quantization algorithms (like INT4 AWQ or FP8) to reduce memory footprint.</li>
<li>Engine Building: Finally, it compiles a highly optimized TRT Engine specifically tuned for the target hardware.</li>
</ul>
</li>
<li>
<p>Intelligent Profiling &amp; Validation</p>
<p>Before deployment, the new engine passes through the Hardware &amp; Inference Runtime Profiler.</p>
<ul>
<li>Benchmarking: This module empirically tests the engine against various hardware configurations (L4 vs. A100) and runtimes (TRT-LLM vs. vLLM).</li>
<li>Optimization: It recommends the optimal configuration that meets latency SLAs (Time-To-First-Token) while minimizing cost.</li>
</ul>
</li>
<li>
<p>Smart Artifact Generation &amp; Distribution</p>
<p>To solve the Kubernetes &quot;Cold Start&quot; problem, the LLM Serving Artifacts Generation module packages the model using a bifurcated strategy:</p>
<ul>
<li>Standard Models: Artifacts are uploaded to Cloud Storage (GCS) and downloaded by pods at startup.</li>
<li>Very Large Models: For massive models (&gt;8GB) where network downloads are too slow, the system pre-caches the model onto Secondary Boot Disks. These disks are attached directly to new GPU nodes during autoscaling, eliminating download wait times.</li>
</ul>
</li>
<li>
<p>Image Streaming &amp; Deployment</p>
<p>Simultaneously, the inference runtime container images are pulled from the Artifact Registry.</p>
<ul>
<li>Image Streaming: We utilize container image streaming to allow pods to start initializing while the massive Triton/Dynamo container layers are still downloading, further shaving seconds off the startup time. link</li>
</ul>
</li>
<li>
<p>The Inference Runtime (Kubernetes)</p>
<p>The workload lands on Kubernetes with Autoscaling.</p>
<ul>
<li>Dynamic Backends: Depending on the profile generated in Stage 3, the pod initializes either TensorRT-LLM (for throughput) or vLLM (for flexibility), or spins up a Dynamo worker for distributed inference.</li>
<li>Data Loading: The pod either downloads the model from Cloud Storage or mounts the pre-warmed Secondary Boot Disk (&quot;Pull from Disk&quot;).</li>
</ul>
</li>
<li>
<p>Client Interaction &amp; Observability</p>
<p>Finally, the LLM Inference Client executes the request.</p>
<ul>
<li>Prompt Injection: The client pulls the specific prompt template ID from the Registry, ensuring the exact versioned instructions are used.</li>
<li>Streaming Response: The request is sent via gRPC, and tokens are streamed back to the user in real-time.</li>
</ul>
</li>
<li>
<p>Observability: Monitoring the Pulse of GenAI</p>
<p>In traditional microservices, success is measured by CPU utilization and request latency (p99). For Large Language Models, these metrics are insufficient. A user doesn&#x27;t care if the GPU is at 80% utilization; they care about how fast the first word appears and how smoothly the rest of the sentence follows.</p>
<p>To capture the true user experience, our platform instrumentation focuses on three critical LLM-specific metrics:</p>
<ol>
<li>
<p>Time to First Token (TTFT)</p>
<ul>
<li>Definition: TTFT measures the time elapsed from the moment a request is received until the very first token is generated and streamed back to the user.</li>
<li>Why it matters: This represents the &quot;Prefill Phase&quot; latency—the time the model takes to process the input prompt and load weights. A high TTFT makes the application feel unresponsive or &quot;hung.&quot;</li>
<li>Optimization: We closely monitor TTFT to ensure our Prefix Caching is effective (aiming for high cache hitrates), which drastically lowers this metric by skipping redundant prompt processing.</li>
</ul>
</li>
<li>
<p>Inter-Token Latency (ITL)</p>
<ul>
<li>Definition: ITL measures the average time interval between the generation of consecutive tokens during the &quot;Decode Phase&quot;.</li>
<li>Why it matters: This defines the &quot;perceived speed&quot; of reading. Even if the first token is fast (low TTFT), high ITL makes the text generation look &quot;jerky&quot; or slow to the user.</li>
<li>Benchmarks: In our testing with Llama 3.1, we track p99 ITL to ensure it stays below human reading speeds to maintain a natural conversational flow.</li>
</ul>
</li>
<li>
<p>Token Throughput vs. Request Throughput</p>
<ul>
<li>We distinguish between two types of throughput to balance system efficiency with user load:</li>
<li>Token Throughput (tokens/sec): The total number of tokens generated across all concurrent requests. This measures the raw compute efficiency of the GPU and the effectiveness of batching.</li>
<li>Request Throughput (req/sec): The number of distinct user queries served per second. We use this to determine autoscaling thresholds, ensuring we scale out before the queue depth impacts ITL.</li>
</ul>
</li>
<li>
<p>The Monitoring Stack</p>
<ul>
<li>Real-time Dashboards: We utilize Grafana to visualize these streaming metrics in real-time, allowing on-call engineers to spot &quot;slow generation&quot; incidents that generic &quot;500 error&quot; alerts would miss.</li>
<li>Request Tracing: Since Triton Inference Server does not log request payloads by default, we integrate a Helix Client to asynchronously publish request logs to Log Tables. This allows us to trace a specific &quot;slow&quot; request back to its prompt to understand if a complex input caused the latency spike.</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="supported-inference-backends-tensorrt-llm--dynamo--vllm">Supported Inference backends (TensorRT LLM,  Dynamo &amp; vLLM)<a href="#supported-inference-backends-tensorrt-llm--dynamo--vllm" class="hash-link" aria-label="Direct link to Supported Inference backends (TensorRT LLM,  Dynamo &amp; vLLM)" title="Direct link to Supported Inference backends (TensorRT LLM,  Dynamo &amp; vLLM)">​</a></h2>
<p>Tailored for the Use Case: We do not believe in a &quot;one-size-fits-all&quot; approach to inference. Different use cases—whether a real-time voice bot requiring ultra-lowsub-second latency or a massive reasoning task requiring huge context windows—demand different runtime characteristics. Our platform is designed to be runtime-agnostic, allowing us to automatically select and tailor the best engine based on the specific requirements of the application:</p>
<ol>
<li>
<p>TensorRT-LLM: The High-Performance Standard</p>
<p>Suitable for: High-throughput production workloads where latency is critical (e.g., customer support chat, real-time voice bots).</p>
<p>TensorRT-LLM serves as our default backend for these scenarios. Our internal benchmarks on Llama 3.1 and 3.2 models demonstrated that a tuned TensorRT-LLM engine significantly outperforms standard runtimes, especially when utilizing INT4 AWQ and FP8 quantization .</p>
<p>Key optimizations we tailor for these high-load cases include:</p>
<ul>
<li>Optimized execution via TensorRT engine compilation</li>
<li>Quantization-aware execution for reduced memory usage and improved throughput</li>
<li>Inflight Batching: Allowing requests to be processed continuously without waiting for the entire batch to finish, drastically improving GPU utilization .</li>
<li>Custom Plugins: Enabling specific NVIDIA plugins like the GEMM plugin and GPT Attention plugin to accelerate matrix multiplications and attention mechanisms .</li>
</ul>
</li>
<li>
<p>Dynamo: Distributed Inference for Reasoning Models</p>
<p>Suitable for: Very large &quot;reasoning&quot; models (70B+) or scenarios requiring massive context windows where a single GPU&#x27;s memory is insufficient.</p>
<p>For these memory-bound tasks, we utilize Dynamo, a low-latency distributed inference framework . Unlike monolithic servers, Dynamo disaggregates the inference process to scale resources horizontally:</p>
<ul>
<li>KV Aware Routing: A specialized router directs requests to workers that already hold the relevant Key-Value (KV) cache, minimizing redundant computation .</li>
<li>Prefill vs. Decode Split: The workload is divided into Prefill Workers (processing the prompt) and Decode Workers (generating tokens), allowing us to scale the compute-heavy &quot;reading&quot; phase independently from the memory-heavy &quot;writing&quot; phase .</li>
<li>Distributed execution across multiple GPU resources</li>
</ul>
</li>
<li>
<p>vLLM: The Flexible Baseline</p>
<p>Suitable for: Rapid prototyping, testing new model architectures, or low-traffic internal tools where ease of deployment outweighs raw throughput.</p>
<p>While TensorRT-LLM is optimized for maximum speed, vLLM provides a robust and flexible baseline .</p>
<ul>
<li>High throughput through dynamic batching and efficient memory utilization</li>
<li>Paged KV cache management for handling long contexts and concurrent requests</li>
<li>Strong support for open-source model ecosystems</li>
<li>Rapid Adoption: It allows us to onboard new model architectures immediately without waiting for a custom TensorRT build.</li>
<li>Benchmarking Insight: In our internal tests, vLLM provided a strong baseline but often lacked the specific max-token optimizations present in our custom TRT engines . We use it strategically for initial testing before committing to a full TensorRT optimization pipeline.</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Large language model inference introduces a fundamentally new class of infrastructure challenges—where performance is governed not just by raw compute, but by memory efficiency, intelligent scheduling, runtime specialization, and lifecycle automation. Unlike traditional ML serving, LLM inference requires systems that understand token-level execution, manage rapidly growing context state, and continuously balance latency, throughput, and cost under highly dynamic workloads.</p>
<p>The LLM Inference Framework addresses these challenges by transforming inference into a fully automated, reproducible lifecycle—from model onboarding and compilation to deployment, optimization, and observability. By integrating automated quantization and engine compilation, intelligent runtime selection, cold-start mitigation strategies, and LLM-specific observability metrics such as Time-to-First-Token and Inter-Token Latency, the platform ensures both high performance and operational simplicity.</p>
<p>Equally important, the framework is designed with flexibility and future evolution in mind. Its runtime-agnostic architecture enables seamless adoption of emerging inference engines, hardware accelerators, and optimization techniques without requiring platform redesign. This ensures that teams can continuously leverage advancements in the rapidly evolving LLM ecosystem while maintaining consistent operational workflows.</p>
<p>Ultimately, the goal of the platform is to make production-scale LLM deployment as seamless and reliable as traditional software deployment—allowing teams to focus on building intelligent applications rather than managing infrastructure complexity. By combining lifecycle automation, runtime optimization, and deep observability, the LLM Inference Framework provides a scalable foundation for delivering fast, cost-efficient, and production-ready LLM experiences.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-explorations">Future Explorations<a href="#future-explorations" class="hash-link" aria-label="Direct link to Future Explorations" title="Direct link to Future Explorations">​</a></h2>
<p>While we have achieved significant milestones in latency and throughput, the landscape of GenAI is evolving rapidly. Our roadmap focuses on increasing flexibility, reducing costs, and enhancing reliability for enterprise-grade workloads. Here is what we are building next:</p>
<ul>
<li>TPU Support: To diversify our hardware supply chain and further optimize cost-per-token, we are evaluating Google Cloud TPUs to bake it into our platform. By leveraging the JAX and PyTorch/XLA ecosystems, we aim to unlock the massive throughput potential of TPU v5e chips, particularly for our open-source Llama models. This will allow the hardware profiler to dynamically choose between NVIDIA GPUs and Google TPUs based on real-time availability and price-performance metrics.</li>
<li>Multi-LoRA Serving (Serverless Experience): Currently, deploying a fine-tuned model requires a dedicated GPU. We are building support for Multi-LoRA serving, which will allow us to serve hundreds of unique, fine-tuned adapters on top of a single frozen base model. This will drastically reduce costs for multi-tenant applications, enabling a &quot;serverless&quot; experience where specific fine-tunes are hot-swapped instantly per request.</li>
<li>Spot Instance Orchestration: To further optimize cloud costs, we are developing fault-tolerant mechanisms to run inference workloads on Spot Instances. By implementing aggressive checkpointing and seamless request draining, we aim to leverage cheaper, preemptible compute capacity without interrupting the user&#x27;s streaming experience.</li>
<li>Semantic Caching Layer: We plan to move beyond standard Prefix Caching to implement Semantic Caching. By using a vector database to fetch responses for semantically similar queries (e.g., &quot;How do I reset my password?&quot; vs. &quot;Password reset steps&quot;), we can bypass the GPU entirely for repetitive queries, reducing latency to near-zero.</li>
<li>Context-Aware Autoscaling: Standard CPU/GPU utilization metrics are often insufficient signals for scaling LLMs. We are working on KV-cache pressure metrics for autoscaling. This ensures that we scale out before the memory fills up, preventing eviction-based slowdowns during traffic spikes.</li>
<li>Online Evaluation &amp; Guardrails: We are integrating a lightweight &quot;Trust Layer&quot; into the proxy. This will allow for low-latency input/output filtering (Guardrails) and asynchronous &quot;LLM-as-a-Judge&quot; evaluation pipelines to monitor response quality in production, not just system health.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/vllm">vllm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/tensorrt-llm">tensorrt-llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/mlplatform">mlplatform</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/meesho">meesho</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/BharatMLStack/blog/tags/bharatmlstack">bharatmlstack</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github Discussions<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discord.gg/XkT7XsV2AU" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/BharatMLStack/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Meesho Ltd. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>