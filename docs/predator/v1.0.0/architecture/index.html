<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-predator/v1.0.0/architecture" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Architecture | BharatMLStack</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://meesho.github.io/BharatMLStack/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://meesho.github.io/BharatMLStack/predator/v1.0.0/architecture"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Architecture | BharatMLStack"><meta data-rh="true" name="description" content="Predator is a scalable, high-performance model inference service built as a wrapper around the NVIDIA Triton Inference Server. It is designed to serve a variety of machine learning models (Deep Learning, Tree-based, etc.) with low latency in a Kubernetes (K8s) environment."><meta data-rh="true" property="og:description" content="Predator is a scalable, high-performance model inference service built as a wrapper around the NVIDIA Triton Inference Server. It is designed to serve a variety of machine learning models (Deep Learning, Tree-based, etc.) with low latency in a Kubernetes (K8s) environment."><link data-rh="true" rel="icon" href="/BharatMLStack/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://meesho.github.io/BharatMLStack/predator/v1.0.0/architecture"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/predator/v1.0.0/architecture" hreflang="en"><link data-rh="true" rel="alternate" href="https://meesho.github.io/BharatMLStack/predator/v1.0.0/architecture" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Predator","item":"https://meesho.github.io/BharatMLStack/category/predator"},{"@type":"ListItem","position":2,"name":"v1.0.0","item":"https://meesho.github.io/BharatMLStack/predator/v1.0.0"},{"@type":"ListItem","position":3,"name":"Architecture","item":"https://meesho.github.io/BharatMLStack/predator/v1.0.0/architecture"}]}</script><link rel="alternate" type="application/rss+xml" href="/BharatMLStack/blog/rss.xml" title="BharatMLStack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/BharatMLStack/blog/atom.xml" title="BharatMLStack Atom Feed"><link rel="stylesheet" href="/BharatMLStack/assets/css/styles.aaf16941.css">
<script src="/BharatMLStack/assets/js/runtime~main.a356c557.js" defer="defer"></script>
<script src="/BharatMLStack/assets/js/main.6f8db0ca.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div class="gradient-bg-global"><div class="gradient-orb-global orb-global-1"></div><div class="gradient-orb-global orb-global-2"></div><div class="gradient-orb-global orb-global-3"></div></div><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/BharatMLStack/"><b class="navbar__title text--truncate">BharatMLStack</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/BharatMLStack/intro">Docs</a><a class="navbar__item navbar__link" href="/BharatMLStack/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/BharatMLStack/intro">BharatMLStack Documentation</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/BharatMLStack/category/online-feature-store">Online Feature Store</a><button aria-label="Expand sidebar category &#x27;Online Feature Store&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/BharatMLStack/category/inferflow">Inferflow</a><button aria-label="Expand sidebar category &#x27;Inferflow&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/BharatMLStack/category/quick-start">Quick Start</a><button aria-label="Expand sidebar category &#x27;Quick Start&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/BharatMLStack/category/trufflebox-ui">Trufflebox UI</a><button aria-label="Expand sidebar category &#x27;Trufflebox UI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/BharatMLStack/category/sdks">SDKs</a><button aria-label="Expand sidebar category &#x27;SDKs&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/BharatMLStack/category/skye">Skye</a><button aria-label="Expand sidebar category &#x27;Skye&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/BharatMLStack/category/numerix">Numerix</a><button aria-label="Expand sidebar category &#x27;Numerix&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/BharatMLStack/category/predator">Predator</a><button aria-label="Collapse sidebar category &#x27;Predator&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/BharatMLStack/predator/v1.0.0">v1.0.0</a><button aria-label="Collapse sidebar category &#x27;v1.0.0&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/BharatMLStack/predator/v1.0.0/architecture">Architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/BharatMLStack/predator/v1.0.0/functionalities">Key Functionalities</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/BharatMLStack/predator/v1.0.0/release-notes">Release Notes</a></li></ul></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/BharatMLStack/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/BharatMLStack/category/predator"><span>Predator</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/BharatMLStack/predator/v1.0.0"><span>v1.0.0</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Architecture</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>BharatMLStack - Predator</h1></header>
<p>Predator is a scalable, high-performance model inference service built as a wrapper around the <strong>NVIDIA Triton Inference Server</strong>. It is designed to serve a variety of machine learning models (Deep Learning, Tree-based, etc.) with low latency in a <strong>Kubernetes (K8s)</strong> environment.</p>
<p>The system integrates seamlessly with the <strong>Online Feature Store (OnFS)</strong> for real-time feature retrieval and uses <strong>Horizon</strong> as the deployment orchestration layer. Deployments follow a <strong>GitOps</strong> pipeline — Horizon generates Helm configurations, commits them to GitHub, and <strong>Argo Sync</strong> reconciles the desired state onto Kubernetes.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="high-level-design">High-Level Design<a href="#high-level-design" class="hash-link" aria-label="Direct link to High-Level Design" title="Direct link to High-Level Design">​</a></h2>
<p><img decoding="async" loading="lazy" alt="Predator HLD - End-to-end deployment and inference architecture" src="/BharatMLStack/assets/images/v1.0.0-predator-hld-949215d6604ae103e724c3978e803443.png" width="1824" height="1124" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-flow">End-to-End Flow<a href="#end-to-end-flow" class="hash-link" aria-label="Direct link to End-to-End Flow" title="Direct link to End-to-End Flow">​</a></h3>
<ol>
<li>
<p><strong>Model Deployment Trigger</strong>: An actor initiates deployment through <strong>Trufflebox UI</strong>, specifying the GCS path (<code>gcs://</code>) of the trained model. Separately, post-training pipelines write model artifacts to <strong>GCS Artifactory</strong>.</p>
</li>
<li>
<p><strong>Orchestration via Horizon</strong>: Trufflebox UI communicates with <strong>Horizon</strong>, the deployment orchestration layer. Horizon generates the appropriate <strong>Helm</strong> chart configuration for the inference service.</p>
</li>
<li>
<p><strong>GitOps Pipeline</strong>: Horizon commits the Helm values to a <strong>GitHub</strong> repository. <strong>Argo Sync</strong> watches the repo and reconciles the desired state onto the Kubernetes cluster, creating or updating deployable units.</p>
</li>
<li>
<p><strong>Deployable Units (Deployable 1 … N)</strong>: Each deployable is an independent Kubernetes deployment that:</p>
<ul>
<li>Downloads model artifacts from <strong>GCS</strong> at startup via an <code>init.sh</code> script.</li>
<li>Launches a <strong>Triton Inference Server</strong> instance loaded with the model.</li>
<li>Runs one or more pods, each containing the inference runtime and configured backends.</li>
</ul>
</li>
<li>
<p><strong>Triton Backends</strong>: Each Triton instance supports pluggable backends based on the model type:</p>
<ul>
<li><strong>FIL</strong> — GPU-accelerated tree-based models (XGBoost, LightGBM, Random Forest).</li>
<li><strong>PyTorch</strong> — Native PyTorch models via LibTorch.</li>
<li><strong>Python</strong> — Custom preprocessing/postprocessing or unsupported model formats.</li>
<li><strong>TRT (TensorRT)</strong> — GPU-optimized serialized TensorRT engines.</li>
<li><strong>ONNX</strong> — Framework-agnostic execution via ONNX Runtime.</li>
<li><strong>DALI</strong> — GPU-accelerated data preprocessing (image, audio, video).</li>
</ul>
</li>
<li>
<p><strong>Autoscaling with KEDA</strong>: The cluster uses <strong>KEDA</strong> (Kubernetes Event-Driven Autoscaling) to scale deployable pods based on custom metrics (CPU utilization, GPU utilization via DCGM, queue depth, etc.). The underlying <strong>Kubernetes</strong> scheduler places pods across GPU/CPU node pools.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-design-principles">Key Design Principles<a href="#key-design-principles" class="hash-link" aria-label="Direct link to Key Design Principles" title="Direct link to Key Design Principles">​</a></h3>
<ul>
<li><strong>GitOps-driven</strong>: All deployment state is version-controlled in Git; Argo Sync ensures cluster state matches the declared configuration.</li>
<li><strong>Isolation per deployable</strong>: Each model or model group gets its own deployable unit, preventing noisy-neighbor interference.</li>
<li><strong>Init-based model loading</strong>: Models are materialized to local disk before Triton starts, ensuring deterministic startup and no runtime dependency on remote storage.</li>
<li><strong>Pluggable backends</strong>: The same infrastructure serves deep learning, tree-based, and custom models through Triton&#x27;s backend abstraction.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="inference-engine-triton-inference-server">Inference Engine: Triton Inference Server<a href="#inference-engine-triton-inference-server" class="hash-link" aria-label="Direct link to Inference Engine: Triton Inference Server" title="Direct link to Inference Engine: Triton Inference Server">​</a></h2>
<p>NVIDIA Triton Inference Server is a high-performance model serving system designed to deploy ML and deep learning models at scale across CPUs and GPUs. It provides a unified inference runtime that supports multiple frameworks, optimized execution, and production-grade scheduling.</p>
<p>Triton operates as a standalone server that loads models from a model repository and exposes standardized HTTP/gRPC APIs. Predator uses <strong>gRPC</strong> for efficient request and response handling via the <strong>helix client</strong>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="core-components">Core Components<a href="#core-components" class="hash-link" aria-label="Direct link to Core Components" title="Direct link to Core Components">​</a></h3>
<ul>
<li><strong>Model Repository</strong>: Central directory where models are stored. Predator typically materializes the model repository onto local disk via an init container, enabling fast model loading and eliminating runtime dependency on remote storage during inference.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="backends">Backends<a href="#backends" class="hash-link" aria-label="Direct link to Backends" title="Direct link to Backends">​</a></h3>
<p>A backend is the runtime responsible for executing a model. Each model specifies which backend runs it via configuration.</p>
<table><thead><tr><th>Backend</th><th>Description</th></tr></thead><tbody><tr><td><strong>TensorRT</strong></td><td>GPU-optimized; executes serialized TensorRT engines (kernel fusion, FP16/INT8).</td></tr><tr><td><strong>PyTorch</strong></td><td>Serves native PyTorch models via LibTorch.</td></tr><tr><td><strong>ONNX Runtime</strong></td><td>Framework-agnostic ONNX execution with TensorRT and other accelerators.</td></tr><tr><td><strong>TensorFlow</strong></td><td>Runs TensorFlow SavedModel format.</td></tr><tr><td><strong>Python backend</strong></td><td>Custom Python code for preprocessing, postprocessing, or unsupported models.</td></tr><tr><td><strong>Custom backends</strong></td><td>C++/Python backends for specialized or proprietary runtimes.</td></tr><tr><td><strong>DALI</strong></td><td>GPU-accelerated data preprocessing (image, audio, video).</td></tr><tr><td><strong>FIL (Forest Inference Library)</strong></td><td>GPU-accelerated tree-based models (XGBoost, LightGBM, Random Forest).</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-features">Key Features<a href="#key-features" class="hash-link" aria-label="Direct link to Key Features" title="Direct link to Key Features">​</a></h3>
<ul>
<li><strong>Dynamic batching</strong>: Combines multiple requests into a single batch at runtime — higher GPU utilization, improved throughput, reduced latency variance.</li>
<li><strong>Concurrent model execution</strong>: Run multiple models or multiple instances of the same model; distribute load across GPUs.</li>
<li><strong>Model versioning</strong>: Support multiple versions per model.</li>
<li><strong>Ensemble models</strong>: Pipeline of models as an ensemble; eliminates intermediate network hops, reduces latency.</li>
<li><strong>Model instance scaling</strong>: Multiple copies of a model for parallel inference and load isolation.</li>
<li><strong>Observability</strong>: Prometheus metrics, granular latency, throughput, GPU utilization.</li>
<li><strong>Warmup requests</strong>: Preload kernels and avoid cold-start latency.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-repository-structure">Model Repository Structure<a href="#model-repository-structure" class="hash-link" aria-label="Direct link to Model Repository Structure" title="Direct link to Model Repository Structure">​</a></h2>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">model_repository/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── model_A/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├── config.pbtxt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├── 1/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   │   └── model.plan</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├── 2/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   │   └── model.plan</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── model_B/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├── config.pbtxt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├── 1/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       └── model.py</span><br></span></code></pre></div></div>
<p>The <code>config.pbtxt</code> file defines how Triton loads and executes a model: input/output tensors, batch settings, hardware execution, backend runtime, and optimization parameters. At minimum it defines: <code>backend/platform</code>, <code>max_batch_size</code>, <code>inputs</code>, <code>outputs</code>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sample-configpbtxt">Sample config.pbtxt<a href="#sample-configpbtxt" class="hash-link" aria-label="Direct link to Sample config.pbtxt" title="Direct link to Sample config.pbtxt">​</a></h3>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">name: &quot;product_ranking_model&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">platform: &quot;tensorrt_plan&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">max_batch_size: 64</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input [ { name: &quot;input_embeddings&quot; data_type: TYPE_FP16 dims: [ 128 ] }, { name: &quot;context_features&quot; data_type: TYPE_FP32 dims: [ 32 ] } ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output [ { name: &quot;scores&quot; data_type: TYPE_FP32 dims: [ 1 ] } ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">instance_group [ { kind: KIND_GPU count: 2 gpus: [0] } ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">dynamic_batching { preferred_batch_size: [8,16,32,64] max_queue_delay_microseconds: 2000 }</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="kubernetes-deployment-architecture">Kubernetes Deployment Architecture<a href="#kubernetes-deployment-architecture" class="hash-link" aria-label="Direct link to Kubernetes Deployment Architecture" title="Direct link to Kubernetes Deployment Architecture">​</a></h2>
<p>Predator inference services are deployed on Kubernetes using <strong>Helm-based</strong> deployments for standardized, scalable, GPU-optimized model serving. Each deployment consists of Triton Inference Server wrapped within a Predator runtime, with autoscaling driven by CPU and GPU utilization.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pod-architecture">Pod Architecture<a href="#pod-architecture" class="hash-link" aria-label="Direct link to Pod Architecture" title="Direct link to Pod Architecture">​</a></h3>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Predator Pod</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── Init Container (Model Sync)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── Triton Inference Server Container</span><br></span></code></pre></div></div>
<p>Model artifacts and runtime are initialized before inference traffic is accepted.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="init-container">Init Container<a href="#init-container" class="hash-link" aria-label="Direct link to Init Container" title="Direct link to Init Container">​</a></h4>
<ul>
<li>Download model artifacts from cloud storage (GCS).</li>
<li>Populate the Triton model repository directory.</li>
<li>Example: <code>gcloud storage cp -r gs://.../model-path/* /models</code></li>
</ul>
<p>Benefits: deterministic startup (Triton starts only after models are available), separation of concerns (image = runtime, repository = data).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="triton-inference-server-container">Triton Inference Server Container<a href="#triton-inference-server-container" class="hash-link" aria-label="Direct link to Triton Inference Server Container" title="Direct link to Triton Inference Server Container">​</a></h4>
<ul>
<li>Load model artifacts from local repository.</li>
<li>Manage inference scheduling, request/response handling, and expose inference endpoints.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="triton-server-image-strategy">Triton Server Image Strategy<a href="#triton-server-image-strategy" class="hash-link" aria-label="Direct link to Triton Server Image Strategy" title="Direct link to Triton Server Image Strategy">​</a></h3>
<p>The Helm chart uses the Triton container image from the internal <strong>artifact registry</strong>. Production uses <strong>custom-built</strong> images (only required backends, e.g. TensorRT, Python) to reduce size and startup time. Unnecessary components are excluded; images are built internally and pushed to the registry.</p>
<p><strong>Response Caching</strong>: Custom cache plugins can be added at image build time for optional inference response caching — reducing redundant execution and GPU use for repeated inputs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-distribution-optimization">Image Distribution Optimization<a href="#image-distribution-optimization" class="hash-link" aria-label="Direct link to Image Distribution Optimization" title="Direct link to Image Distribution Optimization">​</a></h3>
<ul>
<li><strong>Secondary boot disk image caching</strong>: Images are pre-cached on GPU node pool secondary boot disks to avoid repeated pulls during scale-up and reduce pod startup time and cold-start latency.</li>
<li><strong>Image streaming</strong>: Can be used to progressively pull layers for faster time-to-readiness during scaling.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="health-probes">Health Probes<a href="#health-probes" class="hash-link" aria-label="Direct link to Health Probes" title="Direct link to Health Probes">​</a></h3>
<p>Readiness and liveness use <code>/v2/health/ready</code>. Triton receives traffic only after model loading; failed instances are restarted automatically.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="resource-configuration">Resource Configuration<a href="#resource-configuration" class="hash-link" aria-label="Direct link to Resource Configuration" title="Direct link to Resource Configuration">​</a></h3>
<p>Sample GPU resource config:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">limits</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">cpu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> 7000m</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">memory</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> 28Gi</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">gpu</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">1</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="autoscaling-architecture">Autoscaling Architecture<a href="#autoscaling-architecture" class="hash-link" aria-label="Direct link to Autoscaling Architecture" title="Direct link to Autoscaling Architecture">​</a></h3>
<p>Predator uses <strong>KEDA</strong> (Kubernetes Event-Driven Autoscaling) for scaling deployable pods. KEDA supports custom metric sources including:</p>
<ul>
<li><strong>CPU / Memory utilization</strong> for CPU-based deployments.</li>
<li><strong>GPU utilization</strong> via <strong>DCGM</strong> (Data Center GPU Manager) for GPU pods — covering utilization, memory, power, etc.</li>
<li><strong>Custom Prometheus queries</strong> for application-level scaling signals (e.g., inference queue depth, request latency).</li>
</ul>
<p>KEDA ScaledObjects are configured per deployable, enabling fine-grained, independent scaling for each model or model group.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="contributing">Contributing<a href="#contributing" class="hash-link" aria-label="Direct link to Contributing" title="Direct link to Contributing">​</a></h2>
<p>We welcome contributions! See the <a href="https://github.com/Meesho/BharatMLStack/blob/main/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Contributing Guide</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="community--support">Community &amp; Support<a href="#community--support" class="hash-link" aria-label="Direct link to Community &amp; Support" title="Direct link to Community &amp; Support">​</a></h2>
<ul>
<li><strong>Discord</strong>: <a href="https://discord.gg/XkT7XsV2AU" target="_blank" rel="noopener noreferrer">community chat</a></li>
<li><strong>Issues</strong>: <a href="https://github.com/Meesho/BharatMLStack/issues" target="_blank" rel="noopener noreferrer">GitHub Issues</a></li>
<li><strong>Email</strong>: <a href="mailto:ml-oss@meesho.com" target="_blank" rel="noopener noreferrer">ml-oss@meesho.com</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="license">License<a href="#license" class="hash-link" aria-label="Direct link to License" title="Direct link to License">​</a></h2>
<p>BharatMLStack is open-source under the <a href="https://github.com/Meesho/BharatMLStack/blob/main/LICENSE.md" target="_blank" rel="noopener noreferrer">BharatMLStack Business Source License 1.1</a>.</p>
<hr>
<div align="center"><strong>Built with ❤️ for the ML community from Meesho</strong></div>
<div align="center"><strong>If you find this useful, ⭐️ the repo — your support means the world to us!</strong></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/Meesho/BharatMLStack/tree/main/docs/docs/predator/v1.0.0/architecture.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/BharatMLStack/predator/v1.0.0"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">v1.0.0</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/BharatMLStack/predator/v1.0.0/functionalities"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Key Functionalities</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#high-level-design" class="table-of-contents__link toc-highlight">High-Level Design</a><ul><li><a href="#end-to-end-flow" class="table-of-contents__link toc-highlight">End-to-End Flow</a></li><li><a href="#key-design-principles" class="table-of-contents__link toc-highlight">Key Design Principles</a></li></ul></li><li><a href="#inference-engine-triton-inference-server" class="table-of-contents__link toc-highlight">Inference Engine: Triton Inference Server</a><ul><li><a href="#core-components" class="table-of-contents__link toc-highlight">Core Components</a></li><li><a href="#backends" class="table-of-contents__link toc-highlight">Backends</a></li><li><a href="#key-features" class="table-of-contents__link toc-highlight">Key Features</a></li></ul></li><li><a href="#model-repository-structure" class="table-of-contents__link toc-highlight">Model Repository Structure</a><ul><li><a href="#sample-configpbtxt" class="table-of-contents__link toc-highlight">Sample config.pbtxt</a></li></ul></li><li><a href="#kubernetes-deployment-architecture" class="table-of-contents__link toc-highlight">Kubernetes Deployment Architecture</a><ul><li><a href="#pod-architecture" class="table-of-contents__link toc-highlight">Pod Architecture</a></li><li><a href="#triton-server-image-strategy" class="table-of-contents__link toc-highlight">Triton Server Image Strategy</a></li><li><a href="#image-distribution-optimization" class="table-of-contents__link toc-highlight">Image Distribution Optimization</a></li><li><a href="#health-probes" class="table-of-contents__link toc-highlight">Health Probes</a></li><li><a href="#resource-configuration" class="table-of-contents__link toc-highlight">Resource Configuration</a></li><li><a href="#autoscaling-architecture" class="table-of-contents__link toc-highlight">Autoscaling Architecture</a></li></ul></li><li><a href="#contributing" class="table-of-contents__link toc-highlight">Contributing</a></li><li><a href="#community--support" class="table-of-contents__link toc-highlight">Community &amp; Support</a></li><li><a href="#license" class="table-of-contents__link toc-highlight">License</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github Discussions<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discord.gg/XkT7XsV2AU" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/BharatMLStack/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/Meesho/BharatMLStack" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Meesho Ltd. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>