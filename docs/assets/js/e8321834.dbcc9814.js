"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[149],{5842:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"predator/v1.0.0/functionalities","title":"Key Functionalities","description":"Overview","source":"@site/docs/predator/v1.0.0/functionalities.md","sourceDirName":"predator/v1.0.0","slug":"/predator/v1.0.0/functionalities","permalink":"/BharatMLStack/predator/v1.0.0/functionalities","draft":false,"unlisted":false,"editUrl":"https://github.com/Meesho/BharatMLStack/tree/main/docs/docs/predator/v1.0.0/functionalities.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Key Functionalities","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Architecture","permalink":"/BharatMLStack/predator/v1.0.0/architecture"},"next":{"title":"Release Notes","permalink":"/BharatMLStack/predator/v1.0.0/release-notes"}}');var r=i(4848),t=i(8453);const l={title:"Key Functionalities",sidebar_position:2},o="Predator - Key Functionalities",d={},c=[{value:"Overview",id:"overview",level:2},{value:"Core Capabilities",id:"core-capabilities",level:2},{value:"Multi-Backend Inference",id:"multi-backend-inference",level:3},{value:"Dynamic Batching",id:"dynamic-batching",level:3},{value:"Concurrent Model Execution",id:"concurrent-model-execution",level:3},{value:"Model Versioning &amp; Ensembles",id:"model-versioning--ensembles",level:3},{value:"Model Instance Scaling",id:"model-instance-scaling",level:3},{value:"Inference &amp; API",id:"inference--api",level:2},{value:"gRPC via Helix Client",id:"grpc-via-helix-client",level:3},{value:"Model Repository",id:"model-repository",level:3},{value:"Deployment &amp; Operational Features",id:"deployment--operational-features",level:2},{value:"Custom Triton Images",id:"custom-triton-images",level:3},{value:"Image Distribution",id:"image-distribution",level:3},{value:"Health Probes",id:"health-probes",level:3},{value:"Autoscaling",id:"autoscaling",level:3},{value:"Observability",id:"observability",level:2},{value:"Contributing",id:"contributing",level:2},{value:"Community &amp; Support",id:"community--support",level:2},{value:"License",id:"license",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"predator---key-functionalities",children:"Predator - Key Functionalities"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(n.p,{children:["Predator is a scalable, high-performance model inference service built as a wrapper around ",(0,r.jsx)(n.strong,{children:"NVIDIA Triton Inference Server"}),". It serves Deep Learning and tree-based models with low latency in ",(0,r.jsx)(n.strong,{children:"Kubernetes"}),", integrates with the ",(0,r.jsx)(n.strong,{children:"Online Feature Store (OnFS)"})," and uses ",(0,r.jsx)(n.strong,{children:"Interflow"})," for orchestration between clients, feature store, and inference engine. Clients send inference requests via the ",(0,r.jsx)(n.strong,{children:"Helix client"})," over gRPC."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"core-capabilities",children:"Core Capabilities"}),"\n",(0,r.jsx)(n.h3,{id:"multi-backend-inference",children:"Multi-Backend Inference"}),"\n",(0,r.jsx)(n.p,{children:"Predator leverages Triton's pluggable backends so you can serve a variety of model types from a single deployment:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Backend"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"TensorRT"})}),(0,r.jsx)(n.td,{children:"GPU-optimized DL; serialized engines (FP16/INT8)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"PyTorch"})}),(0,r.jsx)(n.td,{children:"Native PyTorch via LibTorch"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"ONNX Runtime"})}),(0,r.jsx)(n.td,{children:"Framework-agnostic ONNX with TensorRT/GPU"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"TensorFlow"})}),(0,r.jsx)(n.td,{children:"SavedModel format"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Python"})}),(0,r.jsx)(n.td,{children:"Custom preprocessing, postprocessing, or unsupported models"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"FIL"})}),(0,r.jsx)(n.td,{children:"Tree-based models (XGBoost, LightGBM, Random Forest) on GPU"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"DALI"})}),(0,r.jsx)(n.td,{children:"GPU-accelerated data preprocessing (image, audio, video)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Custom"})}),(0,r.jsx)(n.td,{children:"C++/Python backends for proprietary or specialized runtimes"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"dynamic-batching",children:"Dynamic Batching"}),"\n",(0,r.jsx)(n.p,{children:"Triton combines multiple incoming requests into a single batch at runtime."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Higher GPU utilization and improved throughput"}),"\n",(0,r.jsx)(n.li,{children:"Reduced latency variance"}),"\n",(0,r.jsxs)(n.li,{children:["Configurable ",(0,r.jsx)(n.code,{children:"preferred_batch_size"})," and ",(0,r.jsx)(n.code,{children:"max_queue_delay_microseconds"})," in ",(0,r.jsx)(n.code,{children:"config.pbtxt"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"concurrent-model-execution",children:"Concurrent Model Execution"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Run multiple models simultaneously"}),"\n",(0,r.jsx)(n.li,{children:"Run multiple instances of the same model"}),"\n",(0,r.jsxs)(n.li,{children:["Distribute load across GPUs via ",(0,r.jsx)(n.code,{children:"instance_group"})," in model config"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"model-versioning--ensembles",children:"Model Versioning & Ensembles"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Versioning"}),": Multiple versions per model (e.g. ",(0,r.jsx)(n.code,{children:"1/"}),", ",(0,r.jsx)(n.code,{children:"2/"})," in the model repository)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ensembles"}),": Define a pipeline of models as an ensemble; eliminates intermediate network hops and reduces latency"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"model-instance-scaling",children:"Model Instance Scaling"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Deploy multiple copies of a model for parallel inference and load isolation"}),"\n",(0,r.jsxs)(n.li,{children:["Configured via ",(0,r.jsx)(n.code,{children:"instance_group"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"inference--api",children:"Inference & API"}),"\n",(0,r.jsx)(n.h3,{id:"grpc-via-helix-client",children:"gRPC via Helix Client"}),"\n",(0,r.jsxs)(n.p,{children:["Predator uses ",(0,r.jsx)(n.strong,{children:"gRPC"})," for efficient request/response handling. Client applications (e.g. Realestate, IOP) send inference requests through the ",(0,r.jsx)(n.strong,{children:"Helix client"}),", which talks to the Triton Inference Server inside the Predator pod."]}),"\n",(0,r.jsx)(n.h3,{id:"model-repository",children:"Model Repository"}),"\n",(0,r.jsxs)(n.p,{children:["Models are stored in a local model repository. Predator materializes this via an ",(0,r.jsx)(n.strong,{children:"Init Container"})," that downloads artifacts from cloud storage (e.g. GCS) so Triton has no runtime dependency on remote storage during inference."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"deployment--operational-features",children:"Deployment & Operational Features"}),"\n",(0,r.jsx)(n.h3,{id:"custom-triton-images",children:"Custom Triton Images"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Production uses ",(0,r.jsx)(n.strong,{children:"custom-built"})," Triton images (only required backends) for smaller size and faster startup"]}),"\n",(0,r.jsxs)(n.li,{children:["Images built on GCP VM, pushed to ",(0,r.jsx)(n.strong,{children:"Artifact Registry"}),", and referenced in Helm deployments"]}),"\n",(0,r.jsxs)(n.li,{children:["Optional ",(0,r.jsx)(n.strong,{children:"response caching"})," via custom cache plugins added at image build time"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"image-distribution",children:"Image Distribution"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Secondary boot disk caching"}),": Triton image pre-cached on GPU node pool to reduce pod startup and scale-up latency"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Image streaming"}),": Optionally used for faster time-to-readiness during scaling"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"health-probes",children:"Health Probes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Readiness and liveness use ",(0,r.jsx)(n.code,{children:"/v2/health/ready"})]}),"\n",(0,r.jsx)(n.li,{children:"Triton receives traffic only after models are loaded; failed instances are restarted automatically"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"autoscaling",children:"Autoscaling"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"CPU-based scaling for generic load"}),"\n",(0,r.jsxs)(n.li,{children:["GPU-based scaling using ",(0,r.jsx)(n.strong,{children:"DCGM"})," metrics (utilization, memory, power); custom queries drive scale-up/scale-down"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"observability",children:"Observability"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prometheus metrics"}),": Latency, throughput, GPU utilization, and more"]}),"\n",(0,r.jsxs)(n.li,{children:["Metrics emitted from the Triton Inference Container and visualized in ",(0,r.jsx)(n.strong,{children:"Grafana"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Warmup requests"}),": Configurable to preload kernels and avoid cold-start latency"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"contributing",children:"Contributing"}),"\n",(0,r.jsxs)(n.p,{children:["We welcome contributions! See the ",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/CONTRIBUTING.md",children:"Contributing Guide"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"community--support",children:"Community & Support"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Discord"}),": ",(0,r.jsx)(n.a,{href:"https://discord.gg/XkT7XsV2AU",children:"community chat"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Issues"}),": ",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/issues",children:"GitHub Issues"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Email"}),": ",(0,r.jsx)(n.a,{href:"mailto:ml-oss@meesho.com",children:"ml-oss@meesho.com"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"license",children:"License"}),"\n",(0,r.jsxs)(n.p,{children:["BharatMLStack is open-source under the ",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/LICENSE.md",children:"BharatMLStack Business Source License 1.1"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)("div",{align:"center",children:(0,r.jsx)("strong",{children:"Built with \u2764\ufe0f for the ML community from Meesho"})}),"\n",(0,r.jsx)("div",{align:"center",children:(0,r.jsx)("strong",{children:"If you find this useful, \u2b50\ufe0f the repo \u2014 your support means the world to us!"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function l(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);