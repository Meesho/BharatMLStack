"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6724],{461:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/interaction-str-d9e7aefea121aefb4e94c6c9f060d016.png"},1106:e=>{e.exports=JSON.parse('{"permalink":"/BharatMLStack/blog/post-two","editUrl":"https://github.com/Meesho/BharatMLStack/tree/main/docs/blog/bharatmlstack-history/post-two/index.md","source":"@site/blog/bharatmlstack-history/post-two/index.md","title":"Building Meesho\u2019s ML Platform: Lessons from the First-Gen System (Part 2)","description":"BharatMLStack","date":"2023-04-10T00:00:00.000Z","tags":[{"inline":true,"label":"inferflow","permalink":"/BharatMLStack/blog/tags/inferflow"},{"inline":true,"label":"interaction-store","permalink":"/BharatMLStack/blog/tags/interaction-store"},{"inline":true,"label":"mlplatform","permalink":"/BharatMLStack/blog/tags/mlplatform"},{"inline":true,"label":"meesho","permalink":"/BharatMLStack/blog/tags/meesho"},{"inline":true,"label":"bharatmlstack","permalink":"/BharatMLStack/blog/tags/bharatmlstack"}],"readingTime":6.31,"hasTruncateMarker":false,"authors":[{"name":"Bhawani Singh","title":"Architect @ Meesho","url":"https://github.com/singh-bhawani","imageURL":"https://github.com/singh-bhawani.png","key":"bhawani","page":null},{"name":"Jigar Dave","title":"Lead Software Engineer @ Meesho","url":"https://github.com/jigarpatel26","imageURL":"https://github.com/jigarpatel26.png","key":"jigar","page":null},{"name":"Adarsha Das","title":"Senior Architect @ Meesho","url":"https://github.com/a0d00kc","imageURL":"https://github.com/a0d00kc.png","key":"adarsha","page":null}],"frontMatter":{"slug":"post-two","title":"Building Meesho\u2019s ML Platform: Lessons from the First-Gen System (Part 2)","authors":["bhawani","jigar","adarsha"],"date":"2023-4-10","tags":["inferflow","interaction-store","mlplatform","meesho","bharatmlstack"]},"unlisted":false,"prevItem":{"title":"Cracking the Code: Scaling Model Inference & Real-Time Embedding Search","permalink":"/BharatMLStack/blog/post-three"},"nextItem":{"title":"Building Meesho\u2019s ML Platform: From Chaos to Cutting-Edge (Part 1)","permalink":"/BharatMLStack/blog/post-one"}}')},2941:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/bms-7399e8796d2cd24617c432518ce3f312.png"},3532:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/mp-matrix-43994f433f78905ccbd10cfe284f3c9f.png"},4215:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var i=t(1106),r=t(4848),s=t(8453);const a={slug:"post-two",title:"Building Meesho\u2019s ML Platform: Lessons from the First-Gen System (Part 2)",authors:["bhawani","jigar","adarsha"],date:"2023-4-10",tags:["inferflow","interaction-store","mlplatform","meesho","bharatmlstack"]},o=void 0,l={authorsImageUrls:[void 0,void 0,void 0]},c=[{value:"Building Meesho\u2019s ML Platform: Lessons from the First-Gen System (Part 2)",id:"building-meeshos-ml-platform-lessons-from-the-first-gen-system-part-2",level:2},{value:"The Cost of Success",id:"the-cost-of-success",level:3},{value:"Scaling Pains (and Cassandra\u2019s Limits)",id:"scaling-pains-and-cassandras-limits",level:3},{value:"Interaction Store Woes",id:"interaction-store-woes",level:3},{value:"Silver Linings",id:"silver-linings",level:3},{value:"Round Two: Solving the Top 2 Bottlenecks",id:"round-two-solving-the-top-2-bottlenecks",level:3},{value:"Problem 1: No-Code Feature Retrieval for Model Inference",id:"problem-1-no-code-feature-retrieval-for-model-inference",level:4},{value:"Problem 2: Scaling Without Breaking the Bank",id:"problem-2-scaling-without-breaking-the-bank",level:4},{value:"Optimizing the Online Feature Store",id:"optimizing-the-online-feature-store",level:4},{value:"Optimizing the Interaction Store",id:"optimizing-the-interaction-store",level:4},{value:"Results",id:"results",level:4},{value:"The Catch: Our ML Hosting Hit a Hard Limit",id:"the-catch-our-ml-hosting-hit-a-hard-limit",level:4},{value:"Conclusion: From Firefighting to Future-Proofing",id:"conclusion-from-firefighting-to-future-proofing",level:3}];function d(e){const n={h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"BharatMLStack",src:t(2941).A+"",width:"1396",height:"460"})}),"\n",(0,r.jsx)(n.h2,{id:"building-meeshos-ml-platform-lessons-from-the-first-gen-system-part-2",children:"Building Meesho\u2019s ML Platform: Lessons from the First-Gen System (Part 2)"}),"\n",(0,r.jsx)(n.p,{children:"By late 2022, we had built something we were truly proud of\u2014a real-time ML serving system with a DAG-based executor, a feature store, and an interaction store powering key ranking and personalization models. It was a major milestone, the culmination of months of effort from data scientists, ML engineers, and backend teams. Our system was live, and we were ready to push the boundaries of experimentation.\nAnd it worked. Mostly.\nBut soon, cracks appeared. Every new model needed custom feature retrieval logic, DAGs became dense and unmanageable, and scaling turned into a constant firefight. Costs surged, and infra bottlenecks slowed experimentation. Our system worked, but it wasn\u2019t built for scale.\nThis is the story of how we tackled these challenges\u2014building Inferflow for seamless feature retrieval, optimizing real-time infra, and cutting costs while scaling to millions of QPS."}),"\n",(0,r.jsx)(n.h3,{id:"the-cost-of-success",children:"The Cost of Success"}),"\n",(0,r.jsx)(n.p,{children:"Every new Ranker model required its own feature set, often pulling from different entities. Each addition meant:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Adding new DAG nodes in IOP"}),"\n",(0,r.jsx)(n.li,{children:"Writing custom logic to fetch features from multiple sources (e.g., user, product, user \xd7 category)"}),"\n",(0,r.jsx)(n.li,{children:"Inferring intermediate features (e.g., extracting category from a product to fetch user \xd7 category data)"}),"\n",(0,r.jsx)(n.li,{children:"Optimizing I/O and dealing with the inevitable bugs"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"What began as clean DAGs soon turned into a tangled web of cross-dependent graphs. Every experimentation cycle meant new nodes, new dependencies, and slower iterations."}),"\n",(0,r.jsx)(n.h3,{id:"scaling-pains-and-cassandras-limits",children:"Scaling Pains (and Cassandra\u2019s Limits)"}),"\n",(0,r.jsx)(n.p,{children:"At some point, we were hitting:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"250\u2013300K reads/sec"}),"\n",(0,r.jsx)(n.li,{children:"1M writes/sec (during lean hours)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"All of this ran on Cassandra. While its distributed architecture had been proven in production, operating large-scale clusters came with considerable infrastructure overhead. Our proof-of-concept (POC) demonstrated throughput of around 100K ops/sec, but as we scaled further, the challenges grew. Ensuring node health, optimizing compaction, and maintaining storage balance became increasingly demanding. We also observed latency spikes under heavy load, alongside a sharp increase in total cost of ownership."}),"\n",(0,r.jsx)(n.h3,{id:"interaction-store-woes",children:"Interaction Store Woes"}),"\n",(0,r.jsx)(n.p,{children:"Our interaction store was another ticking time bomb:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\ud83d\udea8 Clusters kept growing in size and cost"}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\udea8 Latency spikes became increasingly frequent"}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\udea8 The DMC proxy occasionally lost locality of nodes against shards, causing cross-node communication and degraded performance"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Each time this happened, we had to manually rebalance shards just to restore stable latency, making operations unsustainable at scale."}),"\n",(0,r.jsx)(n.h3,{id:"silver-linings",children:"Silver Linings"}),"\n",(0,r.jsx)(n.p,{children:"Despite the chaos, the system was live and delivering value:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Real-time infrastructure was in production"}),"\n",(0,r.jsx)(n.li,{children:"Costs dropped by 60\u201370% compared to offline personalization"}),"\n",(0,r.jsx)(n.li,{children:"New experiments rolled out faster and more successfully"}),"\n",(0,r.jsx)(n.li,{children:"User engagement metrics improved"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"It wasn\u2019t perfect. It was far from easy. But it worked\u2014and that counted for a lot."}),"\n",(0,r.jsx)(n.h3,{id:"round-two-solving-the-top-2-bottlenecks",children:"Round Two: Solving the Top 2 Bottlenecks"}),"\n",(0,r.jsx)(n.p,{children:"With the first-gen system stretched to its limits, we stepped back. Conversations with data scientists and backend engineers revealed three recurring pain points:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Coding feature retrieval logic for every new model was becoming unsustainable"}),"\n",(0,r.jsx)(n.li,{children:"ML scale was exploding\u2014bringing rising infra costs with it"}),"\n",(0,r.jsx)(n.li,{children:"Real-time embedding search was the next big unlock"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"We tackled them one by one\u2014starting with the biggest pain point."}),"\n",(0,r.jsx)(n.h4,{id:"problem-1-no-code-feature-retrieval-for-model-inference",children:"Problem 1: No-Code Feature Retrieval for Model Inference"}),"\n",(0,r.jsx)(n.p,{children:"We noticed a pattern: for personalized ranking, models needed features from:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Product"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 User"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 User \xd7 Category"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Region, cohort, sub-category, etc."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"A key insight emerged: Entities that contribute features for a model always map back to the context entities."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"MP Dag",src:t(6039).A+"",width:"1272",height:"512"})}),"\n",(0,r.jsx)(n.p,{children:"With this, we designed Inferflow, a graph-driven feature retrieval and model orchestration system:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"1\ufe0f\u20e3 Inferflow takes a modelId and context IDs (e.g., userId, productIds)"}),"\n",(0,r.jsx)(n.li,{children:"2\ufe0f\u20e3 Loads a pre-defined feature retrieval graph from ZooKeeper"}),"\n",(0,r.jsx)(n.li,{children:"3\ufe0f\u20e3 Executes the graph to resolve entity relationships dynamically"}),"\n",(0,r.jsx)(n.li,{children:"4\ufe0f\u20e3 Outputs a 2D matrix of feature vectors"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\ud83d\udca1 The impact?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\ud83d\ude80 No more custom feature retrieval code\u2014just graph updates in config"}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\ude80 Feature consistency across experiments"}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\ude80 Faster iteration cycles for ranking, fraud detection, and beyond"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Here\u2019s a visual example that shows how this graph plays out during execution. We further extended the graph to call multiple models as needed:\n",(0,r.jsx)(n.img,{alt:"MP matrix",src:t(3532).A+"",width:"1262",height:"768"}),"\nWe built Inferflow in GoLang, using gRPC and Proto3 serialization for efficiency."]}),"\n",(0,r.jsx)(n.h4,{id:"problem-2-scaling-without-breaking-the-bank",children:"Problem 2: Scaling Without Breaking the Bank"}),"\n",(0,r.jsx)(n.p,{children:"With more ML use cases coming online, we needed to cut costs without compromising performance. We focused on:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\ud83d\udd39 Online Feature Store"}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\udd39 Interaction Store"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"optimizing-the-online-feature-store",children:"Optimizing the Online Feature Store"}),"\n",(0,r.jsx)(n.p,{children:"Our costs were concentrated in:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\ud83d\udccc Database (Cassandra)"}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\udccc Cache (Redis)"}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\udccc Running Pods (Java services)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"1\ufe0f\u20e3 Replacing Cassandra with ScyllaDB\nAs we hit the operational limits of large Cassandra clusters, we transitioned to ScyllaDB, which offered a seamless drop-in replacement without major code changes. The switch brought significant benefits:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Throughput: Matched or exceeded Cassandra's performance under identical workloads, even under high concurrency."}),"\n",(0,r.jsx)(n.li,{children:"Latency: Achieved consistently lower P99 latencies due to ScyllaDB's shard-per-core architecture and better I/O utilization."}),"\n",(0,r.jsx)(n.li,{children:"Cost Efficiency: Reduced infra footprint by ~70% through better CPU and memory efficiency, eliminating the need for over-provisioned nodes."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"2\ufe0f\u20e3 Finding the Right Cache\nTo reduce backend load and improve response times, we benchmarked multiple caching solutions\u2014Memcached, KeyDB, and Dragonfly\u2014under real production traffic patterns. Dragonfly stood out due to its robust architecture and operational simplicity:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Data Skew Handling: Efficiently managed extreme key hotness and uneven access patterns without performance degradation."}),"\n",(0,r.jsx)(n.li,{children:"Throughput: Delivered consistently high throughput, even with large object sizes and concurrent access."}),"\n",(0,r.jsx)(n.li,{children:"Ease of Adoption: Acted as a drop-in Redis replacement with full protocol compatibility\u2014no changes needed in application code or client libraries."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"3\ufe0f\u20e3 Moving to GoLang for Cost-Efficient Serving\nJava services were memory-heavy\u2014so we rewrote core services in GoLang. The results?"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Memory usage dropped by ~80%\n\u2705 CPU utilization was significantly lower\n\u2705 Faster, more efficient deployments"}),"\n",(0,r.jsx)(n.h4,{id:"optimizing-the-interaction-store",children:"Optimizing the Interaction Store"}),"\n",(0,r.jsx)(n.p,{children:"We realized that we only need a user\u2019s interaction data in Redis when they open the app. So, we implemented a tiered storage approach:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\ud83d\udccc Cold Tier (ScyllaDB)\u2014Stores click, order, wishlist events"}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\udccc Hot Tier (Redis)\u2014Loads a user\u2019s past interactions only when they open the app"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Smart Offloading: We introduced an inactivity tracker to detect when a user session ends. At that point, Redis data was flushed back to Scylla, reducing unnecessary writes."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"InteractionStore",src:t(461).A+"",width:"1242",height:"572"})}),"\n",(0,r.jsx)(n.h4,{id:"results",children:"Results"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Online Feature Store hit 1M QPS for the first time during the 2023 Mega Blockbuster Sale\u2014without breaking a sweat"}),"\n",(0,r.jsx)(n.li,{children:"Infra costs for Online Feature Store and Interaction Store dropped by ~60%"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"the-catch-our-ml-hosting-hit-a-hard-limit",children:"The Catch: Our ML Hosting Hit a Hard Limit"}),"\n",(0,r.jsx)(n.p,{children:"While planning for 2023 MBS, we ran into a critical scalability bottleneck:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c Insufficient compute availability in our region for ML instances"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Couldn\u2019t provision enough nodes to handle real-time inference at scale"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This forced us to rethink where and how we hosted our models. The existing setup was great for prototyping\u2014but it wasn\u2019t built to handle the bursty, high-QPS demands of real-world production workloads."}),"\n",(0,r.jsx)(n.h3,{id:"conclusion-from-firefighting-to-future-proofing",children:"Conclusion: From Firefighting to Future-Proofing"}),"\n",(0,r.jsx)(n.p,{children:"What started as an ambitious experiment turned into a real-time ML infrastructure that powered millions of requests per second. We battled scaling pains, rethought feature retrieval with Inferflow, and rebuilt our infra stack for efficiency\u2014driving down costs while improving experimentation velocity.\nBut new challenges emerged. Our infrastructure could now handle scale, but our ML model hosting setup hit a hard limit. With compute availability bottlenecks threatening real-time inference, we faced a critical decision: how do we make model serving as scalable and cost-efficient as the rest of our stack? That\u2019s the next piece of the puzzle\u2014and the story of Part 3."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},6039:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/mp-dag-976ff51caf25f09d977ccc10e70918f3.png"},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(6540);const r={},s=i.createContext(r);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);