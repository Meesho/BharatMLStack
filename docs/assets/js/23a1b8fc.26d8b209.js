"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8439],{211:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"predator/v1.0.0/architecture","title":"Architecture","description":"Predator is a scalable, high-performance model inference service built as a wrapper around the NVIDIA Triton Inference Server. It is designed to serve a variety of machine learning models (Deep Learning, Tree-based, etc.) with low latency in a Kubernetes (K8s) environment.","source":"@site/docs/predator/v1.0.0/architecture.md","sourceDirName":"predator/v1.0.0","slug":"/predator/v1.0.0/architecture","permalink":"/BharatMLStack/predator/v1.0.0/architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/Meesho/BharatMLStack/tree/main/docs/docs/predator/v1.0.0/architecture.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Architecture","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"v1.0.0","permalink":"/BharatMLStack/predator/v1.0.0"},"next":{"title":"Key Functionalities","permalink":"/BharatMLStack/predator/v1.0.0/functionalities"}}');var s=r(4848),t=r(8453);const o={title:"Architecture",sidebar_position:1},l="BharatMLStack - Predator",d={},c=[{value:"High-Level Design",id:"high-level-design",level:2},{value:"End-to-End Flow",id:"end-to-end-flow",level:3},{value:"Key Design Principles",id:"key-design-principles",level:3},{value:"Inference Engine: Triton Inference Server",id:"inference-engine-triton-inference-server",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Backends",id:"backends",level:3},{value:"Key Features",id:"key-features",level:3},{value:"Model Repository Structure",id:"model-repository-structure",level:2},{value:"Sample config.pbtxt",id:"sample-configpbtxt",level:3},{value:"Kubernetes Deployment Architecture",id:"kubernetes-deployment-architecture",level:2},{value:"Pod Architecture",id:"pod-architecture",level:3},{value:"Init Container",id:"init-container",level:4},{value:"Triton Inference Server Container",id:"triton-inference-server-container",level:4},{value:"Triton Server Image Strategy",id:"triton-server-image-strategy",level:3},{value:"Image Distribution Optimization",id:"image-distribution-optimization",level:3},{value:"Health Probes",id:"health-probes",level:3},{value:"Resource Configuration",id:"resource-configuration",level:3},{value:"Autoscaling Architecture",id:"autoscaling-architecture",level:3},{value:"Contributing",id:"contributing",level:2},{value:"Community &amp; Support",id:"community--support",level:2},{value:"License",id:"license",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"bharatmlstack---predator",children:"BharatMLStack - Predator"})}),"\n",(0,s.jsxs)(n.p,{children:["Predator is a scalable, high-performance model inference service built as a wrapper around the ",(0,s.jsx)(n.strong,{children:"NVIDIA Triton Inference Server"}),". It is designed to serve a variety of machine learning models (Deep Learning, Tree-based, etc.) with low latency in a ",(0,s.jsx)(n.strong,{children:"Kubernetes (K8s)"})," environment."]}),"\n",(0,s.jsxs)(n.p,{children:["The system integrates seamlessly with the ",(0,s.jsx)(n.strong,{children:"Online Feature Store (OnFS)"})," for real-time feature retrieval and uses ",(0,s.jsx)(n.strong,{children:"Horizon"})," as the deployment orchestration layer. Deployments follow a ",(0,s.jsx)(n.strong,{children:"GitOps"})," pipeline \u2014 Horizon generates Helm configurations, commits them to GitHub, and ",(0,s.jsx)(n.strong,{children:"Argo Sync"})," reconciles the desired state onto Kubernetes."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"high-level-design",children:"High-Level Design"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Predator HLD - End-to-end deployment and inference architecture",src:r(2202).A+"",width:"1824",height:"1124"})}),"\n",(0,s.jsx)(n.h3,{id:"end-to-end-flow",children:"End-to-End Flow"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Deployment Trigger"}),": An actor initiates deployment through ",(0,s.jsx)(n.strong,{children:"Trufflebox UI"}),", specifying the GCS path (",(0,s.jsx)(n.code,{children:"gcs://"}),") of the trained model. Separately, post-training pipelines write model artifacts to ",(0,s.jsx)(n.strong,{children:"GCS Artifactory"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Orchestration via Horizon"}),": Trufflebox UI communicates with ",(0,s.jsx)(n.strong,{children:"Horizon"}),", the deployment orchestration layer. Horizon generates the appropriate ",(0,s.jsx)(n.strong,{children:"Helm"})," chart configuration for the inference service."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GitOps Pipeline"}),": Horizon commits the Helm values to a ",(0,s.jsx)(n.strong,{children:"GitHub"})," repository. ",(0,s.jsx)(n.strong,{children:"Argo Sync"})," watches the repo and reconciles the desired state onto the Kubernetes cluster, creating or updating deployable units."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Deployable Units (Deployable 1 \u2026 N)"}),": Each deployable is an independent Kubernetes deployment that:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Downloads model artifacts from ",(0,s.jsx)(n.strong,{children:"GCS"})," at startup via an ",(0,s.jsx)(n.code,{children:"init.sh"})," script."]}),"\n",(0,s.jsxs)(n.li,{children:["Launches a ",(0,s.jsx)(n.strong,{children:"Triton Inference Server"})," instance loaded with the model."]}),"\n",(0,s.jsx)(n.li,{children:"Runs one or more pods, each containing the inference runtime and configured backends."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Triton Backends"}),": Each Triton instance supports pluggable backends based on the model type:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FIL"})," \u2014 GPU-accelerated tree-based models (XGBoost, LightGBM, Random Forest)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PyTorch"})," \u2014 Native PyTorch models via LibTorch."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Python"})," \u2014 Custom preprocessing/postprocessing or unsupported model formats."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TRT (TensorRT)"})," \u2014 GPU-optimized serialized TensorRT engines."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ONNX"})," \u2014 Framework-agnostic execution via ONNX Runtime."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DALI"})," \u2014 GPU-accelerated data preprocessing (image, audio, video)."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Autoscaling with KEDA"}),": The cluster uses ",(0,s.jsx)(n.strong,{children:"KEDA"})," (Kubernetes Event-Driven Autoscaling) to scale deployable pods based on custom metrics (CPU utilization, GPU utilization via DCGM, queue depth, etc.). The underlying ",(0,s.jsx)(n.strong,{children:"Kubernetes"})," scheduler places pods across GPU/CPU node pools."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-design-principles",children:"Key Design Principles"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GitOps-driven"}),": All deployment state is version-controlled in Git; Argo Sync ensures cluster state matches the declared configuration."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isolation per deployable"}),": Each model or model group gets its own deployable unit, preventing noisy-neighbor interference."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Init-based model loading"}),": Models are materialized to local disk before Triton starts, ensuring deterministic startup and no runtime dependency on remote storage."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pluggable backends"}),": The same infrastructure serves deep learning, tree-based, and custom models through Triton's backend abstraction."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"inference-engine-triton-inference-server",children:"Inference Engine: Triton Inference Server"}),"\n",(0,s.jsx)(n.p,{children:"NVIDIA Triton Inference Server is a high-performance model serving system designed to deploy ML and deep learning models at scale across CPUs and GPUs. It provides a unified inference runtime that supports multiple frameworks, optimized execution, and production-grade scheduling."}),"\n",(0,s.jsxs)(n.p,{children:["Triton operates as a standalone server that loads models from a model repository and exposes standardized HTTP/gRPC APIs. Predator uses ",(0,s.jsx)(n.strong,{children:"gRPC"})," for efficient request and response handling via the ",(0,s.jsx)(n.strong,{children:"helix client"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"core-components",children:"Core Components"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Repository"}),": Central directory where models are stored. Predator typically materializes the model repository onto local disk via an init container, enabling fast model loading and eliminating runtime dependency on remote storage during inference."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"backends",children:"Backends"}),"\n",(0,s.jsx)(n.p,{children:"A backend is the runtime responsible for executing a model. Each model specifies which backend runs it via configuration."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Backend"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"TensorRT"})}),(0,s.jsx)(n.td,{children:"GPU-optimized; executes serialized TensorRT engines (kernel fusion, FP16/INT8)."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"PyTorch"})}),(0,s.jsx)(n.td,{children:"Serves native PyTorch models via LibTorch."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"ONNX Runtime"})}),(0,s.jsx)(n.td,{children:"Framework-agnostic ONNX execution with TensorRT and other accelerators."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"TensorFlow"})}),(0,s.jsx)(n.td,{children:"Runs TensorFlow SavedModel format."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Python backend"})}),(0,s.jsx)(n.td,{children:"Custom Python code for preprocessing, postprocessing, or unsupported models."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Custom backends"})}),(0,s.jsx)(n.td,{children:"C++/Python backends for specialized or proprietary runtimes."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"DALI"})}),(0,s.jsx)(n.td,{children:"GPU-accelerated data preprocessing (image, audio, video)."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"FIL (Forest Inference Library)"})}),(0,s.jsx)(n.td,{children:"GPU-accelerated tree-based models (XGBoost, LightGBM, Random Forest)."})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamic batching"}),": Combines multiple requests into a single batch at runtime \u2014 higher GPU utilization, improved throughput, reduced latency variance."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Concurrent model execution"}),": Run multiple models or multiple instances of the same model; distribute load across GPUs."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model versioning"}),": Support multiple versions per model."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ensemble models"}),": Pipeline of models as an ensemble; eliminates intermediate network hops, reduces latency."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model instance scaling"}),": Multiple copies of a model for parallel inference and load isolation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Observability"}),": Prometheus metrics, granular latency, throughput, GPU utilization."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Warmup requests"}),": Preload kernels and avoid cold-start latency."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"model-repository-structure",children:"Model Repository Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"model_repository/\n\u251c\u2500\u2500 model_A/\n\u2502   \u251c\u2500\u2500 config.pbtxt\n\u2502   \u251c\u2500\u2500 1/\n\u2502   \u2502   \u2514\u2500\u2500 model.plan\n\u2502   \u251c\u2500\u2500 2/\n\u2502   \u2502   \u2514\u2500\u2500 model.plan\n\u251c\u2500\u2500 model_B/\n\u2502   \u251c\u2500\u2500 config.pbtxt\n\u2502   \u251c\u2500\u2500 1/\n\u2502       \u2514\u2500\u2500 model.py\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"config.pbtxt"})," file defines how Triton loads and executes a model: input/output tensors, batch settings, hardware execution, backend runtime, and optimization parameters. At minimum it defines: ",(0,s.jsx)(n.code,{children:"backend/platform"}),", ",(0,s.jsx)(n.code,{children:"max_batch_size"}),", ",(0,s.jsx)(n.code,{children:"inputs"}),", ",(0,s.jsx)(n.code,{children:"outputs"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"sample-configpbtxt",children:"Sample config.pbtxt"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'name: "product_ranking_model"\nplatform: "tensorrt_plan"\nmax_batch_size: 64\ninput [ { name: "input_embeddings" data_type: TYPE_FP16 dims: [ 128 ] }, { name: "context_features" data_type: TYPE_FP32 dims: [ 32 ] } ]\noutput [ { name: "scores" data_type: TYPE_FP32 dims: [ 1 ] } ]\ninstance_group [ { kind: KIND_GPU count: 2 gpus: [0] } ]\ndynamic_batching { preferred_batch_size: [8,16,32,64] max_queue_delay_microseconds: 2000 }\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"kubernetes-deployment-architecture",children:"Kubernetes Deployment Architecture"}),"\n",(0,s.jsxs)(n.p,{children:["Predator inference services are deployed on Kubernetes using ",(0,s.jsx)(n.strong,{children:"Helm-based"})," deployments for standardized, scalable, GPU-optimized model serving. Each deployment consists of Triton Inference Server wrapped within a Predator runtime, with autoscaling driven by CPU and GPU utilization."]}),"\n",(0,s.jsx)(n.h3,{id:"pod-architecture",children:"Pod Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Predator Pod\n\u251c\u2500\u2500 Init Container (Model Sync)\n\u251c\u2500\u2500 Triton Inference Server Container\n"})}),"\n",(0,s.jsx)(n.p,{children:"Model artifacts and runtime are initialized before inference traffic is accepted."}),"\n",(0,s.jsx)(n.h4,{id:"init-container",children:"Init Container"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Download model artifacts from cloud storage (GCS)."}),"\n",(0,s.jsx)(n.li,{children:"Populate the Triton model repository directory."}),"\n",(0,s.jsxs)(n.li,{children:["Example: ",(0,s.jsx)(n.code,{children:"gcloud storage cp -r gs://.../model-path/* /models"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Benefits: deterministic startup (Triton starts only after models are available), separation of concerns (image = runtime, repository = data)."}),"\n",(0,s.jsx)(n.h4,{id:"triton-inference-server-container",children:"Triton Inference Server Container"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Load model artifacts from local repository."}),"\n",(0,s.jsx)(n.li,{children:"Manage inference scheduling, request/response handling, and expose inference endpoints."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"triton-server-image-strategy",children:"Triton Server Image Strategy"}),"\n",(0,s.jsxs)(n.p,{children:["The Helm chart uses the Triton container image from the internal ",(0,s.jsx)(n.strong,{children:"artifact registry"}),". Production uses ",(0,s.jsx)(n.strong,{children:"custom-built"})," images (only required backends, e.g. TensorRT, Python) to reduce size and startup time. Unnecessary components are excluded; images are built internally and pushed to the registry."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Response Caching"}),": Custom cache plugins can be added at image build time for optional inference response caching \u2014 reducing redundant execution and GPU use for repeated inputs."]}),"\n",(0,s.jsx)(n.h3,{id:"image-distribution-optimization",children:"Image Distribution Optimization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Secondary boot disk image caching"}),": Images are pre-cached on GPU node pool secondary boot disks to avoid repeated pulls during scale-up and reduce pod startup time and cold-start latency."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image streaming"}),": Can be used to progressively pull layers for faster time-to-readiness during scaling."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"health-probes",children:"Health Probes"}),"\n",(0,s.jsxs)(n.p,{children:["Readiness and liveness use ",(0,s.jsx)(n.code,{children:"/v2/health/ready"}),". Triton receives traffic only after model loading; failed instances are restarted automatically."]}),"\n",(0,s.jsx)(n.h3,{id:"resource-configuration",children:"Resource Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Sample GPU resource config:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"limits:\n  cpu: 7000m\n  memory: 28Gi\n  gpu: 1\n"})}),"\n",(0,s.jsx)(n.h3,{id:"autoscaling-architecture",children:"Autoscaling Architecture"}),"\n",(0,s.jsxs)(n.p,{children:["Predator uses ",(0,s.jsx)(n.strong,{children:"KEDA"})," (Kubernetes Event-Driven Autoscaling) for scaling deployable pods. KEDA supports custom metric sources including:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CPU / Memory utilization"})," for CPU-based deployments."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU utilization"})," via ",(0,s.jsx)(n.strong,{children:"DCGM"})," (Data Center GPU Manager) for GPU pods \u2014 covering utilization, memory, power, etc."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Custom Prometheus queries"})," for application-level scaling signals (e.g., inference queue depth, request latency)."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"KEDA ScaledObjects are configured per deployable, enabling fine-grained, independent scaling for each model or model group."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"contributing",children:"Contributing"}),"\n",(0,s.jsxs)(n.p,{children:["We welcome contributions! See the ",(0,s.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/CONTRIBUTING.md",children:"Contributing Guide"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"community--support",children:"Community & Support"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Discord"}),": ",(0,s.jsx)(n.a,{href:"https://discord.gg/XkT7XsV2AU",children:"community chat"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Issues"}),": ",(0,s.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/issues",children:"GitHub Issues"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Email"}),": ",(0,s.jsx)(n.a,{href:"mailto:ml-oss@meesho.com",children:"ml-oss@meesho.com"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"license",children:"License"}),"\n",(0,s.jsxs)(n.p,{children:["BharatMLStack is open-source under the ",(0,s.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/LICENSE.md",children:"BharatMLStack Business Source License 1.1"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("strong",{children:"Built with \u2764\ufe0f for the ML community from Meesho"})}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("strong",{children:"If you find this useful, \u2b50\ufe0f the repo \u2014 your support means the world to us!"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},2202:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/v1.0.0-predator-hld-949215d6604ae103e724c3978e803443.png"},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var i=r(6540);const s={},t=i.createContext(s);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);