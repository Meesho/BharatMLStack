"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6273],{1012:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/interaction-store-v0-68167b64c6e462ef2f177f0f86d55bda.png"},3190:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/schema-d699efc400ed0f83bba421c1f55ab211.png"},3518:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/old-batch-arch-bc2cedbc1fed0fc6f08479ba8fe52996.png"},3983:e=>{e.exports=JSON.parse('{"permalink":"/BharatMLStack/blog/post-one","editUrl":"https://github.com/Meesho/BharatMLStack/tree/main/docs/blog/bharatmlstack-history/post-one/index.md","source":"@site/blog/bharatmlstack-history/post-one/index.md","title":"Building Meesho\u2019s ML Platform: From Chaos to Cutting-Edge (Part 1)","description":"BharatMLStack","date":"2022-11-15T00:00:00.000Z","tags":[{"inline":true,"label":"online-feature-store","permalink":"/BharatMLStack/blog/tags/online-feature-store"},{"inline":true,"label":"interaction-store","permalink":"/BharatMLStack/blog/tags/interaction-store"},{"inline":true,"label":"mlplatform","permalink":"/BharatMLStack/blog/tags/mlplatform"},{"inline":true,"label":"meesho","permalink":"/BharatMLStack/blog/tags/meesho"}],"readingTime":10.25,"hasTruncateMarker":false,"authors":[{"name":"Adarsha Das","title":"Senior Architect @ Meesho","url":"https://github.com/a0d00kc","imageURL":"https://github.com/a0d00kc.png","key":"adarsha","page":null},{"name":"Aditya Kumar","title":"SDE-III @ Meesho","url":"https://github.com/Adit2607","imageURL":"https://github.com/Adit2607.png","key":"aditya","page":null},{"name":"Bhawani Singh","title":"SDE-IV @ Meesho","url":"https://github.com/singh-bhawani","imageURL":"https://github.com/singh-bhawani.png","key":"bhawani","page":null},{"name":"Jigar Dave","title":"SDE-IV @ Meesho","url":"https://github.com/jigarpatel26","imageURL":"https://github.com/jigarpatel26.png","key":"jigar","page":null}],"frontMatter":{"slug":"post-one","title":"Building Meesho\u2019s ML Platform: From Chaos to Cutting-Edge (Part 1)","authors":["adarsha","aditya","bhawani","jigar"],"date":"2022-11-15T00:00:00.000Z","tags":["online-feature-store","interaction-store","mlplatform","meesho"]},"unlisted":false}')},5728:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/online-feature-store-v0-86ec0010947ae24621f39ebd0d1729ca.png"},7131:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/first-gen-arch-7c0b286810aecb7eff42b48f51caee1f.png"},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(6540);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}},8831:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>a,metadata:()=>t,toc:()=>d});var t=i(3983),s=i(4848),r=i(8453);const a={slug:"post-one",title:"Building Meesho\u2019s ML Platform: From Chaos to Cutting-Edge (Part 1)",authors:["adarsha","aditya","bhawani","jigar"],date:new Date("2022-11-15T00:00:00.000Z"),tags:["online-feature-store","interaction-store","mlplatform","meesho"]},o=void 0,l={authorsImageUrls:[void 0,void 0,void 0,void 0]},d=[{value:"The Genesis: How a Friday Night Roast Sparked Meesho\u2019s ML Platform",id:"the-genesis-how-a-friday-night-roast-sparked-meeshos-ml-platform",level:2},{value:"The Turning Point: From Batch to Real-Time",id:"the-turning-point-from-batch-to-real-time",level:2},{value:"First Generation Design",id:"first-generation-design",level:2},{value:"1. IOP Framework: A Real-Time DAG Executor",id:"1-iop-framework-a-real-time-dag-executor",level:3},{value:"2. Online Feature Store - 0th Version",id:"2-online-feature-store---0th-version",level:3},{value:"3. Interaction Store - 0th Version",id:"3-interaction-store---0th-version",level:3},{value:"Building the Online Feature Store - 0th Version",id:"building-the-online-feature-store---0th-version",level:2},{value:"Choosing the Right Tech Stack",id:"choosing-the-right-tech-stack",level:3},{value:"Streamlining the Data Flow",id:"streamlining-the-data-flow",level:3},{value:"The Challenges: Data Format and Storage",id:"the-challenges-data-format-and-storage",level:2},{value:"Feature Consistency",id:"feature-consistency",level:3},{value:"TTL Granularity",id:"ttl-granularity",level:3},{value:"Extensibility Across Databases",id:"extensibility-across-databases",level:3},{value:"Overcoming Technical Constraints",id:"overcoming-technical-constraints",level:2},{value:"The Solution: Schema Separation",id:"the-solution-schema-separation",level:2},{value:"Tracking Changes in Feature Groups",id:"tracking-changes-in-feature-groups",level:2},{value:"Common Real-World Scenarios:",id:"common-real-world-scenarios",level:3},{value:"The Solution: Schema Versioning",id:"the-solution-schema-versioning",level:2},{value:"Backward Compatibility",id:"backward-compatibility",level:3},{value:"Partial Availability Handling",id:"partial-availability-handling",level:3},{value:"Safe Writes Without Pipeline Pauses",id:"safe-writes-without-pipeline-pauses",level:3},{value:"Interaction Store - 0th Version",id:"interaction-store---0th-version",level:2},{value:"Event Ingestion",id:"event-ingestion",level:2},{value:"Storage Design",id:"storage-design",level:2},{value:"Why Redis?",id:"why-redis",level:3},{value:"Storage Structure",id:"storage-structure",level:3},{value:"Built-in Guardrails",id:"built-in-guardrails",level:3},{value:"Conclusion: Laying the Foundation for Real-Time ML",id:"conclusion-laying-the-foundation-for-real-time-ml",level:2}];function h(e){const n={a:"a",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"BharatMLStack",src:i(9930).A+"",width:"1472",height:"892"})}),"\n",(0,s.jsx)(n.h2,{id:"the-genesis-how-a-friday-night-roast-sparked-meeshos-ml-platform",children:"The Genesis: How a Friday Night Roast Sparked Meesho\u2019s ML Platform"}),"\n",(0,s.jsx)(n.p,{children:"It all started in early 2022, over a casual Friday evening catch-up. Like many great origin stories, this one began with friendly banter between a group of backend engineers and data scientists. As the conversations unfolded, so did the roasting\u2014until one remark hit a little too close to home:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:'"Why are we still crunching data for Monthly Active Users (MAU) when the next day it\u2019s all about Daily Active Users (DAU)?"'})}),"\n",(0,s.jsx)(n.p,{children:"The laughter died down, and the question lingered. When we regrouped on Monday\u2014clear-headed and slightly reflective\u2014we decided to dig into the numbers. What they discovered was quite revealing: a large portion of compute resources wasn\u2019t being put to good use.\nMuch of the system\u2019s effort was spent supporting users who weren\u2019t actively engaging, and even for new users, the experience wasn\u2019t optimized to make a meaningful impact."}),"\n",(0,s.jsxs)(n.p,{children:["At the same time, Meesho had just launched a company-wide initiative to reduce costs\u2014and every team had to contribute. This realization sparked the journey that would eventually lead to the ",(0,s.jsx)(n.strong,{children:"Meesho ML Platform"}),", known today as ",(0,s.jsx)(n.strong,{children:"BharatMLStack"}),"."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Alt Text",src:i(3518).A+"",width:"1600",height:"1078"})}),"\n",(0,s.jsx)(n.p,{children:"Before the ML Platform, our recommendation and ranking pipelines followed a batch processing approach:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Ingestion"}),": The Data Platform team executed ETL jobs to ingest raw user data\u2014including user profiles, interaction logs, and product impressions\u2014into designated S3 buckets."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 1"}),": Embedding Generation: On the Data Science side, Spark jobs pulled data from multiple S3 sources, cleaned and preprocessed it, and applied matrix factorization to generate user and item embeddings. The processed data and embeddings were then stored back in S3 in a structured format."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 2"}),": Candidate Generation (CG): In this stage, Spark jobs leveraged embeddings and historical interaction data to generate candidate recommendations for users. These candidate lists were subsequently written to S3."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Layer 3"}),": Ranking and Merging \u2013 A final round of processing ranked the generated candidates using ML models, combined different candidate lists, and stored the final ranked recommendations in a caching system."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Serving"}),': A microservice retrieved ranked recommendations from an in-memory data store via exposed APIs, delivering personalized listings across key surfaces such as "For You" and Category Landing Pages (CLP).']}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This approach held up well\u2014until Meesho started seeing a significant surge in traffic."}),"\n",(0,s.jsx)(n.h2,{id:"the-turning-point-from-batch-to-real-time",children:"The Turning Point: From Batch to Real-Time"}),"\n",(0,s.jsxs)(n.p,{children:["At this time, the team was iterating on new ",(0,s.jsx)(n.strong,{children:"Ranker models"}),", and real-time inference seemed like the next logical step. But Rankers needed ",(0,s.jsx)(n.strong,{children:"real-time feature retrieval"}),", which meant an ",(0,s.jsx)(n.strong,{children:"online feature store"})," had to be built first."]}),"\n",(0,s.jsxs)(n.p,{children:["Exploring open-source options led to ",(0,s.jsx)(n.strong,{children:"cost vs. performance trade-offs"}),", but Meesho\u2019s surging traffic meant that ",(0,s.jsx)(n.strong,{children:"latency and stability were non-negotiable"}),". After multiple debates and stakeholder discussions, a bold decision was made:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"We would build our own feature store."})}),"\n",(0,s.jsxs)(n.p,{children:["Meanwhile, efforts began to bring ",(0,s.jsx)(n.strong,{children:"Candidate Generators (CGs)"})," to real-time. The challenge? ",(0,s.jsx)(n.strong,{children:"Storing and retrieving user interactions quickly enough"})," to power real-time recommendations."]}),"\n",(0,s.jsxs)(n.p,{children:["As the team dove deeper, a new roadblock emerged:",(0,s.jsx)(n.br,{}),"\n","Our ML jobs were orchestrated using ",(0,s.jsx)(n.strong,{children:"Airflow DAGs"}),", giving data scientists flexibility in experimentation. But transitioning to real-time execution threatened this agility. Every change would now require backend engineering support, ",(0,s.jsx)(n.strong,{children:"slowing down iteration cycles"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["That\u2019s when the idea struck:",(0,s.jsx)(n.br,{}),"\n","We needed a ",(0,s.jsx)(n.strong,{children:"framework for real-time DAG execution"}),"\u2014one that preserved the same flexibility as Airflow but worked for ",(0,s.jsx)(n.strong,{children:"streaming data"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["This moment shaped the ",(0,s.jsx)(n.strong,{children:"next phase of our journey"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"first-generation-design",children:"First Generation Design"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Alt Text",src:i(7131).A+"",width:"1600",height:"1006"})}),"\n",(0,s.jsx)(n.h1,{id:"laying-the-groundwork-the-first-gen-ml-platform",children:"Laying the Groundwork: The First-Gen ML Platform"}),"\n",(0,s.jsx)(n.p,{children:"To solve these challenges, the team built three foundational components:"}),"\n",(0,s.jsx)(n.h3,{id:"1-iop-framework-a-real-time-dag-executor",children:"1. IOP Framework: A Real-Time DAG Executor"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reusable Nodes"}),": Each DAG node (e.g., an invocation to a CG service, a ranker, or a filter) had to be implemented only once. After that, it could be reused across any workflow by referencing it in config."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Config-driven Dynamic Graphs"}),": Execution graphs were defined as adjacency lists stored in ",(0,s.jsx)(n.strong,{children:"ZooKeeper"}),", allowing teams to modify the sequence or structure of operations without touching application code."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plug-and-play CGs"}),": The Candidate Generator interface was preserved, so a single CG node could call any CG service by passing ",(0,s.jsx)(n.code,{children:"cg_name"})," in the request. This drastically reduced the code surface area and improved maintainability."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Production-Grade DAGs"}),": DAGs were designed to execute in ",(0,s.jsx)(n.strong,{children:"low-latency real-time environments"}),", with support for ",(0,s.jsx)(n.strong,{children:"parallel execution, retries, and branching"}),"."]}),"\n"]}),"\n",(0,s.jsx)("u",{children:(0,s.jsx)(n.a,{href:"https://www.meesho.io/blog/rebuilding-meeshos-ranking-platform",children:"More about IOP DAG"})}),"\n",(0,s.jsx)(n.h3,{id:"2-online-feature-store---0th-version",children:"2. Online Feature Store - 0th Version"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Used ",(0,s.jsx)(n.strong,{children:"Cassandra"})," and ",(0,s.jsx)(n.strong,{children:"Redis"})," for low-latency feature serving."]}),"\n",(0,s.jsxs)(n.li,{children:["Maintained feature consistency using ",(0,s.jsx)(n.strong,{children:"Feature Groups"})," with TTL-based expiry."]}),"\n",(0,s.jsxs)(n.li,{children:["A hybrid schema was used: feature keys stored in ",(0,s.jsx)(n.strong,{children:"ZooKeeper"}),", data stored in ",(0,s.jsx)(n.strong,{children:"compact arrays"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-interaction-store---0th-version",children:"3. Interaction Store - 0th Version"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Captured real-time user interactions like clicks, orders, and add-to-cart events."}),"\n",(0,s.jsxs)(n.li,{children:["Stored event data in ",(0,s.jsx)(n.strong,{children:"Redis ZSETs (sorted sets)"})," to enable fast lookups for recommendation engines."]}),"\n",(0,s.jsxs)(n.li,{children:["Provided an API to fetch a user's ",(0,s.jsxs)(n.strong,{children:["last ",(0,s.jsx)(n.em,{children:"k"})," interactions"]})," or ",(0,s.jsx)(n.strong,{children:"interactions within a time window"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["With these components in place, ",(0,s.jsx)(n.strong,{children:"real-time ML at Meesho became a reality"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"This was just the beginning."}),"\n",(0,s.jsx)(n.h2,{id:"building-the-online-feature-store---0th-version",children:"Building the Online Feature Store - 0th Version"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Alt text",src:i(5728).A+"",width:"1574",height:"562"})}),"\n",(0,s.jsx)(n.h3,{id:"choosing-the-right-tech-stack",children:"Choosing the Right Tech Stack"}),"\n",(0,s.jsxs)(n.p,{children:["We spent considerable time evaluating various databases, caches, and communication protocols for our ",(0,s.jsx)(n.strong,{children:"online feature store"}),". After carefully weighing ",(0,s.jsx)(n.strong,{children:"cost, latency, throughput"}),", and ",(0,s.jsx)(n.strong,{children:"operational stability"}),", we settled on a combination of:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cassandra"})," and ",(0,s.jsx)(n.strong,{children:"Redis"})," for storage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"gRPC + Proto3"})," as our communication layer"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"streamlining-the-data-flow",children:"Streamlining the Data Flow"}),"\n",(0,s.jsx)(n.p,{children:"To keep things simple in the initial version:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature engineering jobs"})," wrote raw outputs to an ",(0,s.jsx)(n.strong,{children:"S3 bucket"})]}),"\n",(0,s.jsxs)(n.li,{children:["A ",(0,s.jsx)(n.strong,{children:"daily feature push job"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Read from S3"}),"\n",(0,s.jsxs)(n.li,{children:["Grouped related features into ",(0,s.jsx)(n.strong,{children:"Feature Groups"})," (ensuring consistency)"]}),"\n",(0,s.jsxs)(n.li,{children:["Pushed them to ",(0,s.jsx)(n.strong,{children:"Kafka"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"For features requiring frequent updates:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ad-hoc jobs"})," computed features in higher frequency"]}),"\n",(0,s.jsxs)(n.li,{children:["These jobs pushed to both ",(0,s.jsx)(n.strong,{children:"Kafka"})," and ",(0,s.jsx)(n.strong,{children:"S3"}),"  (S3 preserved historical data for future model training)"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-challenges-data-format-and-storage",children:"The Challenges: Data Format and Storage"}),"\n",(0,s.jsxs)(n.p,{children:["One of the most critical design challenges was how to store feature data ",(0,s.jsx)(n.strong,{children:"efficiently and consistently"}),", especially in databases like ",(0,s.jsx)(n.strong,{children:"Cassandra"})," and ",(0,s.jsx)(n.strong,{children:"Redis"}),", which come with unique storage constraints."]}),"\n",(0,s.jsx)(n.p,{children:"We had to solve for three key requirements:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h3,{id:"feature-consistency",children:"Feature Consistency"}),"\n",(0,s.jsxs)(n.p,{children:["When a feature group contains features like ",(0,s.jsx)(n.code,{children:"order_count_1h"})," and ",(0,s.jsx)(n.code,{children:"click_count_1h"}),", both must reflect the ",(0,s.jsx)(n.strong,{children:"same time window"}),". Inconsistent updates would lead to ",(0,s.jsx)(n.strong,{children:"unreliable model predictions"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h3,{id:"ttl-granularity",children:"TTL Granularity"}),"\n",(0,s.jsxs)(n.p,{children:["Each feature group required an ",(0,s.jsx)(n.strong,{children:"expiry timestamp"}),", so that ",(0,s.jsx)(n.strong,{children:"all features within it expired together"}),"\u2014preserving consistency during reads."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h3,{id:"extensibility-across-databases",children:"Extensibility Across Databases"}),"\n",(0,s.jsxs)(n.p,{children:["We anticipated that infra needs would evolve. To future-proof our system, the data format was designed to be ",(0,s.jsx)(n.strong,{children:"decoupled from DB-specific layouts"}),", enabling portability to systems like ",(0,s.jsx)(n.strong,{children:"ScyllaDB"}),", ",(0,s.jsx)(n.strong,{children:"DynamoDB"}),", ",(0,s.jsx)(n.strong,{children:"HBase"}),", or ",(0,s.jsx)(n.strong,{children:"BigTable"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"overcoming-technical-constraints",children:"Overcoming Technical Constraints"}),"\n",(0,s.jsx)(n.p,{children:'At the time, we were using Cassandra, which not only imposed a soft limit of 75 columns per row, but also exhibited significant performance degradation as the number of columns increased further, particularly in memory constrained machines. Wide rows caused high memory usage during reads, unpredictable latencies due to heavy deserialization overhead, and inefficiencies during compactions and repairs. This ruled out the naive "one column per feature" approach. We needed a format that was compact, minimized the number of columns, and remained efficient and portable across different storage systems.'}),"\n",(0,s.jsx)(n.h2,{id:"the-solution-schema-separation",children:"The Solution: Schema Separation"}),"\n",(0,s.jsx)(n.p,{children:"We introduced the concept of Feature Groups\u2014logical groupings of features that must remain consistent with one another.\nTo represent these groups efficiently, we adopted a layered storage approach:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Labels (Keys)"})," were stored in ZooKeeper, serving as the schema."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Values"})," were stored as a comma-separated string array in Cassandra or Redis."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Expiry Timestamp and Schema Version"})," were appended using a semi-colon delimiter at the end of the string."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Example:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"feature_1_value,feature_2_value,feature_3_value;expiry_ts\n"})}),"\n",(0,s.jsx)(n.p,{children:"This format allowed:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Consistent writes and reads at the group level"}),"\n",(0,s.jsx)(n.li,{children:"Easy parsing of feature values using the schema lookup from ZooKeeper"}),"\n",(0,s.jsx)(n.li,{children:"Efficient storage with minimal DB column usage"}),"\n",(0,s.jsx)(n.li,{children:"Support for per-group TTLs and schema evolution"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"tracking-changes-in-feature-groups",children:"Tracking Changes in Feature Groups"}),"\n",(0,s.jsx)(n.p,{children:"Feature groups don\u2019t stay static. As models evolve, features get added, renamed, or removed. But schema changes often go live before the data is ready\u2014and stopping ingestion just to wait for everything to align isn't feasible."}),"\n",(0,s.jsx)(n.h3,{id:"common-real-world-scenarios",children:"Common Real-World Scenarios:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A new feature is added to the schema, but ingestion jobs still use the older schema version."}),"\n",(0,s.jsx)(n.li,{children:"Ongoing writes don\u2019t include the newly added feature, and stopping ingestion would break freshness for existing features."}),"\n",(0,s.jsx)(n.li,{children:"During serving, models request a mix of old and new features, depending on rollout stages."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-solution-schema-versioning",children:"The Solution: Schema Versioning"}),"\n",(0,s.jsx)(n.p,{children:"We solved this with versioned feature group schemas, which unlocked several capabilities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h3,{id:"backward-compatibility",children:"Backward Compatibility"}),"\n","Older ingestion jobs can continue writing using older schema versions. During reads, the system uses the schema version embedded in the value to interpret the data correctly."]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h3,{id:"partial-availability-handling",children:"Partial Availability Handling"}),"\n","During inference, if some features in the request aren\u2019t available (due to rollout delays or missing data), the system serves default values, ensuring the inference call doesn\u2019t fail."]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.h3,{id:"safe-writes-without-pipeline-pauses",children:"Safe Writes Without Pipeline Pauses"}),"\n","With schema versioning, we no longer had to stop ingestion pipelines for schema updates. Writes using previous versions can continue safely, and downstream consumers evolve independently.\nThis design gave us the flexibility to move fast without breaking things\u2014preserving data quality, enabling experimentation, and ensuring reliability at scale."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Alt Text",src:i(3190).A+"",width:"1600",height:"599"})}),"\n",(0,s.jsx)(n.h2,{id:"interaction-store---0th-version",children:"Interaction Store - 0th Version"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Alt Text",src:i(1012).A+"",width:"1600",height:"518"})}),"\n",(0,s.jsxs)(n.p,{children:["To power real-time Candidate Generators (CGs), we needed fast access to user behavior signals\u2014like what a user recently clicked, ordered, or added to their cart. These interactions form the basis for many real-time recommendations, such as ",(0,s.jsx)(n.strong,{children:"Similar Products"}),", ",(0,s.jsx)(n.strong,{children:"People Also Viewed"}),", or ",(0,s.jsx)(n.strong,{children:"Recently Ordered Again"}),".\nFor the ",(0,s.jsx)(n.strong,{children:"0th version"})," of the Interaction Store, we focused on a design that was ",(0,s.jsx)(n.strong,{children:"simple, fast, and reliable"})," \u2014 optimized for high-throughput ingestion and low-latency lookups."]}),"\n",(0,s.jsx)(n.h2,{id:"event-ingestion",children:"Event Ingestion"}),"\n",(0,s.jsx)(n.p,{children:"We instrumented our backend services to emit key user interaction events to Kafka in real time. These included:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Click"}),"\n",(0,s.jsx)(n.li,{children:"Order"}),"\n",(0,s.jsx)(n.li,{children:"Add to Cart"}),"\n",(0,s.jsx)(n.li,{children:"Wishlist"}),"\n",(0,s.jsx)(n.li,{children:"Share"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Each event carried essential metadata:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"userId \u2014 uniquely identifies the user"}),"\n",(0,s.jsx)(n.li,{children:"productId \u2014 the item being interacted with"}),"\n",(0,s.jsx)(n.li,{children:"timestamp \u2014 the moment the interaction occurred"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This decoupled the interaction logging from storage, allowing ingestion and consumption to scale independently."}),"\n",(0,s.jsx)(n.h2,{id:"storage-design",children:"Storage Design"}),"\n",(0,s.jsx)(n.p,{children:"To store these events, we built Kafka consumers that processed the incoming streams and wrote the data into Redis, using sorted sets (ZSETs) as the primary data structure."}),"\n",(0,s.jsx)(n.h3,{id:"why-redis",children:"Why Redis?"}),"\n",(0,s.jsx)(n.p,{children:"Redis gave us:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low-latency"})," reads and writes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Time-ordered data"})," using ZSETs (via score = timestamp)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Native TTL support"}),", if needed in later versions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"In-memory performance"})," \u2014ideal for real-time CGs"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"storage-structure",children:"Storage Structure"}),"\n",(0,s.jsx)(n.p,{children:"Each user\u2019s interactions were stored using a composite key format, uniquely identifying the user and interaction type. This structure allowed efficient organization and quick retrieval of recent activity for recommendation generation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"userId_eventType \u2192 ZSET[...(pid, ts)...]\n"})}),"\n",(0,s.jsx)(n.p,{children:"Within each ZSET:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.strong,{children:"timestamp"})," served as the score, maintaining temporal order"]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.strong,{children:"productId"})," (optionally with metadata) was the ",(0,s.jsx)(n.strong,{children:"value"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This allowed us to efficiently retrieve the interactions with HTTP-based API server with two query modes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Fetch the ",(0,s.jsx)(n.strong,{children:"last k interactions"})," of a specific type for a given user with  ",(0,s.jsx)(n.code,{children:"ZREVRANGE(userId_eventType, count)"})]}),"\n",(0,s.jsxs)(n.li,{children:["Retrieve ",(0,s.jsx)(n.strong,{children:"all interactions within a time range"})," (e.g., last 24 hours) with ",(0,s.jsx)(n.code,{children:"ZREVRANGEBYSCORE(userId_eventType, timeRange)"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"built-in-guardrails",children:"Built-in Guardrails"}),"\n",(0,s.jsx)(n.p,{children:"Since Redis was the sole store, we implemented High Availability (HA) to prevent data loss. To optimize memory usage, we also enforced size limits per event type\u2014only storing the last k interactions per user, with older entries getting truncated."}),"\n",(0,s.jsx)(n.h2,{id:"conclusion-laying-the-foundation-for-real-time-ml",children:"Conclusion: Laying the Foundation for Real-Time ML"}),"\n",(0,s.jsxs)(n.p,{children:["In this first phase, we tackled the ",(0,s.jsx)(n.strong,{children:"fundamentals"}),"\u2014shifting from batch-based recommendations to a ",(0,s.jsx)(n.strong,{children:"real-time Recommendation"})," using ML platform that could keep up with Meesho\u2019s growth."]}),"\n",(0,s.jsxs)(n.p,{children:["With the ",(0,s.jsx)(n.strong,{children:"IOP Framework"}),", ",(0,s.jsx)(n.strong,{children:"Online Feature Store"}),", and ",(0,s.jsx)(n.strong,{children:"Interaction Store"}),", we built the core infrastructure to support real-time personalization at scale. These wins have already unlocked:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Faster, more dynamic recommendations for millions of users."}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Better infrastructure efficiency, reducing wasted compute power."}),"\n",(0,s.jsx)(n.li,{children:"\u2705 A flexible, modular system that allows for further experimentation."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["But this is just the beginning. While we've solved key challenges, ",(0,s.jsx)(n.strong,{children:"certain roadblocks remain"})," \u2014from optimizing ",(0,s.jsx)(n.strong,{children:"cost-performance trade-offs"})," to ",(0,s.jsx)(n.strong,{children:"seamlessly evolving schemas"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["This foundational work laid the path for a reliable and scalable ",(0,s.jsx)(n.strong,{children:"real-time feature serving layer"}),"."]})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},9930:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/bharatmlstack-72e1796337bfa224dee2a0f59ec4e2da.png"}}]);