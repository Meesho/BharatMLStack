"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9473],{6692:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"sdks/python/v1.0.0/spark_feature_push_client","title":"Spark client","description":"PyPI version","source":"@site/docs/sdks/python/v1.0.0/spark_feature_push_client.md","sourceDirName":"sdks/python/v1.0.0","slug":"/sdks/python/v1.0.0/spark_feature_push_client","permalink":"/BharatMLStack/sdks/python/v1.0.0/spark_feature_push_client","draft":false,"unlisted":false,"editUrl":"https://github.com/Meesho/BharatMLStack/tree/main/docs/docs/sdks/python/v1.0.0/spark_feature_push_client.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Spark client","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"GRPC Feature client","permalink":"/BharatMLStack/sdks/python/v1.0.0/grpc_feature_client"}}');var r=a(4848),i=a(8453);const s={title:"Spark client",sidebar_position:1},o="Spark Feature Push Client",l={},c=[{value:"Installation",id:"installation",level:2},{value:"Dependencies",id:"dependencies",level:2},{value:"Architecture Role",id:"architecture-role",level:2},{value:"Features",id:"features",level:2},{value:"When to Use This Client",id:"when-to-use-this-client",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Related Packages",id:"related-packages",level:2},{value:"License",id:"license",level:2},{value:"Contributing",id:"contributing",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Supported Data Sources",id:"supported-data-sources",level:2},{value:"1. Database Tables",id:"1-database-tables",level:3},{value:"2. Cloud Storage - Parquet",id:"2-cloud-storage---parquet",level:3},{value:"3. Cloud Storage - Delta",id:"3-cloud-storage---delta",level:3},{value:"Configuration Examples",id:"configuration-examples",level:2},{value:"Basic Pipeline",id:"basic-pipeline",level:3},{value:"Reading from Multiple Sources",id:"reading-from-multiple-sources",level:3},{value:"Protobuf Serialization &amp; Kafka Publishing",id:"protobuf-serialization--kafka-publishing",level:3},{value:"Data Type Handling",id:"data-type-handling",level:2},{value:"Scalar Types",id:"scalar-types",level:3},{value:"Vector Types",id:"vector-types",level:3},{value:"Production Pipeline Example",id:"production-pipeline-example",level:2},{value:"Configuration Options",id:"configuration-options",level:2},{value:"Client Configuration",id:"client-configuration",level:3},{value:"Protobuf Serialization Options",id:"protobuf-serialization-options",level:3},{value:"Kafka Publishing Options",id:"kafka-publishing-options",level:3},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"Spark Optimizations",id:"spark-optimizations",level:3},{value:"Memory Management",id:"memory-management",level:3},{value:"Kafka Throughput",id:"kafka-throughput",level:3},{value:"Monitoring &amp; Debugging",id:"monitoring--debugging",level:2},{value:"DataFrame Inspection",id:"dataframe-inspection",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"Integration with Other SDKs",id:"integration-with-other-sdks",level:2},{value:"With gRPC Feature Client",id:"with-grpc-feature-client",level:3},{value:"With HTTP Feature Client (bharatml_common)",id:"with-http-feature-client-bharatml_common",level:3},{value:"Common Use Cases",id:"common-use-cases",level:2},{value:"1. Daily Batch ETL",id:"1-daily-batch-etl",level:3},{value:"2. Historical Backfill",id:"2-historical-backfill",level:3},{value:"3. Real-time Streaming (Advanced)",id:"3-real-time-streaming-advanced",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debug Mode",id:"debug-mode",level:3},{value:"Migration from Legacy Clients",id:"migration-from-legacy-clients",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Contributing",id:"contributing-1",level:2},{value:"Community &amp; Support",id:"community--support",level:2},{value:"License",id:"license-1",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"spark-feature-push-client",children:"Spark Feature Push Client"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://badge.fury.io/py/spark_feature_push_client",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/pypi/v/spark_feature_push_client?label=pypi-package&color=light%20green",alt:"PyPI version"})}),"\n",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/actions/workflows/py-sdk.yml",children:(0,r.jsx)(n.img,{src:"https://github.com/Meesho/BharatMLStack/actions/workflows/py-sdk.yml/badge.svg",alt:"Build Status"})}),"\n",(0,r.jsx)(n.a,{href:"https://www.python.org/downloads/",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/python-3.7+-blue.svg",alt:"Python 3.7+"})}),"\n",(0,r.jsx)(n.a,{href:"https://discord.gg/XkT7XsV2AU",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/Discord-Join%20Chat-7289da?style=flat&logo=discord&logoColor=white",alt:"Discord"})}),"\n",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/LICENSE.md",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/License-BharatMLStack%20BSL%201.1-blue.svg",alt:"License"})})]}),"\n",(0,r.jsxs)(n.p,{children:["Apache Spark-based client for pushing ML features from offline batch sources to the BharatML Stack Online Feature Store via Kafka. This client is designed for ",(0,r.jsx)(n.strong,{children:"data pipeline operations"})," - reading from batch sources and publishing to Kafka for online consumption."]}),"\n",(0,r.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install spark_feature_push_client\n"})}),"\n",(0,r.jsx)(n.h2,{id:"dependencies",children:"Dependencies"}),"\n",(0,r.jsx)(n.p,{children:"This package depends on:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://pypi.org/project/bharatml_commons/",children:"bharatml_commons"})}),": Common utilities and protobuf definitions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PySpark 3.0+"}),": For distributed data processing"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"architecture-role",children:"Architecture Role"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Batch Sources \u2502\u2500\u2500\u2500\u25b6\u2502 Spark Feature Push   \u2502\u2500\u2500\u2500\u25b6\u2502   Kafka     \u2502\u2500\u2500\u2500\u25b6\u2502 Online Feature  \u2502\n\u2502 \u2022 Tables        \u2502    \u2502      Client          \u2502    \u2502             \u2502    \u2502     Store       \u2502\n\u2502 \u2022 Parquet       \u2502    \u2502 \u2022 Read & Transform   \u2502    \u2502             \u2502    \u2502                 \u2502\n\u2502 \u2022 Delta         \u2502    \u2502 \u2022 Protobuf Serialize \u2502    \u2502             \u2502    \u2502                 \u2502\n\u2502 \u2022 S3/GCS/ADLS   \u2502    \u2502 \u2022 Batch Processing   \u2502    \u2502             \u2502    \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                                \u25b2\n                                                                                \u2502\n                                                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                                      \u2502 grpc_feature_   \u2502\n                                                                      \u2502 client          \u2502\n                                                                      \u2502 (Real-time)     \u2502\n                                                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h2,{id:"features",children:"Features"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Batch Source Integration"}),": Read from Tables (Hive/Delta), Parquet, and Delta files on cloud storage"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Spark Processing"}),": Leverage Apache Spark for distributed data processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Protobuf Serialization"}),": Convert feature data to protobuf format using bharatml_commons schemas"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kafka Publishing"}),": Push serialized features to Kafka topics for online consumption"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Metadata Integration"}),": Fetch feature schemas and configurations via REST API"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Type Support"}),": Handle scalar and vector types (strings, numbers, booleans, arrays)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Batch Optimization"}),": Configurable batch sizes for optimal Kafka throughput"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"when-to-use-this-client",children:"When to Use This Client"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use spark_feature_push_client for:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd04 ",(0,r.jsx)(n.strong,{children:"Batch ETL Pipelines"}),": Scheduled feature computation and publishing"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Historical Data Backfill"}),": Loading historical features into online store"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udfd7\ufe0f ",(0,r.jsx)(n.strong,{children:"Data Engineering"}),": Spark-based feature transformations"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcc8 ",(0,r.jsx)(n.strong,{children:"Large Scale Processing"}),": Processing millions of records efficiently"]}),"\n",(0,r.jsxs)(n.li,{children:["\u26a1 ",(0,r.jsx)(n.strong,{children:"Offline-to-Online"}),": Bridge between batch and real-time systems"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use grpc_feature_client for:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\ude80 ",(0,r.jsx)(n.strong,{children:"Real-time Operations"}),": Direct persist/retrieve operations"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd0d ",(0,r.jsx)(n.strong,{children:"Interactive Queries"}),": Low-latency feature lookups"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udfaf ",(0,r.jsx)(n.strong,{children:"API Integration"}),": Service-to-service communication"]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udca8 ",(0,r.jsx)(n.strong,{children:"Single Records"}),": Persisting individual feature records"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from spark_feature_push_client import OnlineFeatureStorePyClient\n\n# Initialize client with metadata source\nclient = OnlineFeatureStorePyClient(\n    features_metadata_source_url="https://api.example.com/metadata",\n    job_id="feature-pipeline-job",\n    job_token="your-auth-token"\n)\n\n# Get feature configuration \nfeature_details = client.get_features_details()\n\n# Process your Spark DataFrame\nproto_df = client.generate_df_with_protobuf_messages(your_spark_df)\n\n# Push to Kafka\nclient.write_protobuf_df_to_kafka(\n    proto_df,\n    kafka_bootstrap_servers="localhost:9092",\n    kafka_topic="features.user_features"\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"related-packages",children:"Related Packages"}),"\n",(0,r.jsx)(n.p,{children:"This package is part of the BharatML Stack ecosystem:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://pypi.org/project/bharatml_commons/",children:"bharatml_commons"})}),": Common utilities and protobuf definitions (required dependency)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://pypi.org/project/grpc_feature_client/",children:"grpc_feature_client"})}),": High-performance gRPC client for real-time operations"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"license",children:"License"}),"\n",(0,r.jsxs)(n.p,{children:["Licensed under the BharatMLStack Business Source License 1.1. See ",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/LICENSE.md",children:"LICENSE"})," for details."]}),"\n",(0,r.jsx)(n.h2,{id:"contributing",children:"Contributing"}),"\n",(0,r.jsxs)(n.p,{children:["We welcome contributions! Please see our ",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/CONTRIBUTION.md",children:"Contributing Guide"})," for details."]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apache Spark 3.0+"}),": For distributed processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kafka Connector"}),": ",(0,r.jsx)(n.code,{children:"spark-sql-kafka"})," for Kafka integration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Java 8/11"}),": Required by Spark"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"bharatml_common"}),": For protobuf schemas"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Example Spark session setup\nspark = SparkSession.builder \\\n    .appName("FeaturePipeline") \\\n    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0") \\\n    .getOrCreate()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"supported-data-sources",children:"Supported Data Sources"}),"\n",(0,r.jsx)(n.h3,{id:"1-database-tables",children:"1. Database Tables"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Hive/Delta tables\ndf = spark.sql("SELECT * FROM feature_db.user_features")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-cloud-storage---parquet",children:"2. Cloud Storage - Parquet"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# AWS S3\ndf = spark.read.parquet("s3a://bucket/path/to/features/")\n\n# Google Cloud Storage  \ndf = spark.read.parquet("gs://bucket/path/to/features/")\n\n# Azure Data Lake\ndf = spark.read.parquet("abfss://container@account.dfs.core.windows.net/path/")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-cloud-storage---delta",children:"3. Cloud Storage - Delta"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Delta format on cloud storage\ndf = spark.read.format("delta").load("s3a://bucket/delta-table/")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"configuration-examples",children:"Configuration Examples"}),"\n",(0,r.jsx)(n.h3,{id:"basic-pipeline",children:"Basic Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from pyspark.sql import SparkSession\nfrom spark_feature_push_client import OnlineFeatureStorePyClient\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName("FeatureETL") \\\n    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0") \\\n    .getOrCreate()\n\n# Initialize client\nclient = OnlineFeatureStorePyClient(\n    features_metadata_source_url="https://metadata-service.example.com/api/v1/features",\n    job_id="daily-feature-pipeline",\n    job_token="pipeline-secret-token",\n    fgs_to_consider=["user_demographics", "user_behavior"]  # Optional: filter feature groups\n)\n\n# Get metadata and column mappings\n(\n    offline_src_type_columns,\n    offline_col_to_default_values_map, \n    entity_column_names\n) = client.get_features_details()\n\nprint(f"Entity columns: {entity_column_names}")\nprint(f"Feature mappings: {offline_src_type_columns}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"reading-from-multiple-sources",children:"Reading from Multiple Sources"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def get_features_from_all_sources(spark, entity_columns, feature_mapping, default_values):\n    """\n    Read and combine features from multiple offline sources\n    """\n    dataframes = []\n    \n    for source_info in feature_mapping:\n        table_name, source_type, feature_list = source_info\n        \n        if source_type == "TABLE":\n            # Read from Hive/Delta table\n            df = spark.table(table_name)\n            \n        elif source_type.startswith("PARQUET_"):\n            # Read from Parquet files\n            df = spark.read.parquet(table_name)\n            \n        elif source_type.startswith("DELTA_"):\n            # Read from Delta files\n            df = spark.read.format("delta").load(table_name)\n        \n        # Select and rename columns\n        select_cols = entity_columns.copy()\n        for original_col, renamed_col in feature_list:\n            if original_col in df.columns:\n                df = df.withColumnRenamed(original_col, renamed_col)\n                select_cols.append(renamed_col)\n        \n        df = df.select(select_cols)\n        dataframes.append(df)\n    \n    # Union all dataframes\n    if dataframes:\n        combined_df = dataframes[0]\n        for df in dataframes[1:]:\n            combined_df = combined_df.unionByName(df, allowMissingColumns=True)\n        \n        # Fill missing values with defaults\n        for col, default_val in default_values.items():\n            if col in combined_df.columns:\n                combined_df = combined_df.fillna({col: default_val})\n        \n        return combined_df\n    \n    return None\n\n# Use the function\ndf = get_features_from_all_sources(\n    spark, \n    entity_column_names, \n    offline_src_type_columns, \n    offline_col_to_default_values_map\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"protobuf-serialization--kafka-publishing",children:"Protobuf Serialization & Kafka Publishing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Convert DataFrame to protobuf messages\n# This creates binary protobuf messages suitable for Kafka\nproto_df = client.generate_df_with_protobuf_messages(\n    df, \n    intra_batch_size=20  # Batch size for serialization\n)\n\n# The proto_df has schema: [value: binary, intra_batch_id: long]\nproto_df.printSchema()\n# root\n#  |-- value: binary (nullable = false)  \n#  |-- intra_batch_id: long (nullable = false)\n\n# Write to Kafka with batching for better throughput\nclient.write_protobuf_df_to_kafka(\n    proto_df,\n    kafka_bootstrap_servers="broker1:9092,broker2:9092,broker3:9092",\n    kafka_topic="features.user_features",\n    additional_options={\n        "kafka.acks": "all",\n        "kafka.retries": "3",\n        "kafka.compression.type": "snappy"\n    },\n    kafka_num_batches=4  # Split into 4 parallel Kafka writes\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"data-type-handling",children:"Data Type Handling"}),"\n",(0,r.jsx)(n.p,{children:"The client automatically handles the protobuf data type mappings:"}),"\n",(0,r.jsx)(n.h3,{id:"scalar-types",children:"Scalar Types"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Example DataFrame with different types\ndata = [\n    ("user123", 25, 185.5, True, "premium"),      # int, float, bool, string\n    ("user456", 30, 170.0, False, "basic")\n]\ndf = spark.createDataFrame(data, ["user_id", "age", "height", "is_premium", "tier"])\n\n# Automatically mapped to protobuf:\n# age -> int32_values\n# height -> fp32_values  \n# is_premium -> bool_values\n# tier -> string_values\n'})}),"\n",(0,r.jsx)(n.h3,{id:"vector-types",children:"Vector Types"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Example with vector/array features\nfrom pyspark.sql.functions import array, lit\n\ndf = spark.createDataFrame([\n    ("user123", [0.1, 0.2, 0.3], ["tech", "sports"], [1, 2, 3])\n], ["user_id", "embeddings", "interests", "scores"])\n\n# Automatically mapped to protobuf vectors:\n# embeddings -> fp32_values in Vector\n# interests -> string_values in Vector\n# scores -> int32_values in Vector\n'})}),"\n",(0,r.jsx)(n.h2,{id:"production-pipeline-example",children:"Production Pipeline Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def run_feature_pipeline():\n    """\n    Complete feature pipeline from batch sources to Kafka\n    """\n    \n    # 1. Initialize Spark\n    spark = SparkSession.builder \\\n        .appName("DailyFeaturePipeline") \\\n        .config("spark.sql.adaptive.enabled", "true") \\\n        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0") \\\n        .getOrCreate()\n    \n    try:\n        # 2. Initialize feature client\n        client = OnlineFeatureStorePyClient(\n            features_metadata_source_url=os.getenv("METADATA_URL"),\n            job_id=os.getenv("JOB_ID"),\n            job_token=os.getenv("JOB_TOKEN")\n        )\n        \n        # 3. Get feature configuration\n        feature_mapping, default_values, entity_columns = client.get_features_details()\n        \n        # 4. Read and process data\n        df = get_features_from_all_sources(spark, entity_columns, feature_mapping, default_values)\n        \n        if df is None or df.count() == 0:\n            raise ValueError("No data found in sources")\n        \n        # 5. Convert to protobuf\n        proto_df = client.generate_df_with_protobuf_messages(df, intra_batch_size=50)\n        \n        # 6. Publish to Kafka\n        client.write_protobuf_df_to_kafka(\n            proto_df,\n            kafka_bootstrap_servers=os.getenv("KAFKA_BROKERS"),\n            kafka_topic=os.getenv("KAFKA_TOPIC"),\n            additional_options={\n                "kafka.acks": "all",\n                "kafka.compression.type": "snappy",\n                "kafka.max.request.size": "10485760"  # 10MB\n            },\n            kafka_num_batches=int(os.getenv("KAFKA_BATCHES", "4"))\n        )\n        \n        print(f"\u2705 Successfully processed {df.count()} records")\n        \n    finally:\n        spark.stop()\n\nif __name__ == "__main__":\n    run_feature_pipeline()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"configuration-options",children:"Configuration Options"}),"\n",(0,r.jsx)(n.h3,{id:"client-configuration",children:"Client Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'client = OnlineFeatureStorePyClient(\n    features_metadata_source_url="https://api.example.com/metadata",  # Required\n    job_id="pipeline-job-001",                                       # Required  \n    job_token="secret-token-123",                                    # Required\n    fgs_to_consider=["user_features", "item_features"]               # Optional: filter feature groups\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"protobuf-serialization-options",children:"Protobuf Serialization Options"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"proto_df = client.generate_df_with_protobuf_messages(\n    df,\n    intra_batch_size=20  # Records per protobuf message (default: 20)\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"kafka-publishing-options",children:"Kafka Publishing Options"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'client.write_protobuf_df_to_kafka(\n    proto_df,\n    kafka_bootstrap_servers="localhost:9092",\n    kafka_topic="features.topic",\n    additional_options={\n        "kafka.acks": "all",                    # Acknowledgment level\n        "kafka.retries": "3",                   # Retry attempts\n        "kafka.compression.type": "snappy",     # Compression\n        "kafka.batch.size": "16384",            # Batch size\n        "kafka.linger.ms": "100",               # Batching delay\n        "kafka.max.request.size": "10485760"    # Max message size\n    },\n    kafka_num_batches=1  # Number of parallel Kafka writers (default: 1)\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,r.jsx)(n.h3,{id:"spark-optimizations",children:"Spark Optimizations"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'spark = SparkSession.builder \\\n    .appName("FeaturePipeline") \\\n    .config("spark.sql.adaptive.enabled", "true") \\\n    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \\\n    .config("spark.sql.adaptive.skewJoin.enabled", "true") \\\n    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \\\n    .getOrCreate()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# For large datasets, consider:\ndf = df.repartition(200)  # Optimal partition count\ndf.cache()  # Cache if reused multiple times\n"})}),"\n",(0,r.jsx)(n.h3,{id:"kafka-throughput",children:"Kafka Throughput"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# For high-throughput scenarios:\nclient.write_protobuf_df_to_kafka(\n    proto_df,\n    kafka_bootstrap_servers="brokers",\n    kafka_topic="topic", \n    kafka_num_batches=8,  # Increase parallel writers\n    additional_options={\n        "kafka.batch.size": "65536",      # Larger batches\n        "kafka.linger.ms": "100",         # Allow batching delay\n        "kafka.compression.type": "lz4"   # Fast compression\n    }\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"monitoring--debugging",children:"Monitoring & Debugging"}),"\n",(0,r.jsx)(n.h3,{id:"dataframe-inspection",children:"DataFrame Inspection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Check data before processing\nprint(f"Records: {df.count()}")\nprint(f"Columns: {df.columns}")\ndf.printSchema()\ndf.show(5)\n\n# Check protobuf output\nproto_df.show(5, truncate=False)\nprint(f"Protobuf messages: {proto_df.count()}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'try:\n    proto_df = client.generate_df_with_protobuf_messages(df)\n    client.write_protobuf_df_to_kafka(proto_df, brokers, topic)\n    \nexcept Exception as e:\n    print(f"Pipeline failed: {e}")\n    # Log to monitoring system\n    # Send alerts\n    raise\n'})}),"\n",(0,r.jsx)(n.h2,{id:"integration-with-other-sdks",children:"Integration with Other SDKs"}),"\n",(0,r.jsx)(n.h3,{id:"with-grpc-feature-client",children:"With gRPC Feature Client"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Spark client pushes features to Kafka\nspark_client = OnlineFeatureStorePyClient(...)\nspark_client.write_protobuf_df_to_kafka(proto_df, brokers, topic)\n\n# gRPC client retrieves features in real-time\nfrom grpc_feature_client import GRPCFeatureClient\ngrpc_client = GRPCFeatureClient(config)\nfeatures = grpc_client.retrieve_decoded_features(...)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"with-http-feature-client-bharatml_common",children:"With HTTP Feature Client (bharatml_common)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Use HTTP client for metadata validation\nfrom bharatml_common import HTTPFeatureClient\nhttp_client = HTTPFeatureClient(base_url, job_id, token)\nmetadata = http_client.get_feature_metadata()\n\n# Validate feature names using shared utilities\nfrom bharatml_common import clean_column_name\nclean_features = [clean_column_name(name) for name in feature_names]\n\n# Process with Spark client\nspark_client.generate_df_with_protobuf_messages(df)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"common-use-cases",children:"Common Use Cases"}),"\n",(0,r.jsx)(n.h3,{id:"1-daily-batch-etl",children:"1. Daily Batch ETL"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Cron job: 0 2 * * * (daily at 2 AM)\nspark-submit \\\n  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0 \\\n  --conf spark.sql.adaptive.enabled=true \\\n  daily_feature_pipeline.py\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-historical-backfill",children:"2. Historical Backfill"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Backfill last 30 days\nfrom datetime import datetime, timedelta\n\nfor i in range(30):\n    date = datetime.now() - timedelta(days=i)\n    df = spark.sql(f"""\n        SELECT * FROM features \n        WHERE date = \'{date.strftime(\'%Y-%m-%d\')}\'\n    """)\n    \n    proto_df = client.generate_df_with_protobuf_messages(df)\n    client.write_protobuf_df_to_kafka(proto_df, brokers, f"backfill.{date.strftime(\'%Y%m%d\')}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-real-time-streaming-advanced",children:"3. Real-time Streaming (Advanced)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Read from streaming source, process, and publish\nstreaming_df = spark.readStream \\\n    .format("kafka") \\\n    .option("kafka.bootstrap.servers", input_brokers) \\\n    .option("subscribe", input_topic) \\\n    .load()\n\n# Process streaming DataFrame\nprocessed_df = streaming_df.select(...)\n\n# Write to output Kafka (requires structured streaming)\nquery = processed_df.writeStream \\\n    .format("kafka") \\\n    .option("kafka.bootstrap.servers", output_brokers) \\\n    .option("topic", output_topic) \\\n    .start()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"OutOfMemoryError"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Increase driver memory or reduce partition size\nspark.conf.set("spark.sql.adaptive.coalescePartitions.minPartitionNum", "50")\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Kafka Connection Timeout"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Check network connectivity and broker addresses\nadditional_options = {\n    "kafka.request.timeout.ms": "60000",\n    "kafka.session.timeout.ms": "30000"\n}\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Protobuf Serialization Errors"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Check data types and null values\ndf = df.fillna({"string_col": "", "numeric_col": 0})\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Metadata API Errors"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Verify job_id, job_token, and URL\n# Check API server logs\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"debug-mode",children:"Debug Mode"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Enable Spark SQL logging\nspark.sparkContext.setLogLevel("INFO")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"migration-from-legacy-clients",children:"Migration from Legacy Clients"}),"\n",(0,r.jsx)(n.p,{children:"If migrating from older versions:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Old import\n# from online_feature_store_py_client import OnlineFeatureStorePyClient\n\n# New import (same interface)\nfrom spark_feature_push_client import OnlineFeatureStorePyClient\n\n# API remains the same - no code changes needed!\n"})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resource Management"}),": Always stop Spark sessions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Error Handling"}),": Implement proper exception handling and retries"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Monitoring"}),": Add metrics and logging to your pipelines"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Testing"}),": Test with sample data before production runs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Security"}),": Use secure Kafka configurations in production"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": Monitor Spark UI for optimization opportunities"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The Spark Feature Push Client is your gateway from batch data sources to the real-time online feature store! \ud83d\ude80"}),"\n",(0,r.jsx)(n.h2,{id:"contributing-1",children:"Contributing"}),"\n",(0,r.jsxs)(n.p,{children:["We welcome contributions from the community! Please see our ",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/CONTRIBUTING.md",children:"Contributing Guide"})," for details on how to get started."]}),"\n",(0,r.jsx)(n.h2,{id:"community--support",children:"Community & Support"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcac ",(0,r.jsx)(n.strong,{children:"Discord"}),": Join our ",(0,r.jsx)(n.a,{href:"https://discord.gg/XkT7XsV2AU",children:"community chat"})]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udc1b ",(0,r.jsx)(n.strong,{children:"Issues"}),": Report bugs and request features on ",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/issues",children:"GitHub Issues"})]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udce7 ",(0,r.jsx)(n.strong,{children:"Email"}),": Contact us at ",(0,r.jsx)(n.a,{href:"mailto:ml-oss@meesho.com",children:"ml-oss@meesho.com"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"license-1",children:"License"}),"\n",(0,r.jsxs)(n.p,{children:["BharatMLStack is open-source software licensed under the ",(0,r.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/LICENSE.md",children:"BharatMLStack Business Source License 1.1"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)("div",{align:"center",children:(0,r.jsx)("strong",{children:"Built with \u2764\ufe0f for the ML community from Meesho"})}),"\n",(0,r.jsx)("div",{align:"center",children:(0,r.jsx)("strong",{children:"If you find this useful, \u2b50\ufe0f the repo \u2014 your support means the world to us!"})})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>o});var t=a(6540);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);