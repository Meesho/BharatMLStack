"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9095],{762:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/bms-7399e8796d2cd24617c432518ce3f312.png"},1737:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>d});var i=n(3306),s=n(4848),r=n(8453);const a={title:"Beyond Vector RAG: Building Agent Memory That Learns From Experience.",description:"Current agent memory is just search. We built an episodic memory system that tracks outcomes, forms causal links, extracts reasoning heuristics, and actually learns from failure \u2014 without retraining the model.",slug:"episodic-memory-for-agents",authors:["adarsha"],date:new Date("2026-02-19T00:00:00.000Z"),tags:["ai-agents","memory","architecture","llm","episodic-memory"]},o=void 0,c={authorsImageUrls:[void 0]},d=[{value:"The Gap Nobody Talks About",id:"the-gap-nobody-talks-about",level:2},{value:"What&#39;s Wrong With Vector RAG as Memory",id:"whats-wrong-with-vector-rag-as-memory",level:2},{value:"The Architecture: Episodic Memory",id:"the-architecture-episodic-memory",level:2},{value:"Layer 1: Immutable Timeline",id:"layer-1-immutable-timeline",level:3},{value:"Layer 2: Episode Segmentation",id:"layer-2-episode-segmentation",level:3},{value:"Layer 3: Episodic Graph",id:"layer-3-episodic-graph",level:3},{value:"Layer 4: Generalized Facts",id:"layer-4-generalized-facts",level:3},{value:"The Reinforcement Loop",id:"the-reinforcement-loop",level:3},{value:"The Experiment",id:"the-experiment",level:2},{value:"Results",id:"results",level:2},{value:"Decision Accuracy",id:"decision-accuracy",level:3},{value:"Where the Gap Opened",id:"where-the-gap-opened",level:3},{value:"Retrieval Quality",id:"retrieval-quality",level:3},{value:"What Didn&#39;t Work",id:"what-didnt-work",level:2},{value:"What This Means",id:"what-this-means",level:2},{value:"How It Compares to Existing Solutions",id:"how-it-compares-to-existing-solutions",level:2},{value:"Try It Yourself",id:"try-it-yourself",level:2},{value:"Conclusion",id:"conclusion",level:2}];function l(e){const t={blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.img,{alt:"BharatMLStack",src:n(762).A+"",width:"1396",height:"460"}),"\nAgent memory has come a long way. Persistent context, vector retrieval, knowledge graphs \u2014 the building blocks are real and getting better fast."]}),"\n",(0,s.jsx)(t.p,{children:'But most of what we call "memory" today is still closer to search: chunk text, embed it, retrieve whatever looks similar at query time. That works well for recalling facts and preferences. It starts to break down when you need an agent to recall what happened last time, learn from a mistake, or avoid repeating a failed approach.'}),"\n",(0,s.jsx)(t.p,{children:"We are trying to experiment something different. An episodic memory system where a frozen LLM \u2014 same weights, no retraining \u2014 produces increasingly better decisions over time because the memory feeding it context is continuously evolving.\nThen we tested it. The results were interesting."}),"\n",(0,s.jsx)(t.h2,{id:"the-gap-nobody-talks-about",children:"The Gap Nobody Talks About"}),"\n",(0,s.jsx)(t.p,{children:"Here's a scenario every engineering team has encountered: AI agent hits a Redis connection pool exhaustion issue. It misdiagnoses it as a database problem. You correct it. Next week, a different service has the exact same failure pattern. The agent makes the exact same mistake."}),"\n",(0,s.jsx)(t.p,{children:"Why? Because LLMs don't learn at inference time. Corrections adjust behavior within a conversation. Once the session ends, the lesson is gone. The model weights haven't changed. The next conversation starts from zero."}),"\n",(0,s.jsx)(t.p,{children:'Current "memory" systems don\'t fully address this. They store facts \u2014 user preferences, document chunks, conversation summaries. But facts aren\'t experience. Knowing that "Redis connection pools can exhaust under load" is different from remembering "last time I saw 500 errors under load, I assumed it was the database, I was wrong, it was actually the connection pool, and here\'s the correction I received."'}),"\n",(0,s.jsx)(t.p,{children:"The first is a fact. The second is an episode. The difference matters."}),"\n",(0,s.jsx)(t.h2,{id:"whats-wrong-with-vector-rag-as-memory",children:"What's Wrong With Vector RAG as Memory"}),"\n",(0,s.jsx)(t.p,{children:"We identified five structural gaps in how current agent frameworks handle memory:"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"No concept of time."})," Two events are either semantically similar or they're not. The system can't represent \"this happened after that\" without distorting similarity scores. An agent can't reason about sequence or causality."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"No concept of situation."})," A production incident and a design review might use the same technical vocabulary. Flat vector search can't distinguish them. Your agent retrieves planning notes when it should be retrieving incident postmortems."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"No outcome tracking."})," The system stores ",(0,s.jsx)(t.em,{children:"what happened"})," but not ",(0,s.jsx)(t.em,{children:"whether it worked"}),". A failed approach and a successful one are equally retrievable. The agent has no way to prefer strategies that worked over strategies that didn't."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Summaries destroy evidence."})," Summarization-based memory compresses experience but discards the reasoning chain. The agent loses the ability to explain ",(0,s.jsx)(t.em,{children:"how"})," it arrived at a conclusion. The audit trail is gone."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"No causal links."})," Each memory chunk is independent. There's no way to express that incident A caused decision B, which led to outcome C, which was corrected by approach D. Without this structure, the agent can't traverse chains of reasoning."]}),"\n",(0,s.jsx)(t.p,{children:"These gaps compound. As an agent accumulates more experience, flat vector memory gets noisier, more contradictory, and less useful. The system degrades precisely when it should be improving."}),"\n",(0,s.jsx)(t.h2,{id:"the-architecture-episodic-memory",children:"The Architecture: Episodic Memory"}),"\n",(0,s.jsx)(t.p,{children:"We are building a memory system modeled on how human episodic memory works \u2014 not as a metaphor, but as an engineering specification."}),"\n",(0,s.jsx)(t.p,{children:"The system has four layers:"}),"\n",(0,s.jsx)(t.h3,{id:"layer-1-immutable-timeline",children:"Layer 1: Immutable Timeline"}),"\n",(0,s.jsx)(t.p,{children:"Every piece of agent experience is recorded as an append-only timeline entry. Each entry carries a semantic embedding (what it means), a timestamp (when it happened), and a state label (what situation the agent was in \u2014 debugging, planning, code review, incident response). Entries are never modified, never deleted, never summarized. This is the source of truth."}),"\n",(0,s.jsx)(t.h3,{id:"layer-2-episode-segmentation",children:"Layer 2: Episode Segmentation"}),"\n",(0,s.jsx)(t.p,{children:"The system watches the timeline and detects when one coherent unit of experience ends and another begins \u2014 via state transitions, semantic shifts, temporal gaps, or explicit signals. Each episode is a reference into the timeline (not a copy) with a generated summary, an outcome (SUCCESS, FAILURE, PARTIAL, UNKNOWN), decisions made, assumptions held, and corrections received."}),"\n",(0,s.jsx)(t.p,{children:"The outcome field is the most important thing that doesn't exist in any current memory system. Without it, you can't learn from mistakes."}),"\n",(0,s.jsx)(t.h3,{id:"layer-3-episodic-graph",children:"Layer 3: Episodic Graph"}),"\n",(0,s.jsx)(t.p,{children:'Episodes are connected through typed, weighted links: CAUSED_BY, LED_TO, RETRY_OF, LEARNED_FROM, CONTINUATION, CONTRADICTED. Over time, this forms a directed graph that enables traversal by meaning and causality. You can follow the chain: "this incident caused that investigation, which led to a failed fix, which was corrected by this approach."'}),"\n",(0,s.jsx)(t.h3,{id:"layer-4-generalized-facts",children:"Layer 4: Generalized Facts"}),"\n",(0,s.jsx)(t.p,{children:'When multiple episodes exhibit consistent patterns, the system extracts reasoning heuristics: "When services fail immediately after deployment with no traffic change, investigate configuration errors before connection pool problems." Facts are versioned, never overwritten, and maintain links back to supporting and contradicting episodes. When contradicting evidence accumulates, confidence decreases. When confidence drops below a threshold, the fact is revised \u2014 but the old version is preserved.'}),"\n",(0,s.jsx)(t.p,{children:"The LLM sits above all four layers. At query time, the system assembles structured context \u2014 relevant episodes with outcomes, applicable facts with confidence scores, causal narratives \u2014 and passes it to the LLM for reasoning. The model reasons over structured memory. It doesn't store or manage memory."}),"\n",(0,s.jsx)(t.h3,{id:"the-reinforcement-loop",children:"The Reinforcement Loop"}),"\n",(0,s.jsx)(t.p,{children:"This is where it comes together:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"Agent reasons using retrieved episodes and facts"}),"\n",(0,s.jsx)(t.li,{children:"Outcome is detected (CI pass/fail, user correction, test result)"}),"\n",(0,s.jsx)(t.li,{children:"New episode is created with outcome tracking"}),"\n",(0,s.jsx)(t.li,{children:"Links are created between the retrieved episodes and the new episode"}),"\n",(0,s.jsx)(t.li,{children:"Facts are reinforced (if outcome aligned) or contradicted (if outcome conflicted)"}),"\n",(0,s.jsx)(t.li,{children:"If the decision was wrong and corrected, a LEARNED_FROM link is created"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"The model weights never change. The memory structure evolves continuously. A frozen LLM produces better decisions over time because it receives better context from richer memory."}),"\n",(0,s.jsx)(t.h2,{id:"the-experiment",children:"The Experiment"}),"\n",(0,s.jsx)(t.p,{children:"We built the full system in Python (~1,000 lines) and tested it head-to-head against a baseline flat-vector RAG agent across a 9-round synthetic debugging scenario. Both agents used the identical LLM (Claude Sonnet 4) for reasoning. The only variable was the memory system."}),"\n",(0,s.jsx)(t.p,{children:"The scenario was designed to test five capabilities:"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Round Type"}),(0,s.jsx)(t.th,{children:"What It Tests"}),(0,s.jsx)(t.th,{children:"Rounds"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"LEARN"}),(0,s.jsx)(t.td,{children:"Can the agent build experience from failures?"}),(0,s.jsx)(t.td,{children:"1, 2, 4"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"RED HERRING"}),(0,s.jsx)(t.td,{children:"Can the agent resist applying a pattern when it doesn't fit?"}),(0,s.jsx)(t.td,{children:"3"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"TEST"}),(0,s.jsx)(t.td,{children:"Can the agent apply learned patterns to new services?"}),(0,s.jsx)(t.td,{children:"5, 6"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"SUBTLE"}),(0,s.jsx)(t.td,{children:"Can the agent generalize to different symptoms, same root cause?"}),(0,s.jsx)(t.td,{children:"7"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"CORRECTION"}),(0,s.jsx)(t.td,{children:"After being corrected, does the agent adapt?"}),(0,s.jsx)(t.td,{children:"8, 9"})]})]})]}),"\n",(0,s.jsxs)(t.p,{children:["Rounds 1-4 build experience: three connection pool failures across different services, plus one red herring (a deployment config error that ",(0,s.jsx)(t.em,{children:"looks"})," like a connection pool issue). Rounds 5-7 test whether the agent applies the learned pattern to unfamiliar services and subtle symptom variations. Rounds 8-9 are the critical test: the agent is corrected after misdiagnosing a deployment-correlated error, then tested on a near-identical scenario to see if it adapts."]}),"\n",(0,s.jsx)(t.h2,{id:"results",children:"Results"}),"\n",(0,s.jsx)(t.h3,{id:"decision-accuracy",children:"Decision Accuracy"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Round"}),(0,s.jsx)(t.th,{children:"Type"}),(0,s.jsx)(t.th,{children:"Episodic Agent"}),(0,s.jsx)(t.th,{children:"Baseline Agent"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"1"}),(0,s.jsx)(t.td,{children:"LEARN"}),(0,s.jsx)(t.td,{children:"\u2717"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"2"}),(0,s.jsx)(t.td,{children:"LEARN"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"3"}),(0,s.jsx)(t.td,{children:"RED HERRING"}),(0,s.jsx)(t.td,{children:"\u2717"}),(0,s.jsx)(t.td,{children:"\u2717"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"4"}),(0,s.jsx)(t.td,{children:"LEARN"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"5"}),(0,s.jsx)(t.td,{children:"TEST"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"\u2713"})}),(0,s.jsx)(t.td,{children:"\u2717"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"6"}),(0,s.jsx)(t.td,{children:"TEST"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"\u2713"})}),(0,s.jsx)(t.td,{children:"\u2717"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"7"}),(0,s.jsx)(t.td,{children:"SUBTLE"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"\u2713"})}),(0,s.jsx)(t.td,{children:"\u2717"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"8"}),(0,s.jsx)(t.td,{children:"CORRECTION"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"9"}),(0,s.jsx)(t.td,{children:"CORRECTION"}),(0,s.jsx)(t.td,{children:"\u2713"}),(0,s.jsx)(t.td,{children:"\u2713"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"Total"})}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"7/9 (78%)"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"5/9 (56%)"})})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"The episodic agent won 7-5. A 40% relative improvement in decision accuracy using the exact same LLM."}),"\n",(0,s.jsx)(t.h3,{id:"where-the-gap-opened",children:"Where the Gap Opened"}),"\n",(0,s.jsx)(t.p,{children:"The episodic agent's advantage concentrated in exactly the rounds designed to test memory quality:"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Rounds 5-6 (pattern application):"})," The episodic agent cited 4 past failure episodes with connection pool exhaustion as root cause, complete with correction annotations. It correctly identified pool exhaustion in new services. The baseline retrieved disconnected chunks and suggested checking timeout configurations \u2014 a pattern it picked up from the Round 3 red herring."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Round 7 (subtle symptoms \u2014 latency increase, no errors):"}),' Both agents had the same evidence available. The episodic agent\'s retrieval surfaced a diverse set of episodes (thanks to MMR diversity filtering) including the Redis pool exhaustion from Round 6, which primed it to recognize that latency without errors can still be pool contention. The baseline defaulted to "check recent config changes."']}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Round 9 (adaptation after correction):"})," This is the result we're most proud of. Look at the episodic agent's reasoning:"]}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.em,{children:'"Episode 1 directly parallels this situation \u2014 errors spiking immediately after a deployment (v2.4.1 then, v3.1.0 now) with no traffic change. In that case, the root cause was a database migration that dropped an index. The generalized fact confirms that deployment-related issues with immediate onset after version changes are more likely caused by configuration errors or missing dependencies than by connection pool problems."'})}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["It cited a specific past episode by analogy, quoted a generalized fact, and explained ",(0,s.jsx)(t.em,{children:"why"})," this situation matches the deployment pattern rather than the connection pool pattern. The baseline gave a vaguer assessment."]}),"\n",(0,s.jsx)(t.h3,{id:"retrieval-quality",children:"Retrieval Quality"}),"\n",(0,s.jsx)(t.p,{children:"This is where the structural difference is most visible:"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Metric"}),(0,s.jsx)(t.th,{children:"Episodic Agent"}),(0,s.jsx)(t.th,{children:"Baseline Agent"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Retrieved items with explicit outcome labels"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"100%"})}),(0,s.jsx)(t.td,{children:"25%"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Correct pattern applications (Rounds 4-7)"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"4/4"})}),(0,s.jsx)(t.td,{children:"1/4"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"False positives (Rounds 8-9)"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"0"})}),(0,s.jsx)(t.td,{children:"0"})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"Every item the episodic agent retrieved carried a structured outcome label (SUCCESS or FAILURE) with correction details. Only 25% of the baseline's chunks contained any outcome information \u2014 and those were incidental text mentions, not structured labels."}),"\n",(0,s.jsx)(t.p,{children:"The episodic agent correctly applied the connection pool pattern in all four rounds where it was the root cause, and correctly avoided it in both rounds where it wasn't. The baseline applied it correctly once."}),"\n",(0,s.jsx)(t.h2,{id:"what-didnt-work",children:"What Didn't Work"}),"\n",(0,s.jsx)(t.p,{children:"Two things didn't work as anticipated:"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Round 3 (red herring):"})," Both agents failed. The symptoms looked like connection pool issues, but the root cause was a deployment config change. At this point, the episodic agent had only seen connection pool episodes \u2014 it had no counter-evidence for deployment-correlated errors. You can't distinguish patterns you've only seen one side of. After Round 8 introduced a correction, the agent successfully avoided this mistake in Round 9."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Fact quality variance."}),' Some extracted facts were specific and actionable ("Deployment-related issues with immediate onset are more likely configuration errors"). Others were vague ("Initial symptom-based diagnosis often leads to misidentifying the root cause"). A production system needs a usefulness filter, not just a confidence score.']}),"\n",(0,s.jsx)(t.h2,{id:"what-this-means",children:"What This Means"}),"\n",(0,s.jsx)(t.p,{children:"The most important finding isn't the accuracy improvement. It's that the reinforcement loop closes without retraining."}),"\n",(0,s.jsx)(t.p,{children:"In the POC, we observed:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Rounds 1-4: Agent encounters failures, episodes recorded with outcomes and corrections"}),"\n",(0,s.jsx)(t.li,{children:'After Round 4: Fact extracted \u2014 "Connection pool exhaustion is a common root cause under load"'}),"\n",(0,s.jsx)(t.li,{children:"Rounds 5-7: Agent applies the pattern with increasing confidence (fact support count grows)"}),"\n",(0,s.jsx)(t.li,{children:"Round 8: Agent encounters a deployment error, correctly identifies it as config, gets corrected"}),"\n",(0,s.jsx)(t.li,{children:'After Round 8: New fact \u2014 "Deployment-related issues with immediate onset are more likely configuration errors"'}),"\n",(0,s.jsx)(t.li,{children:"Round 9: Agent receives near-identical scenario, correctly avoids connection pool pattern, cites the Round 8 correction"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"The model didn't change. The memory evolved. That's the whole point."}),"\n",(0,s.jsx)(t.h2,{id:"how-it-compares-to-existing-solutions",children:"How It Compares to Existing Solutions"}),"\n",(0,s.jsx)(t.p,{children:"Agent memory is a fast-moving space with several strong systems, each solving a different slice of the problem:"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Mem0"})," excels at persistent personalization \u2014 extracting user preferences, managing session context, and reducing token costs through intelligent compression. It's the most production-ready memory layer available and integrates with nearly every agent framework. Its focus is on remembering about users and conversations rather than learning from task-level outcomes, which is a different problem than the one we're exploring here."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Zep/Graphiti"})," is doing some of the most interesting work in temporal knowledge graphs. Their bi-temporal model \u2014 tracking both when an event occurred and when it was ingested \u2014 addresses a real structural gap in how agent memory handles changing facts over time. Their episode and entity subgraphs share some philosophical DNA with our approach. Where our work diverges is in outcome tracking and reinforcement: we're specifically focused on whether a decision worked, and using that signal to update memory structure."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Letta (formerly MemGPT)"}),' pioneered self-editing memory \u2014 giving the LLM tools to manage its own memory blocks. This is a powerful paradigm, and their recent work on "Context Repositories" and sleep-time compute suggests they\'re actively pushing toward agents that learn over time. Their team has been transparent that experiential learning is an unsolved problem, which is part of what motivated our exploration.']}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"MemRL (Jan 2026 paper)"})," is the closest to our work academically. It shares the core insight of decoupling stable LLM reasoning from plastic, evolving memory. Their approach uses reinforcement learning to assign utility Q-values to memories, which is elegant but requires training a value function. Our approach is purely structural \u2014 no training step, no Q-values, just graph evolution and LLM-based reasoning over outcomes."]}),"\n",(0,s.jsx)(t.p,{children:"The common thread: most existing systems focus on knowledge persistence \u2014 remembering facts, preferences, and conversation history across sessions. The problem we're exploring is experiential learning \u2014 tracking whether past decisions worked, forming causal chains between episodes, and extracting reasoning heuristics that improve over time. These are complementary capabilities that would be needed by an ideal production system."}),"\n",(0,s.jsx)(t.h2,{id:"try-it-yourself",children:"Try It Yourself"}),"\n",(0,s.jsx)(t.p,{children:"The prototype is available in our experiments directory:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"experiments/episodic-memory-prototype/\n\u251c\u2500\u2500 memory/          # Timeline, encoder, episodes, graph, facts, retriever, reinforcer\n\u251c\u2500\u2500 agent/           # Episodic memory agent\n\u251c\u2500\u2500 baseline/        # Flat vector RAG agent (comparison)\n\u251c\u2500\u2500 simulator/       # 9-round debugging scenario\n\u251c\u2500\u2500 eval/            # Head-to-head comparison + scoring\n\u2514\u2500\u2500 tests/\n"})}),"\n",(0,s.jsx)(t.p,{children:"To run the comparison:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"cd experiments/episodic-memory-prototype\npython -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\nexport ANTHROPIC_API_KEY=sk-ant-...\npython -m eval.compare\n"})}),"\n",(0,s.jsx)(t.p,{children:"Without an API key, it runs in heuristic mode (keyword-based decisions). With a key, both agents use Claude Sonnet for reasoning \u2014 that's where the quality gap becomes visible."}),"\n",(0,s.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(t.p,{children:"This is a 9-round synthetic scenario we designed. It demonstrates the poc architecture works end-to-end and shows where episodic memory provides qualitatively different reasoning. It is not a peer-reviewed benchmark and should not be interpreted as a statistically rigorous claim. We're publishing the prototype so others can reproduce and extend the evaluation.\nIf this sparks interest do trigger github discussion."}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsxs)(t.em,{children:["The episodic memory prototype is available in ",(0,s.jsx)(t.code,{children:"BharatMLStack"})," repo at ",(0,s.jsx)(t.code,{children:"/experiments/episodic-memory-prototype"})]})})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},3306:e=>{e.exports=JSON.parse('{"permalink":"/BharatMLStack/blog/episodic-memory-for-agents","editUrl":"https://github.com/Meesho/BharatMLStack/tree/main/docs/blog/bharatmlstack-history/episodic-memory-for-agents/index.md","source":"@site/blog/bharatmlstack-history/episodic-memory-for-agents/index.md","title":"Beyond Vector RAG: Building Agent Memory That Learns From Experience.","description":"Current agent memory is just search. We built an episodic memory system that tracks outcomes, forms causal links, extracts reasoning heuristics, and actually learns from failure \u2014 without retraining the model.","date":"2026-02-19T00:00:00.000Z","tags":[{"inline":true,"label":"ai-agents","permalink":"/BharatMLStack/blog/tags/ai-agents"},{"inline":true,"label":"memory","permalink":"/BharatMLStack/blog/tags/memory"},{"inline":true,"label":"architecture","permalink":"/BharatMLStack/blog/tags/architecture"},{"inline":true,"label":"llm","permalink":"/BharatMLStack/blog/tags/llm"},{"inline":true,"label":"episodic-memory","permalink":"/BharatMLStack/blog/tags/episodic-memory"}],"readingTime":11.67,"hasTruncateMarker":true,"authors":[{"name":"Adarsha Das","title":"Senior Architect @ Meesho","url":"https://github.com/a0d00kc","imageURL":"https://github.com/a0d00kc.png","key":"adarsha","page":null}],"frontMatter":{"title":"Beyond Vector RAG: Building Agent Memory That Learns From Experience.","description":"Current agent memory is just search. We built an episodic memory system that tracks outcomes, forms causal links, extracts reasoning heuristics, and actually learns from failure \u2014 without retraining the model.","slug":"episodic-memory-for-agents","authors":["adarsha"],"date":"2026-02-19T00:00:00.000Z","tags":["ai-agents","memory","architecture","llm","episodic-memory"]},"unlisted":false,"nextItem":{"title":"LLM Inference Optimization Techniques: Engineering Sub-Second Latency at Scale","permalink":"/BharatMLStack/blog/llm-inference-optimization-sub-sec-latency"}}')},8453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var i=n(6540);const s={},r=i.createContext(s);function a(e){const t=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);