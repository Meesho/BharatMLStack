"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8439],{211:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"predator/v1.0.0/architecture","title":"Architecture","description":"Predator is a scalable, high-performance model inference service built as a wrapper around the NVIDIA Triton Inference Server. It is designed to serve a variety of machine learning models (Deep Learning, Tree-based, etc.) with low latency in a Kubernetes (K8s) environment.","source":"@site/docs/predator/v1.0.0/architecture.md","sourceDirName":"predator/v1.0.0","slug":"/predator/v1.0.0/architecture","permalink":"/BharatMLStack/predator/v1.0.0/architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/Meesho/BharatMLStack/tree/main/docs/docs/predator/v1.0.0/architecture.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Architecture","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"v1.0.0","permalink":"/BharatMLStack/predator/v1.0.0"},"next":{"title":"Key Functionalities","permalink":"/BharatMLStack/predator/v1.0.0/functionalities"}}');var t=r(4848),s=r(8453);const o={title:"Architecture",sidebar_position:1},a="BharatMLStack - Predator",d={},c=[{value:"High-Level Design",id:"high-level-design",level:2},{value:"Inference Engine: Triton Inference Server",id:"inference-engine-triton-inference-server",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Backends",id:"backends",level:3},{value:"Key Features",id:"key-features",level:3},{value:"Model Repository Structure",id:"model-repository-structure",level:2},{value:"Sample config.pbtxt",id:"sample-configpbtxt",level:3},{value:"Kubernetes Deployment Architecture",id:"kubernetes-deployment-architecture",level:2},{value:"Pod Architecture",id:"pod-architecture",level:3},{value:"Init Container",id:"init-container",level:4},{value:"Triton Inference Server Container",id:"triton-inference-server-container",level:4},{value:"Triton Server Image Strategy",id:"triton-server-image-strategy",level:3},{value:"Image Distribution Optimization",id:"image-distribution-optimization",level:3},{value:"Health Probes",id:"health-probes",level:3},{value:"Resource Configuration",id:"resource-configuration",level:3},{value:"Autoscaling Architecture",id:"autoscaling-architecture",level:3},{value:"Contributing",id:"contributing",level:2},{value:"Community &amp; Support",id:"community--support",level:2},{value:"License",id:"license",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"bharatmlstack---predator",children:"BharatMLStack - Predator"})}),"\n",(0,t.jsxs)(n.p,{children:["Predator is a scalable, high-performance model inference service built as a wrapper around the ",(0,t.jsx)(n.strong,{children:"NVIDIA Triton Inference Server"}),". It is designed to serve a variety of machine learning models (Deep Learning, Tree-based, etc.) with low latency in a ",(0,t.jsx)(n.strong,{children:"Kubernetes (K8s)"})," environment."]}),"\n",(0,t.jsxs)(n.p,{children:["The system integrates seamlessly with the ",(0,t.jsx)(n.strong,{children:"Online Feature Store (OnFS)"})," for real-time feature retrieval and uses ",(0,t.jsx)(n.strong,{children:"Interflow"})," as an orchestration layer to manage traffic between client applications (e.g. IOP), feature store and inference engine."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"high-level-design",children:"High-Level Design"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Predator HLD - Triton image build, K8s pod, inference flow, metrics",src:r(2202).A+"",width:"2260",height:"1034"})}),"\n",(0,t.jsxs)(n.p,{children:["The diagram shows the Predator inference service in Kubernetes: custom Triton images are built on a GCP VM, pushed to Artifact Registry, and cached in the nodepool. The ",(0,t.jsx)(n.strong,{children:"Predator K8s GPU/CPU Pod"})," runs an ",(0,t.jsx)(n.strong,{children:"Init Container"})," that downloads model artifacts from ",(0,t.jsx)(n.strong,{children:"GCS"}),", and a ",(0,t.jsx)(n.strong,{children:"Triton Inference Container"})," that loads models and serves inference requests via the ",(0,t.jsx)(n.strong,{children:"Helix Client"}),". Metrics are emitted to ",(0,t.jsx)(n.strong,{children:"Grafana"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"inference-engine-triton-inference-server",children:"Inference Engine: Triton Inference Server"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Triton Inference Server is a high-performance model serving system designed to deploy ML and deep learning models at scale across CPUs and GPUs. It provides a unified inference runtime that supports multiple frameworks, optimized execution, and production-grade scheduling."}),"\n",(0,t.jsxs)(n.p,{children:["Triton operates as a standalone server that loads models from a model repository and exposes standardized HTTP/gRPC APIs. Predator uses ",(0,t.jsx)(n.strong,{children:"gRPC"})," for efficient request and response handling via the ",(0,t.jsx)(n.strong,{children:"helix client"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"core-components",children:"Core Components"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Repository"}),": Central directory where models are stored. Predator typically materializes the model repository onto local disk via an init container, enabling fast model loading and eliminating runtime dependency on remote storage during inference."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"backends",children:"Backends"}),"\n",(0,t.jsx)(n.p,{children:"A backend is the runtime responsible for executing a model. Each model specifies which backend runs it via configuration."}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Backend"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"TensorRT"})}),(0,t.jsx)(n.td,{children:"GPU-optimized; executes serialized TensorRT engines (kernel fusion, FP16/INT8)."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"PyTorch"})}),(0,t.jsx)(n.td,{children:"Serves native PyTorch models via LibTorch."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"ONNX Runtime"})}),(0,t.jsx)(n.td,{children:"Framework-agnostic ONNX execution with TensorRT and other accelerators."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"TensorFlow"})}),(0,t.jsx)(n.td,{children:"Runs TensorFlow SavedModel format."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Python backend"})}),(0,t.jsx)(n.td,{children:"Custom Python code for preprocessing, postprocessing, or unsupported models."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Custom backends"})}),(0,t.jsx)(n.td,{children:"C++/Python backends for specialized or proprietary runtimes."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"DALI"})}),(0,t.jsx)(n.td,{children:"GPU-accelerated data preprocessing (image, audio, video)."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"FIL (Forest Inference Library)"})}),(0,t.jsx)(n.td,{children:"GPU-accelerated tree-based models (XGBoost, LightGBM, Random Forest)."})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic batching"}),": Combines multiple requests into a single batch at runtime \u2014 higher GPU utilization, improved throughput, reduced latency variance."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Concurrent model execution"}),": Run multiple models or multiple instances of the same model; distribute load across GPUs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model versioning"}),": Support multiple versions per model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ensemble models"}),": Pipeline of models as an ensemble; eliminates intermediate network hops, reduces latency."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model instance scaling"}),": Multiple copies of a model for parallel inference and load isolation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Observability"}),": Prometheus metrics, granular latency, throughput, GPU utilization."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Warmup requests"}),": Preload kernels and avoid cold-start latency."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"model-repository-structure",children:"Model Repository Structure"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"model_repository/\n\u251c\u2500\u2500 model_A/\n\u2502   \u251c\u2500\u2500 config.pbtxt\n\u2502   \u251c\u2500\u2500 1/\n\u2502   \u2502   \u2514\u2500\u2500 model.plan\n\u2502   \u251c\u2500\u2500 2/\n\u2502   \u2502   \u2514\u2500\u2500 model.plan\n\u251c\u2500\u2500 model_B/\n\u2502   \u251c\u2500\u2500 config.pbtxt\n\u2502   \u251c\u2500\u2500 1/\n\u2502       \u2514\u2500\u2500 model.py\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"config.pbtxt"})," file defines how Triton loads and executes a model: input/output tensors, batch settings, hardware execution, backend runtime, and optimization parameters. At minimum it defines: ",(0,t.jsx)(n.code,{children:"backend/platform"}),", ",(0,t.jsx)(n.code,{children:"max_batch_size"}),", ",(0,t.jsx)(n.code,{children:"inputs"}),", ",(0,t.jsx)(n.code,{children:"outputs"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"sample-configpbtxt",children:"Sample config.pbtxt"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'name: "product_ranking_model"\nplatform: "tensorrt_plan"\nmax_batch_size: 64\ninput [ { name: "input_embeddings" data_type: TYPE_FP16 dims: [ 128 ] }, { name: "context_features" data_type: TYPE_FP32 dims: [ 32 ] } ]\noutput [ { name: "scores" data_type: TYPE_FP32 dims: [ 1 ] } ]\ninstance_group [ { kind: KIND_GPU count: 2 gpus: [0] } ]\ndynamic_batching { preferred_batch_size: [8,16,32,64] max_queue_delay_microseconds: 2000 }\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"kubernetes-deployment-architecture",children:"Kubernetes Deployment Architecture"}),"\n",(0,t.jsxs)(n.p,{children:["Predator inference services are deployed on Kubernetes using ",(0,t.jsx)(n.strong,{children:"Helm-based"})," deployments for standardized, scalable, GPU-optimized model serving. Each deployment consists of Triton Inference Server wrapped within a Predator runtime, with autoscaling driven by CPU and GPU utilization."]}),"\n",(0,t.jsx)(n.h3,{id:"pod-architecture",children:"Pod Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Predator Pod\n\u251c\u2500\u2500 Init Container (Model Sync)\n\u251c\u2500\u2500 Triton Inference Server Container\n"})}),"\n",(0,t.jsx)(n.p,{children:"Model artifacts and runtime are initialized before inference traffic is accepted."}),"\n",(0,t.jsx)(n.h4,{id:"init-container",children:"Init Container"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Download model artifacts from cloud storage (GCS)."}),"\n",(0,t.jsx)(n.li,{children:"Populate the Triton model repository directory."}),"\n",(0,t.jsxs)(n.li,{children:["Example: ",(0,t.jsx)(n.code,{children:"gcloud storage cp -r gs://.../model-path/* /models"})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Benefits: deterministic startup (Triton starts only after models are available), separation of concerns (image = runtime, repository = data)."}),"\n",(0,t.jsx)(n.h4,{id:"triton-inference-server-container",children:"Triton Inference Server Container"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Load model artifacts from local repository."}),"\n",(0,t.jsx)(n.li,{children:"Manage inference scheduling, request/response handling, and expose inference endpoints."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"triton-server-image-strategy",children:"Triton Server Image Strategy"}),"\n",(0,t.jsxs)(n.p,{children:["The Helm chart uses the Triton container image from the internal ",(0,t.jsx)(n.strong,{children:"artifact registry"}),". Production uses ",(0,t.jsx)(n.strong,{children:"custom-built"})," images (only required backends, e.g. TensorRT, Python) to reduce size and startup time. Unnecessary components are excluded; images are built internally and pushed to the registry."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Response Caching"}),": Custom cache plugins can be added at image build time for optional inference response caching \u2014 reducing redundant execution and GPU use for repeated inputs."]}),"\n",(0,t.jsx)(n.h3,{id:"image-distribution-optimization",children:"Image Distribution Optimization"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Secondary boot disk image caching"}),": Images are pre-cached on GPU node pool secondary boot disks to avoid repeated pulls during scale-up and reduce pod startup time and cold-start latency."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Image streaming"}),": Can be used to progressively pull layers for faster time-to-readiness during scaling."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"health-probes",children:"Health Probes"}),"\n",(0,t.jsxs)(n.p,{children:["Readiness and liveness use ",(0,t.jsx)(n.code,{children:"/v2/health/ready"}),". Triton receives traffic only after model loading; failed instances are restarted automatically."]}),"\n",(0,t.jsx)(n.h3,{id:"resource-configuration",children:"Resource Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Sample GPU resource config:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"limits:\n  cpu: 7000m\n  memory: 28Gi\n  gpu: 1\n"})}),"\n",(0,t.jsx)(n.h3,{id:"autoscaling-architecture",children:"Autoscaling Architecture"}),"\n",(0,t.jsxs)(n.p,{children:["Predator uses autoscaling based on ",(0,t.jsx)(n.strong,{children:"CPU"})," and ",(0,t.jsx)(n.strong,{children:"GPU"})," (for GPU pods) metrics. GPU scaling uses ",(0,t.jsx)(n.strong,{children:"DCGM"})," (Data Center GPU Manager) for utilization, memory, power, etc.; custom queries drive scale-up/scale-down."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"contributing",children:"Contributing"}),"\n",(0,t.jsxs)(n.p,{children:["We welcome contributions! See the ",(0,t.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/CONTRIBUTING.md",children:"Contributing Guide"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"community--support",children:"Community & Support"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Discord"}),": ",(0,t.jsx)(n.a,{href:"https://discord.gg/XkT7XsV2AU",children:"community chat"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Issues"}),": ",(0,t.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/issues",children:"GitHub Issues"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Email"}),": ",(0,t.jsx)(n.a,{href:"mailto:ml-oss@meesho.com",children:"ml-oss@meesho.com"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"license",children:"License"}),"\n",(0,t.jsxs)(n.p,{children:["BharatMLStack is open-source under the ",(0,t.jsx)(n.a,{href:"https://github.com/Meesho/BharatMLStack/blob/main/LICENSE.md",children:"BharatMLStack Business Source License 1.1"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("strong",{children:"Built with \u2764\ufe0f for the ML community from Meesho"})}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("strong",{children:"If you find this useful, \u2b50\ufe0f the repo \u2014 your support means the world to us!"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},2202:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/v1.0.0-predator-hld-a1ffe61641c1ed39ea4abfa3132a9f91.png"},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var i=r(6540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);