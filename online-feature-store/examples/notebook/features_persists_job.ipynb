{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to add the spark-sql-kafka dependency from the maven repository to your spark cluster\n",
    "# Package URL -> https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the online-feature-store-py-client if not already installed\n",
    "!pip install online-feature-store-py-client==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion_py_client import OrionPyClient\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "job_config = config.job_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helpers import get_features_from_all_sources, write_to_cloud_storage, fill_na_features_with_default_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try accessing the existing spark session\n",
    "    spark\n",
    "except NameError:\n",
    "    # If spark is not defined, create a new session\n",
    "    print(\"Creating a new spark session\")\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Online Feature Store Feature Push\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_metadata_source_url = job_config[\"features_metadata_source_url\"]\n",
    "entities_configs = job_config[\"entities_configs\"]\n",
    "\n",
    "for entity_config in entities_configs:\n",
    "    job_id = entity_config[\"job_id\"]\n",
    "    job_token = entity_config[\"job_token\"]\n",
    "    \n",
    "    print(\"#\" * 10 + f\" using job_id: {job_id} \" + \"#\" * 10)\n",
    "\n",
    "    if \"fgs_to_consider\" in entity_config:\n",
    "        fgs_to_consider = entity_config[\"fgs_to_consider\"]\n",
    "    else:\n",
    "        fgs_to_consider = (\n",
    "            []\n",
    "        )  # if fgs_to_consider is empty, all the feature groups will be considered\n",
    "\n",
    "    # create a new OnlineFeatureStore client\n",
    "    opy_client = OnlineFeatureStorePyClient(features_metadata_source_url, job_id, job_token)\n",
    "\n",
    "    # get the features details\n",
    "    (\n",
    "        offline_src_type_columns,\n",
    "        offline_col_to_default_values_map,\n",
    "        entity_column_names,\n",
    "    ) = opy_client.get_features_details()\n",
    "\n",
    "    if (\n",
    "        \"src_type_to_partition_col_map\" in entity_config\n",
    "    ):  # check if src_type_to_partition_col_map is present in the entity config\n",
    "        df = get_features_from_all_sources(\n",
    "            spark,\n",
    "            entity_column_names,\n",
    "            offline_src_type_columns,\n",
    "            entity_config[\"src_type_to_partition_col_map\"],\n",
    "        )\n",
    "    else:\n",
    "        # uses the default mapping for partition cols: {\"TABLE\": \"process_date\", \"PARQUET_GCS\": \"ts\", \"PARQUET_S3\": \"ts\", \"PARQUET_ADLS\": \"ts\",  \"DELTA_GCS\": \"ts\", \"DELTA_S3\": \"ts\", \"DELTA_S3\": \"ts\"}\n",
    "        df = get_features_from_all_sources(\n",
    "            spark,\n",
    "            entity_column_names,\n",
    "            offline_src_type_columns\n",
    "        )\n",
    "        \n",
    "    offline_col_to_datatype_map = opy_client.get_offline_col_to_datatype_map()\n",
    "    df = fill_na_features_with_default_values(df, offline_col_to_default_values_map, offline_col_to_datatype_map)\n",
    "\n",
    "    if (\n",
    "        \"rows_limit\" in entity_config\n",
    "    ):  # check if rows_limit is present in the entity config\n",
    "        print(\n",
    "            f\"limiting the number of rows to {entity_config['rows_limit']}\"\n",
    "        )\n",
    "        rows_limit = entity_config[\"rows_limit\"]\n",
    "        df = df.limit(rows_limit)\n",
    "        \n",
    "    if job_config[\"features_write_to_cloud_storage\"]:\n",
    "        curr_ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        out_path = os.path.join(job_config[\"features_output_cloud_storage_path\"], f\"job_id={job_id}/ts={curr_ts}\")\n",
    "        print(f\"writing features of job_id={job_id} to {out_path}\")\n",
    "        write_to_cloud_storage(df, out_path)\n",
    "        df = spark.read.parquet(out_path)\n",
    "        print(f\"count of features df : {df.count()}\")\n",
    "\n",
    "    proto_df = opy_client.generate_df_with_protobuf_messages(\n",
    "        df, intra_batch_size=20\n",
    "    )  # generate a dataframe with protobuf messages. If intra_batch_size is not provided, default is 20\n",
    "    \n",
    "    proto_out_path = os.path.join(job_config[\"features_output_cloud_storage_path\"], f\"job_id={job_id}/proto_df/ts={curr_ts}\")\n",
    "    print(f\"writing proto_df of job_id={job_id} to {proto_out_path}\")\n",
    "    write_to_cloud_storage(proto_df, proto_out_path)\n",
    "    proto_df = spark.read.parquet(proto_out_path)\n",
    "    \n",
    "    # write the protobuf dataframe to kafka\n",
    "    \n",
    "    print(\"writing data to kafka\")\n",
    "    \n",
    "    kafka_bootstrap_servers = job_config[\"kafka_config\"][\"kafka.bootstrap.servers\"]\n",
    "    kafka_topic = job_config[\"kafka_config\"][\"topic\"]\n",
    "    additional_options = job_config[\"kafka_config\"][\"additional_options\"]\n",
    "    \n",
    "    if \"push_to_kafka_in_batches\" in entity_config and entity_config[\"push_to_kafka_in_batches\"]:\n",
    "        kafka_num_batches = entity_config[\"kafka_num_batches\"]\n",
    "        opy_client.write_protobuf_df_to_kafka(\n",
    "            proto_df, kafka_bootstrap_servers, kafka_topic, additional_options, kafka_num_batches\n",
    "        )\n",
    "    else:\n",
    "        opy_client.write_protobuf_df_to_kafka(\n",
    "            proto_df, kafka_bootstrap_servers, kafka_topic, additional_options\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
